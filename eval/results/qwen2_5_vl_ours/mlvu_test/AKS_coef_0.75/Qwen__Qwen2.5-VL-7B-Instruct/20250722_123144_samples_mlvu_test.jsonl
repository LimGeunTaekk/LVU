{"doc_id": 0, "doc": {"video_name": "test_surveil_27.mp4", "duration": 297.57, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Shoplifting\n(C) Arrest\n(D) Normal\n(E) Shooting\n(F) Vandalism", "candidates": ["Fighting", "Shoplifting", "Arrest", "Normal", "Shooting", "Vandalism"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_0", "indices": ["210", "390", "690", "1050", "1110", "1440", "1830", "1980", "2460", "2520", "2760", "3030", "3510", "3600", "4050", "4380", "4590", "4920", "5190", "5520", "5550", "5910", "6330", "6390", "6720", "6960", "7290", "7680", "7800", "8220", "8520", "8700"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96891f8f74d7a95a9de3c1b429944f4b81ce4e98872468486cdce114e2ebcaa4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Shoplifting\n(C) Arrest\n(D) Normal\n(E) Shooting\n(F) Vandalism", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Shoplifting\n(C) Arrest\n(D) Normal\n(E) Shooting\n(F) Vandalism\nOnly give the best option.\nBest option: ("}
{"doc_id": 4, "doc": {"video_name": "test_surveil_31.mp4", "duration": 602.03, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Vandalism\n(D) Arson\n(E) Shooting\n(F) RoadAccidents", "candidates": ["Normal", "Burglary", "Vandalism", "Arson", "Shooting", "RoadAccidents"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_4", "indices": ["270", "900", "1590", "1950", "2520", "2820", "3810", "4440", "4650", "5340", "5820", "6510", "7200", "7320", "7890", "8550", "9120", "9570", "10590", "10740", "11430", "12090", "12690", "13110", "14010", "14220", "14640", "15570", "15780", "16800", "16980", "17850"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) RoadAccidents"]], "filtered_resps": ["(F) RoadAccidents"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b22e14bf3a2339c2a3da60a83ab68ec49663f9be0e026ccb0aa13880fcb4a6cf", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Vandalism\n(D) Arson\n(E) Shooting\n(F) RoadAccidents", "task_type": "anomaly_reco", "pred_answer": "F", "answer": "F"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Normal\n(B) Burglary\n(C) Vandalism\n(D) Arson\n(E) Shooting\n(F) RoadAccidents\nOnly give the best option.\nBest option: ("}
{"doc_id": 8, "doc": {"video_name": "test_surveil_18.mp4", "duration": 187.53, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Arson\n(D) RoadAccidents\n(E) Stealing\n(F) Robbery", "candidates": ["Vandalism", "Shoplifting", "Arson", "RoadAccidents", "Stealing", "Robbery"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_8", "indices": ["30", "240", "360", "540", "690", "840", "1110", "1290", "1410", "1530", "1740", "1890", "2220", "2370", "2550", "2760", "2820", "3090", "3150", "3330", "3510", "3810", "3900", "4020", "4260", "4410", "4590", "4860", "4980", "5130", "5250", "5520"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Stealing"]], "filtered_resps": ["(E) Stealing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19401255a426441d63c7ff08352933efd5d51a83ea695db5363076fd7608e655", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Arson\n(D) RoadAccidents\n(E) Stealing\n(F) Robbery", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "E"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Arson\n(D) RoadAccidents\n(E) Stealing\n(F) Robbery\nOnly give the best option.\nBest option: ("}
{"doc_id": 12, "doc": {"video_name": "test_surveil_21.mp4", "duration": 197.17000000000002, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Abuse\n(C) Normal\n(D) Assault\n(E) Robbery\n(F) Shooting", "candidates": ["Fighting", "Abuse", "Normal", "Assault", "Robbery", "Shooting"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_12", "indices": ["30", "300", "480", "600", "720", "1050", "1170", "1380", "1560", "1680", "1920", "2010", "2190", "2520", "2550", "2820", "2940", "3120", "3330", "3570", "3660", "3870", "4170", "4320", "4440", "4650", "4920", "5040", "5160", "5370", "5670", "5880"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Normal"]], "filtered_resps": ["(C) Normal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0895e6afce6a8f0b0f6a4dbde5defb259c8d7586a1bc7274899c6671d23125aa", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Abuse\n(C) Normal\n(D) Assault\n(E) Robbery\n(F) Shooting", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Fighting\n(B) Abuse\n(C) Normal\n(D) Assault\n(E) Robbery\n(F) Shooting\nOnly give the best option.\nBest option: ("}
{"doc_id": 16, "doc": {"video_name": "test_surveil_15.mp4", "duration": 471.52, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Robbery\n(C) Stealing\n(D) Shooting\n(E) Assault\n(F) Abuse", "candidates": ["RoadAccidents", "Robbery", "Stealing", "Shooting", "Assault", "Abuse"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_16", "indices": ["60", "720", "1200", "1680", "1800", "2430", "2940", "3060", "3510", "4050", "4620", "5070", "5460", "6090", "6450", "6600", "7140", "7620", "7920", "8610", "8940", "9420", "10020", "10230", "10920", "11160", "11790", "12210", "12390", "12810", "13260", "14010"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Assault"]], "filtered_resps": ["(E) Assault"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a9f21e798ad82cbaaad9bb1519590dade1e6ef2a2c35e77cc4236fe68586fdf", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Robbery\n(C) Stealing\n(D) Shooting\n(E) Assault\n(F) Abuse", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "F"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Robbery\n(C) Stealing\n(D) Shooting\n(E) Assault\n(F) Abuse\nOnly give the best option.\nBest option: ("}
{"doc_id": 20, "doc": {"video_name": "test_surveil_17.mp4", "duration": 262.53, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Stealing\n(D) Burglary\n(E) Assault\n(F) Shoplifting", "candidates": ["Robbery", "Abuse", "Stealing", "Burglary", "Assault", "Shoplifting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_20", "indices": ["60", "330", "570", "870", "1170", "1320", "1620", "1800", "2130", "2190", "2520", "2850", "3060", "3180", "3450", "3750", "3930", "4350", "4500", "4860", "4920", "5250", "5370", "5700", "5880", "6180", "6390", "6840", "7020", "7110", "7440", "7680"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Assault"]], "filtered_resps": ["(E) Assault"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cd638aac5b2b1af3e03e933d42d80ebb04a43a668e7daa3c37b0ec8ab6e466", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Stealing\n(D) Burglary\n(E) Assault\n(F) Shoplifting", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Abuse\n(C) Stealing\n(D) Burglary\n(E) Assault\n(F) Shoplifting\nOnly give the best option.\nBest option: ("}
{"doc_id": 24, "doc": {"video_name": "test_surveil_20.mp4", "duration": 195.06, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n(E) Normal\n(F) Stealing", "candidates": ["Shoplifting", "RoadAccidents", "Abuse", "Shooting", "Normal", "Stealing"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_24", "indices": ["0", "180", "450", "570", "840", "990", "1170", "1320", "1440", "1770", "1800", "2010", "2310", "2460", "2610", "2790", "2910", "3240", "3270", "3480", "3660", "3840", "4050", "4230", "4440", "4620", "4740", "4980", "5070", "5340", "5490", "5670"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Normal"]], "filtered_resps": ["(E) Normal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb74a33ac79c9d84987de92df5509e66c54dd010c9ccf0734f58fbfd3c54316b", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n(E) Normal\n(F) Stealing", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "F"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) RoadAccidents\n(C) Abuse\n(D) Shooting\n(E) Normal\n(F) Stealing\nOnly give the best option.\nBest option: ("}
{"doc_id": 28, "doc": {"video_name": "test_surveil_8.mp4", "duration": 526.55, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arrest\n(E) Shoplifting\n(F) Arson", "candidates": ["Robbery", "Shooting", "Burglary", "Arrest", "Shoplifting", "Arson"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_28", "indices": ["330", "810", "960", "1680", "2280", "2460", "3000", "3870", "3990", "4710", "5370", "5430", "6030", "6870", "7140", "7860", "7890", "8430", "9270", "9540", "9840", "10410", "11250", "11310", "12060", "12750", "13110", "13740", "14220", "14610", "15090", "15360"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) Robbery"]], "filtered_resps": ["(A) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab99669f99506e2000ff0cc47874b49dc78ffebee9d0827d463a9eb4666a8432", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arrest\n(E) Shoplifting\n(F) Arson", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Shooting\n(C) Burglary\n(D) Arrest\n(E) Shoplifting\n(F) Arson\nOnly give the best option.\nBest option: ("}
{"doc_id": 32, "doc": {"video_name": "test_surveil_25.mp4", "duration": 231.67000000000002, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Shoplifting\n(C) Assault\n(D) Stealing\n(E) Arrest\n(F) Arson", "candidates": ["Robbery", "Shoplifting", "Assault", "Stealing", "Arrest", "Arson"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_32", "indices": ["180", "210", "540", "750", "840", "1170", "1260", "1530", "1860", "1980", "2250", "2520", "2610", "2820", "3150", "3360", "3570", "3720", "4020", "4080", "4500", "4650", "4740", "4950", "5370", "5400", "5730", "5880", "6210", "6270", "6540", "6720"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Assault"]], "filtered_resps": ["(C) Assault"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "816c16ca65a99148e9b876c872e61434ecbd054ec9ca91f4579dc4fefe1d702f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Shoplifting\n(C) Assault\n(D) Stealing\n(E) Arrest\n(F) Arson", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Robbery\n(B) Shoplifting\n(C) Assault\n(D) Stealing\n(E) Arrest\n(F) Arson\nOnly give the best option.\nBest option: ("}
{"doc_id": 36, "doc": {"video_name": "test_surveil_23.mp4", "duration": 195.5, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Normal\n(E) Robbery\n(F) Arrest", "candidates": ["Fighting", "RoadAccidents", "Abuse", "Normal", "Robbery", "Arrest"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_36", "indices": ["90", "270", "390", "600", "840", "990", "1140", "1320", "1440", "1710", "1890", "1980", "2190", "2490", "2670", "2850", "2970", "3090", "3300", "3480", "3780", "3900", "4050", "4350", "4380", "4680", "4770", "4980", "5220", "5400", "5460", "5760"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Normal"]], "filtered_resps": ["(D) Normal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53a23e3f8b729abf8621b36f7173218fa501e79672f9b8ff348d6e66c5516a97", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Normal\n(E) Robbery\n(F) Arrest", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Normal\n(E) Robbery\n(F) Arrest\nOnly give the best option.\nBest option: ("}
{"doc_id": 40, "doc": {"video_name": "test_sports_7.mp4", "duration": 750.82, "question": "What interaction did the players from both teams have before entering the field?\n(A) No interaction\n(B) Group photo\n(C) They greeted each other with high-fives\n(D) Provoking each other\n(E) Fighting\n(F) Exchanging gifts", "candidates": ["No interaction", "Group photo", "They greeted each other with high-fives", "Provoking each other", "Fighting", "Exchanging gifts"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_1", "indices": ["354", "2183", "4130", "4248", "6549", "8201", "8378", "10384", "12095", "13924", "14337", "16048", "18172", "18821", "20060", "22125", "23423", "24957", "25901", "28025", "28143", "30090", "31034", "32686", "34869", "36285", "37288", "38763", "39412", "41064", "42716", "44781"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "139513052a88029b340c80aa6ac937df459ed2d71d92219218e8a0c352e3d7db", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What interaction did the players from both teams have before entering the field?\n(A) No interaction\n(B) Group photo\n(C) They greeted each other with high-fives\n(D) Provoking each other\n(E) Fighting\n(F) Exchanging gifts", "task_type": "sportsQA", "pred_answer": "D", "answer": "C"}, "input": "What interaction did the players from both teams have before entering the field?\n(A) No interaction\n(B) Group photo\n(C) They greeted each other with high-fives\n(D) Provoking each other\n(E) Fighting\n(F) Exchanging gifts\nOnly give the best option.\nBest option: ("}
{"doc_id": 44, "doc": {"video_name": "test_sports_23.mp4", "duration": 440.88, "question": "How many people jump into the water first at the beginning of the video?\n(A) 7 people\n(B) 3 people\n(C) 8 people\n(D) 2 people\n(E) 5 people\n(F) 4 people", "candidates": ["7 people", "3 people", "8 people", "2 people", "5 people", "4 people"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_5", "indices": ["225", "425", "950", "1275", "1525", "2025", "2275", "2725", "2950", "3075", "3475", "3950", "4425", "4775", "5050", "5450", "5625", "5825", "6475", "6625", "7125", "7300", "7600", "8100", "8500", "8900", "8925", "9600", "9625", "10250", "10575", "10750"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c3f2249abf811789957fc039d8b67a4ccfda2e744fc2f403943753095cfbf80", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "How many people jump into the water first at the beginning of the video?\n(A) 7 people\n(B) 3 people\n(C) 8 people\n(D) 2 people\n(E) 5 people\n(F) 4 people", "task_type": "sportsQA", "pred_answer": "B", "answer": "F"}, "input": "How many people jump into the water first at the beginning of the video?\n(A) 7 people\n(B) 3 people\n(C) 8 people\n(D) 2 people\n(E) 5 people\n(F) 4 people\nOnly give the best option.\nBest option: ("}
{"doc_id": 48, "doc": {"video_name": "test_sports_21.mp4", "duration": 516.28, "question": "What swimming style do the competitors use to return to the finish line?\n(A) Breaststroke\n(B) Freestyle\n(C) Medley\n(D) Underwater swimming\n(E) Backstroke\n(F) Butterfly", "candidates": ["Breaststroke", "Freestyle", "Medley", "Underwater swimming", "Backstroke", "Butterfly"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_9", "indices": ["275", "425", "800", "1525", "1825", "2200", "2625", "2800", "3400", "3725", "4225", "4650", "4825", "5550", "5625", "6050", "6575", "7200", "7450", "7650", "8075", "8550", "8925", "9250", "10000", "10200", "10575", "11250", "11300", "11975", "12125", "12825"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f6b5201ba0863658c8036be9b394d363f80b8fcaa2208862b9d6b1ca32c853f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What swimming style do the competitors use to return to the finish line?\n(A) Breaststroke\n(B) Freestyle\n(C) Medley\n(D) Underwater swimming\n(E) Backstroke\n(F) Butterfly", "task_type": "sportsQA", "pred_answer": "C", "answer": "E"}, "input": "What swimming style do the competitors use to return to the finish line?\n(A) Breaststroke\n(B) Freestyle\n(C) Medley\n(D) Underwater swimming\n(E) Backstroke\n(F) Butterfly\nOnly give the best option.\nBest option: ("}
{"doc_id": 52, "doc": {"video_name": "test_sports_25.mp4", "duration": 511.72, "question": "How many different swimming styles do the competitors use in the race?\n(A) 3 styles\n(B) 4 styles\n(C) 8 styles\n(D) 2 styles\n(E) 5 styles\n(F) 1 style", "candidates": ["3 styles", "4 styles", "8 styles", "2 styles", "5 styles", "1 style"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_13", "indices": ["50", "450", "1000", "1375", "1725", "2300", "2750", "2800", "3475", "3625", "4050", "4650", "4850", "5450", "5825", "6300", "6675", "6825", "7200", "7650", "8225", "8650", "8775", "9475", "9575", "10250", "10525", "10775", "11425", "11850", "12100", "12725"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5dc45de2a2628fc50ab15a306140a742eca786c50239b51faef6cc6d000dc2b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many different swimming styles do the competitors use in the race?\n(A) 3 styles\n(B) 4 styles\n(C) 8 styles\n(D) 2 styles\n(E) 5 styles\n(F) 1 style", "task_type": "sportsQA", "pred_answer": "B", "answer": "B"}, "input": "How many different swimming styles do the competitors use in the race?\n(A) 3 styles\n(B) 4 styles\n(C) 8 styles\n(D) 2 styles\n(E) 5 styles\n(F) 1 style\nOnly give the best option.\nBest option: ("}
{"doc_id": 56, "doc": {"video_name": "test_sports_8.mp4", "duration": 668.23, "question": "During the penalty kick, which direction did the green-jerseyed goalkeeper move to block the shot from the player in yellow jersey and blue shorts number 10?\n(A) Moved to the top right corner to block the shot\n(B) Moved to the right to block the shot\n(C) Moved upwards to block the shot\n(D) Moved forward to block the shot\n(E) Moved to the top left corner to block the shot\n(F) Stayed in place", "candidates": ["Moved to the top right corner to block the shot", "Moved to the right to block the shot", "Moved upwards to block the shot", "Moved forward to block the shot", "Moved to the top left corner to block the shot", "Stayed in place"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_17", "indices": ["1062", "1416", "2773", "4012", "5900", "6372", "8260", "8673", "10502", "12095", "12449", "14632", "15045", "16402", "17995", "19175", "20886", "21653", "23600", "24367", "25488", "26432", "28261", "29736", "29972", "31388", "32568", "34161", "36049", "36816", "37583", "39353"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Moved to the top left corner to block the shot"]], "filtered_resps": ["(E) Moved to the top left corner to block the shot"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0580718f5bc51007f6d88bcc35269ce07b26aa713d205c2dd22ed2360bdcb2fe", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "During the penalty kick, which direction did the green-jerseyed goalkeeper move to block the shot from the player in yellow jersey and blue shorts number 10?\n(A) Moved to the top right corner to block the shot\n(B) Moved to the right to block the shot\n(C) Moved upwards to block the shot\n(D) Moved forward to block the shot\n(E) Moved to the top left corner to block the shot\n(F) Stayed in place", "task_type": "sportsQA", "pred_answer": "E", "answer": "B"}, "input": "During the penalty kick, which direction did the green-jerseyed goalkeeper move to block the shot from the player in yellow jersey and blue shorts number 10?\n(A) Moved to the top right corner to block the shot\n(B) Moved to the right to block the shot\n(C) Moved upwards to block the shot\n(D) Moved forward to block the shot\n(E) Moved to the top left corner to block the shot\n(F) Stayed in place\nOnly give the best option.\nBest option: ("}
{"doc_id": 60, "doc": {"video_name": "test_sports_4.mp4", "duration": 978.94, "question": "Which player in deep blue jersey scored an own goal?\n(A) Player number 4 in deep blue jersey\n(B) Player number 10 in deep blue jersey\n(C) Player number 3 in deep blue jersey\n(D) Player number 16 in deep blue jersey\n(E) Player number 9 in deep blue jersey\n(F) Player number 5 in deep blue jersey", "candidates": ["Player number 4 in deep blue jersey", "Player number 10 in deep blue jersey", "Player number 3 in deep blue jersey", "Player number 16 in deep blue jersey", "Player number 9 in deep blue jersey", "Player number 5 in deep blue jersey"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_21", "indices": ["950", "2500", "4150", "5050", "6100", "8500", "9300", "11450", "13650", "15050", "15300", "17800", "18650", "21200", "22100", "22950", "25300", "27050", "27500", "30150", "30550", "32500", "33950", "36450", "37350", "38350", "40200", "42550", "43950", "45000", "46200", "47500"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Player number 3 in deep blue jersey"]], "filtered_resps": ["(C) Player number 3 in deep blue jersey"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ef7c15904f203e9e12d6145fa4cfe2742a125d407559faa5ed39b0e5fcacf93", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which player in deep blue jersey scored an own goal?\n(A) Player number 4 in deep blue jersey\n(B) Player number 10 in deep blue jersey\n(C) Player number 3 in deep blue jersey\n(D) Player number 16 in deep blue jersey\n(E) Player number 9 in deep blue jersey\n(F) Player number 5 in deep blue jersey", "task_type": "sportsQA", "pred_answer": "C", "answer": "C"}, "input": "Which player in deep blue jersey scored an own goal?\n(A) Player number 4 in deep blue jersey\n(B) Player number 10 in deep blue jersey\n(C) Player number 3 in deep blue jersey\n(D) Player number 16 in deep blue jersey\n(E) Player number 9 in deep blue jersey\n(F) Player number 5 in deep blue jersey\nOnly give the best option.\nBest option: ("}
{"doc_id": 64, "doc": {"video_name": "test_sports_1.mp4", "duration": 645.0, "question": "When the player in white and blue jersey took the free kick, which direction did the goalkeeper number 12 from the red team dive?\n(A) He stayed in place\n(B) He dived forward\n(C) He dived to the top right\n(D) He dived to the right\n(E) He dived to the top left\n(F) He dived upwards", "candidates": ["He stayed in place", "He dived forward", "He dived to the top right", "He dived to the right", "He dived to the top left", "He dived upwards"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_25", "indices": ["420", "1020", "1710", "1950", "2910", "3360", "4050", "4200", "4830", "6000", "6150", "6990", "7740", "8160", "8610", "9120", "10200", "10260", "11070", "11520", "12210", "12990", "13470", "14190", "14760", "15330", "16110", "16410", "17160", "17550", "18150", "18810"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a093a72ad209ec5542361e783cb36e24afed5862a5eac49cd3c7f7ca84aadca5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "When the player in white and blue jersey took the free kick, which direction did the goalkeeper number 12 from the red team dive?\n(A) He stayed in place\n(B) He dived forward\n(C) He dived to the top right\n(D) He dived to the right\n(E) He dived to the top left\n(F) He dived upwards", "task_type": "sportsQA", "pred_answer": "D", "answer": "E"}, "input": "When the player in white and blue jersey took the free kick, which direction did the goalkeeper number 12 from the red team dive?\n(A) He stayed in place\n(B) He dived forward\n(C) He dived to the top right\n(D) He dived to the right\n(E) He dived to the top left\n(F) He dived upwards\nOnly give the best option.\nBest option: ("}
{"doc_id": 68, "doc": {"video_name": "test_sports_30.mp4", "duration": 5787.76, "question": "Which country's team performs the first synchronized diving in the video?\n(A) Belgium\n(B) United States\n(C) Sweden\n(D) China\n(E) Canada\n(F) South Korea", "candidates": ["Belgium", "United States", "Sweden", "China", "Canada", "South Korea"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_29", "indices": ["3950", "5125", "10675", "17400", "21775", "23425", "28400", "32200", "37550", "41700", "48875", "52725", "55075", "62125", "64175", "69500", "73525", "78175", "83700", "88200", "91900", "95900", "100925", "105100", "110975", "113200", "120225", "126000", "128975", "131975", "136500", "141550"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Canada"]], "filtered_resps": ["(E) Canada"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c3958c7067400391ac38d8e7dcbc0df6f8f96d4a803170583297bffe7294ce02", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which country's team performs the first synchronized diving in the video?\n(A) Belgium\n(B) United States\n(C) Sweden\n(D) China\n(E) Canada\n(F) South Korea", "task_type": "sportsQA", "pred_answer": "E", "answer": "E"}, "input": "Which country's team performs the first synchronized diving in the video?\n(A) Belgium\n(B) United States\n(C) Sweden\n(D) China\n(E) Canada\n(F) South Korea\nOnly give the best option.\nBest option: ("}
{"doc_id": 72, "doc": {"video_name": "test_sports_10.mp4", "duration": 1271.68, "question": "How did the white team score their first goal?\n(A) Penalty kick\n(B) Header\n(C) Free kick\n(D) Long shot\n(E) Corner kick\n(F) Individual breakthrough", "candidates": ["Penalty kick", "Header", "Free kick", "Long shot", "Corner kick", "Individual breakthrough"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_33", "indices": ["1850", "2500", "5600", "7850", "8850", "11800", "12550", "14100", "16950", "18450", "20000", "22950", "24500", "25850", "29700", "30600", "33350", "34400", "36750", "38600", "40250", "42000", "43850", "47250", "48150", "51200", "52000", "54450", "55650", "58450", "59700", "61650"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Corner kick"]], "filtered_resps": ["(E) Corner kick"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "270bb27dd0730124ac831df9c18efffb275a3244efe46dc6c32d68724cad892b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How did the white team score their first goal?\n(A) Penalty kick\n(B) Header\n(C) Free kick\n(D) Long shot\n(E) Corner kick\n(F) Individual breakthrough", "task_type": "sportsQA", "pred_answer": "E", "answer": "A"}, "input": "How did the white team score their first goal?\n(A) Penalty kick\n(B) Header\n(C) Free kick\n(D) Long shot\n(E) Corner kick\n(F) Individual breakthrough\nOnly give the best option.\nBest option: ("}
{"doc_id": 76, "doc": {"video_name": "test_order_31.mp4", "duration": 635.25, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> baking cookies --> riding mule --> pole vault\n(B) baking cookies --> pole vault --> carving pumpkin --> riding mule\n(C) baking cookies --> carving pumpkin --> riding mule --> pole vault\n(D) carving pumpkin --> baking cookies --> pole vault --> riding mule\n(E) riding mule --> carving pumpkin --> pole vault --> baking cookies\n(F) riding mule --> baking cookies --> pole vault --> carving pumpkin", "candidates": ["carving pumpkin --> baking cookies --> riding mule --> pole vault", "baking cookies --> pole vault --> carving pumpkin --> riding mule", "baking cookies --> carving pumpkin --> riding mule --> pole vault", "carving pumpkin --> baking cookies --> pole vault --> riding mule", "riding mule --> carving pumpkin --> pole vault --> baking cookies", "riding mule --> baking cookies --> pole vault --> carving pumpkin"], "answer": "C", "task_type": "order", "question_id": "AO_1", "indices": ["330", "900", "1200", "2280", "2550", "3150", "3660", "4170", "5130", "5730", "6000", "6810", "7260", "8160", "8610", "9210", "9840", "10320", "11250", "11280", "12120", "12600", "13500", "13920", "14370", "15030", "15810", "16560", "17010", "17820", "17910", "18630"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D)"]], "filtered_resps": ["(D)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "39c6509a219d64667c4201b96d98d96843d05b7b39c28d15a948f03793a8d1bd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> baking cookies --> riding mule --> pole vault\n(B) baking cookies --> pole vault --> carving pumpkin --> riding mule\n(C) baking cookies --> carving pumpkin --> riding mule --> pole vault\n(D) carving pumpkin --> baking cookies --> pole vault --> riding mule\n(E) riding mule --> carving pumpkin --> pole vault --> baking cookies\n(F) riding mule --> baking cookies --> pole vault --> carving pumpkin", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> baking cookies --> riding mule --> pole vault\n(B) baking cookies --> pole vault --> carving pumpkin --> riding mule\n(C) baking cookies --> carving pumpkin --> riding mule --> pole vault\n(D) carving pumpkin --> baking cookies --> pole vault --> riding mule\n(E) riding mule --> carving pumpkin --> pole vault --> baking cookies\n(F) riding mule --> baking cookies --> pole vault --> carving pumpkin\nOnly give the best option.\nBest option: ("}
{"doc_id": 80, "doc": {"video_name": "test_order_69.mp4", "duration": 969.4200000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 1->4->3->2\n(E) 3->2->4->1\n(F) 1->2->3->4", "candidates": ["2->3->1->4", "4->1->2->3", "4->3->2->1", "1->4->3->2", "3->2->4->1", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "AO_5", "indices": ["667", "1218", "2494", "2871", "3712", "5104", "6235", "7134", "7250", "8961", "9106", "10846", "11455", "12383", "13137", "14181", "14906", "16153", "16675", "17429", "18618", "19198", "19923", "21344", "22272", "23461", "24215", "25230", "25868", "26941", "27666", "28304"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3321f33a3ad80f4d7b79fe191c5f6337494c74b4c5b580f9b117d0953cea9950", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 1->4->3->2\n(E) 3->2->4->1\n(F) 1->2->3->4", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The camera focuses on an older man's face; (2)The two children dance together; (3)The camera focuses on a bug on the wall; (4)The two children interact with each other in a cluttered room.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 1->4->3->2\n(E) 3->2->4->1\n(F) 1->2->3->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 84, "doc": {"video_name": "test_order_41.mp4", "duration": 710.4599999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> clean and jerk --> carving pumpkin --> water sliding\n(B) stomping grapes --> carving pumpkin --> water sliding --> clean and jerk\n(C) clean and jerk --> stomping grapes --> carving pumpkin --> water sliding\n(D) water sliding --> clean and jerk --> stomping grapes --> carving pumpkin\n(E) carving pumpkin --> stomping grapes --> water sliding --> clean and jerk\n(F) carving pumpkin --> water sliding --> clean and jerk --> stomping grapes", "candidates": ["stomping grapes --> clean and jerk --> carving pumpkin --> water sliding", "stomping grapes --> carving pumpkin --> water sliding --> clean and jerk", "clean and jerk --> stomping grapes --> carving pumpkin --> water sliding", "water sliding --> clean and jerk --> stomping grapes --> carving pumpkin", "carving pumpkin --> stomping grapes --> water sliding --> clean and jerk", "carving pumpkin --> water sliding --> clean and jerk --> stomping grapes"], "answer": "E", "task_type": "order", "question_id": "AO_9", "indices": ["0", "660", "1950", "2070", "3060", "3780", "4050", "5190", "5730", "6060", "7080", "7950", "8580", "8730", "9420", "10560", "11280", "11520", "12300", "12690", "13800", "14010", "14880", "15420", "16260", "17160", "17430", "18120", "19110", "19440", "20460", "20610"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c041637c4427b7fb20feccf5cae36859dc710742cf28ade0eb8fc8bbf7713a2f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> clean and jerk --> carving pumpkin --> water sliding\n(B) stomping grapes --> carving pumpkin --> water sliding --> clean and jerk\n(C) clean and jerk --> stomping grapes --> carving pumpkin --> water sliding\n(D) water sliding --> clean and jerk --> stomping grapes --> carving pumpkin\n(E) carving pumpkin --> stomping grapes --> water sliding --> clean and jerk\n(F) carving pumpkin --> water sliding --> clean and jerk --> stomping grapes", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events shown in the video. The", "answer": "E"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) stomping grapes --> clean and jerk --> carving pumpkin --> water sliding\n(B) stomping grapes --> carving pumpkin --> water sliding --> clean and jerk\n(C) clean and jerk --> stomping grapes --> carving pumpkin --> water sliding\n(D) water sliding --> clean and jerk --> stomping grapes --> carving pumpkin\n(E) carving pumpkin --> stomping grapes --> water sliding --> clean and jerk\n(F) carving pumpkin --> water sliding --> clean and jerk --> stomping grapes\nOnly give the best option.\nBest option: ("}
{"doc_id": 88, "doc": {"video_name": "test_order_60.mp4", "duration": 675.73, "question": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 3->4->1->2\n(C) 1->2->3->4\n(D) 1->2->4->3\n(E) 4->3->2->1\n(F) 2->1->4->3", "candidates": ["2->1->3->4", "3->4->1->2", "1->2->3->4", "1->2->4->3", "4->3->2->1", "2->1->4->3"], "answer": "F", "task_type": "order", "question_id": "AO_13", "indices": ["25", "1000", "1525", "1700", "2375", "2950", "3425", "3700", "4350", "4800", "5725", "6000", "6350", "7300", "7800", "8225", "8900", "9300", "9950", "10375", "10650", "11325", "11600", "12350", "13150", "13650", "13725", "14325", "15100", "15625", "15825", "16800"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) 2->1->3->4"]], "filtered_resps": ["(A) 2->1->3->4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06a6e25597ce7d3f405e62cd945d5bb15fa694b9581657443302016852ac27cc", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 3->4->1->2\n(C) 1->2->3->4\n(D) 1->2->4->3\n(E) 4->3->2->1\n(F) 2->1->4->3", "task_type": "order", "pred_answer": "A", "answer": "F"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man snowboards down a hill and turns around; (2)An old man holds a surfboard and puts on a helmet to snowboard; (3)A young person sits on the snow wearing a snowboard; (4)The man has a hot drink with other people.\n(A) 2->1->3->4\n(B) 3->4->1->2\n(C) 1->2->3->4\n(D) 1->2->4->3\n(E) 4->3->2->1\n(F) 2->1->4->3\nOnly give the best option.\nBest option: ("}
{"doc_id": 92, "doc": {"video_name": "test_order_66.mp4", "duration": 673.3499999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->3->1->2\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n(E) 2->4->1->3\n(F) 3->4->2->1", "candidates": ["4->3->1->2", "4->2->1->3", "1->2->3->4", "2->1->4->3", "2->4->1->3", "3->4->2->1"], "answer": "E", "task_type": "order", "question_id": "AO_17", "indices": ["150", "1025", "1200", "1900", "2475", "2825", "3450", "3800", "4650", "5200", "5675", "5925", "6550", "7225", "7375", "7950", "8400", "9200", "9675", "10375", "10600", "11325", "11900", "12575", "12600", "13200", "14100", "14475", "15200", "15275", "16150", "16325"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e039fc3e70d58983a158c5c94f28bf71a2882eea2a4b6fd23621e19a2939dd3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->3->1->2\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n(E) 2->4->1->3\n(F) 3->4->2->1", "task_type": "order", "pred_answer": "E", "answer": "E"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Man is on lake side talking to the camera like other couples as well; (2)Man is talking to the camera; (3)People are kayaking on calm river and have a good picnic day; (4)People are standing on a side of a rock wall.\n(A) 4->3->1->2\n(B) 4->2->1->3\n(C) 1->2->3->4\n(D) 2->1->4->3\n(E) 2->4->1->3\n(F) 3->4->2->1\nOnly give the best option.\nBest option: ("}
{"doc_id": 96, "doc": {"video_name": "test_order_50.mp4", "duration": 832.9000000000001, "question": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->1->4->2\n(E) 3->4->1->2\n(F) 4->2->1->3", "candidates": ["2->1->3->4", "1->2->3->4", "4->3->2->1", "3->1->4->2", "3->4->1->2", "4->2->1->3"], "answer": "E", "task_type": "order", "question_id": "AO_21", "indices": ["30", "1410", "1650", "2550", "3450", "3960", "5160", "6120", "6870", "7080", "8400", "8670", "9390", "10440", "11040", "11940", "12840", "13680", "14040", "15330", "15630", "16680", "17220", "18690", "19200", "19770", "20280", "21360", "22380", "23280", "23880", "24360"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "69e5a3cf044f1577421f64766f3197528e699c5f712971727ac445b8632018d1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->1->4->2\n(E) 3->4->1->2\n(F) 4->2->1->3", "task_type": "order", "pred_answer": "E", "answer": "E"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The chef pours soy sauce into the cup; (2)The chef shows off shredded garlic before throwing it into the cup; (3)The chef grabs a bowl of salad and shows it off; (4)The chef grabs a cup of nuts and throws it on top of a salad.\n(A) 2->1->3->4\n(B) 1->2->3->4\n(C) 4->3->2->1\n(D) 3->1->4->2\n(E) 3->4->1->2\n(F) 4->2->1->3\nOnly give the best option.\nBest option: ("}
{"doc_id": 100, "doc": {"video_name": "test_order_0.mp4", "duration": 490.03000000000003, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) milking cow --> cleaning toilet --> clean and jerk --> tossing coin\n(B) clean and jerk --> tossing coin --> cleaning toilet --> milking cow\n(C) milking cow --> clean and jerk --> tossing coin --> cleaning toilet\n(D) clean and jerk --> cleaning toilet --> milking cow --> tossing coin\n(E) tossing coin --> cleaning toilet --> milking cow --> clean and jerk\n(F) clean and jerk --> cleaning toilet --> tossing coin --> milking cow", "candidates": ["milking cow --> cleaning toilet --> clean and jerk --> tossing coin", "clean and jerk --> tossing coin --> cleaning toilet --> milking cow", "milking cow --> clean and jerk --> tossing coin --> cleaning toilet", "clean and jerk --> cleaning toilet --> milking cow --> tossing coin", "tossing coin --> cleaning toilet --> milking cow --> clean and jerk", "clean and jerk --> cleaning toilet --> tossing coin --> milking cow"], "answer": "F", "task_type": "order", "question_id": "AO_25", "indices": ["30", "750", "1020", "1530", "2160", "2640", "2730", "3420", "3960", "4440", "4560", "5130", "5520", "6300", "6810", "7290", "7650", "8220", "8460", "8940", "9600", "9990", "10230", "10620", "11400", "11670", "12330", "12630", "13140", "13380", "13770", "14580"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) clean and jerk --> tossing coin --> cleaning toilet --> milking cow"]], "filtered_resps": ["(B) clean and jerk --> tossing coin --> cleaning toilet --> milking cow"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a2eb408020c2f828ebd7e88ed6d81063996879614b0308d15c582726ff9cf54", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) milking cow --> cleaning toilet --> clean and jerk --> tossing coin\n(B) clean and jerk --> tossing coin --> cleaning toilet --> milking cow\n(C) milking cow --> clean and jerk --> tossing coin --> cleaning toilet\n(D) clean and jerk --> cleaning toilet --> milking cow --> tossing coin\n(E) tossing coin --> cleaning toilet --> milking cow --> clean and jerk\n(F) clean and jerk --> cleaning toilet --> tossing coin --> milking cow", "task_type": "order", "pred_answer": "B", "answer": "F"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) milking cow --> cleaning toilet --> clean and jerk --> tossing coin\n(B) clean and jerk --> tossing coin --> cleaning toilet --> milking cow\n(C) milking cow --> clean and jerk --> tossing coin --> cleaning toilet\n(D) clean and jerk --> cleaning toilet --> milking cow --> tossing coin\n(E) tossing coin --> cleaning toilet --> milking cow --> clean and jerk\n(F) clean and jerk --> cleaning toilet --> tossing coin --> milking cow\nOnly give the best option.\nBest option: ("}
{"doc_id": 104, "doc": {"video_name": "test_order_42.mp4", "duration": 5152.6, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> cleaning toilet --> playing trombone --> making jewelry\n(B) cleaning toilet --> jetskiing --> making jewelry --> playing trombone\n(C) playing trombone --> making jewelry --> cleaning toilet --> jetskiing\n(D) playing trombone --> jetskiing --> cleaning toilet --> making jewelry\n(E) jetskiing --> cleaning toilet --> making jewelry --> playing trombone\n(F) making jewelry --> cleaning toilet --> jetskiing --> playing trombone", "candidates": ["jetskiing --> cleaning toilet --> playing trombone --> making jewelry", "cleaning toilet --> jetskiing --> making jewelry --> playing trombone", "playing trombone --> making jewelry --> cleaning toilet --> jetskiing", "playing trombone --> jetskiing --> cleaning toilet --> making jewelry", "jetskiing --> cleaning toilet --> making jewelry --> playing trombone", "making jewelry --> cleaning toilet --> jetskiing --> playing trombone"], "answer": "D", "task_type": "order", "question_id": "AO_29", "indices": ["2668", "5771", "13485", "17110", "20561", "25462", "32944", "37555", "43181", "44747", "52200", "53708", "62205", "66004", "71311", "76502", "78706", "86101", "89755", "93380", "100572", "101471", "110867", "111505", "120321", "121858", "126701", "131312", "135140", "142999", "148161", "150742"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9cf1c2b2539dc2fce75873f8bc68959541e59b66c02f33e7f07c20f9fc3785d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> cleaning toilet --> playing trombone --> making jewelry\n(B) cleaning toilet --> jetskiing --> making jewelry --> playing trombone\n(C) playing trombone --> making jewelry --> cleaning toilet --> jetskiing\n(D) playing trombone --> jetskiing --> cleaning toilet --> making jewelry\n(E) jetskiing --> cleaning toilet --> making jewelry --> playing trombone\n(F) making jewelry --> cleaning toilet --> jetskiing --> playing trombone", "task_type": "order", "pred_answer": "E", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) jetskiing --> cleaning toilet --> playing trombone --> making jewelry\n(B) cleaning toilet --> jetskiing --> making jewelry --> playing trombone\n(C) playing trombone --> making jewelry --> cleaning toilet --> jetskiing\n(D) playing trombone --> jetskiing --> cleaning toilet --> making jewelry\n(E) jetskiing --> cleaning toilet --> making jewelry --> playing trombone\n(F) making jewelry --> cleaning toilet --> jetskiing --> playing trombone\nOnly give the best option.\nBest option: ("}
{"doc_id": 108, "doc": {"video_name": "test_order_6.mp4", "duration": 490.08, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> pole vault --> playing harp\n(B) tossing coin --> milking cow --> playing harp --> pole vault\n(C) tossing coin --> playing harp --> pole vault --> milking cow\n(D) tossing coin --> milking cow --> pole vault --> playing harp\n(E) pole vault --> playing harp --> tossing coin --> milking cow\n(F) pole vault --> milking cow --> tossing coin --> playing harp", "candidates": ["milking cow --> tossing coin --> pole vault --> playing harp", "tossing coin --> milking cow --> playing harp --> pole vault", "tossing coin --> playing harp --> pole vault --> milking cow", "tossing coin --> milking cow --> pole vault --> playing harp", "pole vault --> playing harp --> tossing coin --> milking cow", "pole vault --> milking cow --> tossing coin --> playing harp"], "answer": "B", "task_type": "order", "question_id": "AO_33", "indices": ["232", "725", "1276", "1711", "1943", "2552", "3016", "3393", "3915", "4176", "4930", "5017", "5800", "6148", "6612", "7308", "7511", "8178", "8294", "8758", "9396", "9686", "10382", "10962", "10991", "11629", "12064", "12354", "12876", "13543", "14094", "14616"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D)"]], "filtered_resps": ["(D)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ec9a514086dd3ef0dc9a9158874a6e826f4a6bb6d7db8eb1b830a663c14d970", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> pole vault --> playing harp\n(B) tossing coin --> milking cow --> playing harp --> pole vault\n(C) tossing coin --> playing harp --> pole vault --> milking cow\n(D) tossing coin --> milking cow --> pole vault --> playing harp\n(E) pole vault --> playing harp --> tossing coin --> milking cow\n(F) pole vault --> milking cow --> tossing coin --> playing harp", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) milking cow --> tossing coin --> pole vault --> playing harp\n(B) tossing coin --> milking cow --> playing harp --> pole vault\n(C) tossing coin --> playing harp --> pole vault --> milking cow\n(D) tossing coin --> milking cow --> pole vault --> playing harp\n(E) pole vault --> playing harp --> tossing coin --> milking cow\n(F) pole vault --> milking cow --> tossing coin --> playing harp\nOnly give the best option.\nBest option: ("}
{"doc_id": 112, "doc": {"video_name": "test_order_68.mp4", "duration": 824.42, "question": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->4->3->1\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 4->1->2->3\n(E) 3->2->1->4\n(F) 1->2->3->4", "candidates": ["2->4->3->1", "4->3->2->1", "2->1->3->4", "4->1->2->3", "3->2->1->4", "1->2->3->4"], "answer": "F", "task_type": "order", "question_id": "AO_37", "indices": ["638", "1363", "2262", "2291", "3248", "4002", "5191", "5974", "6351", "6931", "7946", "9106", "9512", "10440", "11165", "12151", "13021", "13166", "14529", "14761", "16037", "16530", "16994", "18009", "18531", "19807", "20097", "20909", "22069", "22968", "23577", "24389"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbf0c0162e27e594036184f0e90898c2d2bac44a1d2cfe20d96b09ebe8e2758d", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->4->3->1\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 4->1->2->3\n(E) 3->2->1->4\n(F) 1->2->3->4", "task_type": "order", "pred_answer": "F", "answer": "F"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A person in a red coat cleans the snow off their car; (2)The trunk of the car is lifted open; (3)A person in a tan coat cleans off the front of the car; (4)A man in a white jacket starts to clear the snow off of another car.\n(A) 2->4->3->1\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 4->1->2->3\n(E) 3->2->1->4\n(F) 1->2->3->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 116, "doc": {"video_name": "test_order_37.mp4", "duration": 463.57, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cleaning toilet --> pole vault --> shredding paper\n(B) riding mule --> pole vault --> shredding paper --> cleaning toilet\n(C) cleaning toilet --> riding mule --> shredding paper --> pole vault\n(D) riding mule --> shredding paper --> cleaning toilet --> pole vault\n(E) pole vault --> cleaning toilet --> riding mule --> shredding paper\n(F) cleaning toilet --> shredding paper --> pole vault --> riding mule", "candidates": ["riding mule --> cleaning toilet --> pole vault --> shredding paper", "riding mule --> pole vault --> shredding paper --> cleaning toilet", "cleaning toilet --> riding mule --> shredding paper --> pole vault", "riding mule --> shredding paper --> cleaning toilet --> pole vault", "pole vault --> cleaning toilet --> riding mule --> shredding paper", "cleaning toilet --> shredding paper --> pole vault --> riding mule"], "answer": "A", "task_type": "order", "question_id": "AO_41", "indices": ["30", "600", "870", "1530", "1710", "2190", "2610", "3030", "3750", "4230", "4650", "5130", "5190", "5700", "6390", "6660", "6960", "7740", "8010", "8370", "8700", "9120", "9810", "10140", "10470", "11220", "11520", "11730", "12450", "12720", "13350", "13620"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) cleaning toilet --> shredding paper --> pole vault --> riding mule"]], "filtered_resps": ["(F) cleaning toilet --> shredding paper --> pole vault --> riding mule"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a9a640e8ebf3a758c8253b64dc9a30de34c96b35f3dacd560a3b6b23a4b113c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cleaning toilet --> pole vault --> shredding paper\n(B) riding mule --> pole vault --> shredding paper --> cleaning toilet\n(C) cleaning toilet --> riding mule --> shredding paper --> pole vault\n(D) riding mule --> shredding paper --> cleaning toilet --> pole vault\n(E) pole vault --> cleaning toilet --> riding mule --> shredding paper\n(F) cleaning toilet --> shredding paper --> pole vault --> riding mule", "task_type": "order", "pred_answer": "F", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) riding mule --> cleaning toilet --> pole vault --> shredding paper\n(B) riding mule --> pole vault --> shredding paper --> cleaning toilet\n(C) cleaning toilet --> riding mule --> shredding paper --> pole vault\n(D) riding mule --> shredding paper --> cleaning toilet --> pole vault\n(E) pole vault --> cleaning toilet --> riding mule --> shredding paper\n(F) cleaning toilet --> shredding paper --> pole vault --> riding mule\nOnly give the best option.\nBest option: ("}
{"doc_id": 120, "doc": {"video_name": "test_order_67.mp4", "duration": 5970.219999999999, "question": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n(E) 2->4->1->3\n(F) 4->3->2->1", "candidates": ["3->2->1->4", "1->2->3->4", "2->1->3->4", "1->3->4->2", "2->4->1->3", "4->3->2->1"], "answer": "B", "task_type": "order", "question_id": "AO_45", "indices": ["105937", "105966", "106024", "106053", "106082", "106633", "106894", "106923", "106981", "107010", "107039", "107271", "107358", "107387", "107416", "107822", "107938", "107967", "107996", "108083", "108286", "109069", "111012", "111041", "111070", "111186", "111331", "111360", "111389", "111418", "111447", "111476"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c8133ab708ba4faeb27daffe3371476e26670fe0a9ea953b36089975ef09e9eb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n(E) 2->4->1->3\n(F) 4->3->2->1", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The boy begins hopping on the squares, starting from his driveway; (2)The girl joins him near the sidewalk and walks along his side as he hops across the squares; (3)He hops till he reaches the end of the sidewalk which marks the end of the hopscotch squares; (4)After he's done hopping he smiles and begins walking back.\n(A) 3->2->1->4\n(B) 1->2->3->4\n(C) 2->1->3->4\n(D) 1->3->4->2\n(E) 2->4->1->3\n(F) 4->3->2->1\nOnly give the best option.\nBest option: ("}
{"doc_id": 124, "doc": {"video_name": "test_order_64.mp4", "duration": 1317.81, "question": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->2->1->4\n(F) 1->4->3->2", "candidates": ["3->4->1->2", "2->1->3->4", "1->2->3->4", "4->3->2->1", "3->2->1->4", "1->4->3->2"], "answer": "B", "task_type": "order", "question_id": "AO_49", "indices": ["775", "1400", "2200", "3300", "4250", "5800", "7125", "7175", "8375", "10075", "10325", "11875", "12775", "14025", "14725", "16000", "17000", "17050", "17075", "17150", "17175", "17600", "17625", "17700", "17750", "17850", "17900", "17925", "17950", "17975", "18650", "18800"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad5d9f57573c575cb160f9529ae2e1f8631defd61b500860d74f2ee8ecd211d1", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->2->1->4\n(F) 1->4->3->2", "task_type": "order", "pred_answer": "E", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man melts the wax with the tool and wipes the ski; (2)The man adds a substance from a jug to the ski and wipes it with a paper towel; (3)The man exchanges skis and waxes the second one with the tool; (4)The man then scrapes the wax off the ski and uses a different tools and paper towel on the ski.\n(A) 3->4->1->2\n(B) 2->1->3->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->2->1->4\n(F) 1->4->3->2\nOnly give the best option.\nBest option: ("}
{"doc_id": 128, "doc": {"video_name": "test_order_5.mp4", "duration": 667.34, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> paragliding --> abseiling\n(B) abseiling --> jetskiing --> paragliding --> shredding paper\n(C) paragliding --> shredding paper --> abseiling --> jetskiing\n(D) abseiling --> shredding paper --> jetskiing --> paragliding\n(E) jetskiing --> paragliding --> abseiling --> shredding paper\n(F) shredding paper --> jetskiing --> abseiling --> paragliding", "candidates": ["jetskiing --> shredding paper --> paragliding --> abseiling", "abseiling --> jetskiing --> paragliding --> shredding paper", "paragliding --> shredding paper --> abseiling --> jetskiing", "abseiling --> shredding paper --> jetskiing --> paragliding", "jetskiing --> paragliding --> abseiling --> shredding paper", "shredding paper --> jetskiing --> abseiling --> paragliding"], "answer": "F", "task_type": "order", "question_id": "AO_53", "indices": ["300", "960", "1260", "1890", "2820", "3210", "4320", "4350", "5310", "6120", "6720", "6870", "7740", "8280", "9090", "9600", "10020", "11160", "11490", "12420", "12930", "13680", "14310", "14610", "15270", "16020", "16560", "17160", "18060", "18450", "19230", "19500"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f3bb90138c9485717973fe1d92b1787f1fa5022c650999ec894127088a939d86", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> paragliding --> abseiling\n(B) abseiling --> jetskiing --> paragliding --> shredding paper\n(C) paragliding --> shredding paper --> abseiling --> jetskiing\n(D) abseiling --> shredding paper --> jetskiing --> paragliding\n(E) jetskiing --> paragliding --> abseiling --> shredding paper\n(F) shredding paper --> jetskiing --> abseiling --> paragliding", "task_type": "order", "pred_answer": "E", "answer": "F"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> shredding paper --> paragliding --> abseiling\n(B) abseiling --> jetskiing --> paragliding --> shredding paper\n(C) paragliding --> shredding paper --> abseiling --> jetskiing\n(D) abseiling --> shredding paper --> jetskiing --> paragliding\n(E) jetskiing --> paragliding --> abseiling --> shredding paper\n(F) shredding paper --> jetskiing --> abseiling --> paragliding\nOnly give the best option.\nBest option: ("}
{"doc_id": 132, "doc": {"video_name": "test_order_65.mp4", "duration": 672.36, "question": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n(E) 3->1->2->4\n(F) 1->3->2->4", "candidates": ["2->1->3->4", "2->3->1->4", "3->2->4->1", "1->2->3->4", "3->1->2->4", "1->3->2->4"], "answer": "D", "task_type": "order", "question_id": "AO_57", "indices": ["0", "630", "1350", "2340", "3000", "3420", "4380", "4920", "5280", "6240", "6780", "6990", "8070", "8400", "8910", "9990", "10140", "11220", "11340", "12060", "13140", "13230", "13860", "15090", "15150", "15750", "16740", "17580", "18240", "18810", "19020", "19530"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6463f0b02d0663479e83c434423592f9c07b8a091268373baa18827fc127bc8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n(E) 3->1->2->4\n(F) 1->3->2->4", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A guy approaches a weight on a stage; (2)A man massages a guy's shoulders; (3)A guy lifts a weight on a stage and releases it; (4)A guy kisses the weight plates.\n(A) 2->1->3->4\n(B) 2->3->1->4\n(C) 3->2->4->1\n(D) 1->2->3->4\n(E) 3->1->2->4\n(F) 1->3->2->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 136, "doc": {"video_name": "test_order_18.mp4", "duration": 767.2199999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> baking cookies --> playing trombone --> milking cow\n(B) making jewelry --> milking cow --> playing trombone --> baking cookies\n(C) playing trombone --> milking cow --> baking cookies --> making jewelry\n(D) milking cow --> baking cookies --> playing trombone --> making jewelry\n(E) baking cookies --> milking cow --> making jewelry --> playing trombone\n(F) making jewelry --> playing trombone --> baking cookies --> milking cow", "candidates": ["making jewelry --> baking cookies --> playing trombone --> milking cow", "making jewelry --> milking cow --> playing trombone --> baking cookies", "playing trombone --> milking cow --> baking cookies --> making jewelry", "milking cow --> baking cookies --> playing trombone --> making jewelry", "baking cookies --> milking cow --> making jewelry --> playing trombone", "making jewelry --> playing trombone --> baking cookies --> milking cow"], "answer": "F", "task_type": "order", "question_id": "AO_61", "indices": ["360", "840", "1890", "2250", "3300", "3630", "4590", "5610", "6240", "7020", "7380", "8370", "8640", "9570", "10230", "11220", "11700", "12390", "13260", "14010", "14520", "15600", "16230", "16830", "17430", "18300", "18990", "19680", "20580", "20850", "22050", "22890"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) making jewelry --> playing trombone --> baking cookies --> milking cow"]], "filtered_resps": ["(F) making jewelry --> playing trombone --> baking cookies --> milking cow"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b8d76ac95cf6ef0ae7a91a24d732b977f425e13d71f78476403f647fc67b646", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> baking cookies --> playing trombone --> milking cow\n(B) making jewelry --> milking cow --> playing trombone --> baking cookies\n(C) playing trombone --> milking cow --> baking cookies --> making jewelry\n(D) milking cow --> baking cookies --> playing trombone --> making jewelry\n(E) baking cookies --> milking cow --> making jewelry --> playing trombone\n(F) making jewelry --> playing trombone --> baking cookies --> milking cow", "task_type": "order", "pred_answer": "F", "answer": "F"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) making jewelry --> baking cookies --> playing trombone --> milking cow\n(B) making jewelry --> milking cow --> playing trombone --> baking cookies\n(C) playing trombone --> milking cow --> baking cookies --> making jewelry\n(D) milking cow --> baking cookies --> playing trombone --> making jewelry\n(E) baking cookies --> milking cow --> making jewelry --> playing trombone\n(F) making jewelry --> playing trombone --> baking cookies --> milking cow\nOnly give the best option.\nBest option: ("}
{"doc_id": 140, "doc": {"video_name": "test_order_40.mp4", "duration": 712.41, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> pole vault --> abseiling --> tossing coin\n(B) pole vault --> tossing coin --> paragliding --> abseiling\n(C) pole vault --> paragliding --> tossing coin --> abseiling\n(D) paragliding --> pole vault --> tossing coin --> abseiling\n(E) abseiling --> pole vault --> tossing coin --> paragliding\n(F) pole vault --> abseiling --> paragliding --> tossing coin", "candidates": ["paragliding --> pole vault --> abseiling --> tossing coin", "pole vault --> tossing coin --> paragliding --> abseiling", "pole vault --> paragliding --> tossing coin --> abseiling", "paragliding --> pole vault --> tossing coin --> abseiling", "abseiling --> pole vault --> tossing coin --> paragliding", "pole vault --> abseiling --> paragliding --> tossing coin"], "answer": "A", "task_type": "order", "question_id": "AO_65", "indices": ["0", "957", "1943", "2523", "3074", "3364", "4640", "5249", "5336", "6612", "6728", "7714", "8323", "8787", "9454", "10643", "10904", "11571", "12528", "12702", "13485", "14065", "15254", "15428", "16327", "17168", "17429", "18096", "19140", "19546", "20503", "20677"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["The correct answer is:\n\n(F) pole vault --> abseiling --> parag"]], "filtered_resps": ["The correct answer is:\n\n(F) pole vault --> abseiling --> parag"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71d2fe22fe8a9b36a180384de2b566ab73e788f26ed63b2658759f92b58a0217", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> pole vault --> abseiling --> tossing coin\n(B) pole vault --> tossing coin --> paragliding --> abseiling\n(C) pole vault --> paragliding --> tossing coin --> abseiling\n(D) paragliding --> pole vault --> tossing coin --> abseiling\n(E) abseiling --> pole vault --> tossing coin --> paragliding\n(F) pole vault --> abseiling --> paragliding --> tossing coin", "task_type": "order", "pred_answer": "F", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) paragliding --> pole vault --> abseiling --> tossing coin\n(B) pole vault --> tossing coin --> paragliding --> abseiling\n(C) pole vault --> paragliding --> tossing coin --> abseiling\n(D) paragliding --> pole vault --> tossing coin --> abseiling\n(E) abseiling --> pole vault --> tossing coin --> paragliding\n(F) pole vault --> abseiling --> paragliding --> tossing coin\nOnly give the best option.\nBest option: ("}
{"doc_id": 144, "doc": {"video_name": "test_order_33.mp4", "duration": 558.54, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> pole vault --> playing harp --> playing trombone\n(B) playing harp --> jetskiing --> pole vault --> playing trombone\n(C) playing trombone --> playing harp --> pole vault --> jetskiing\n(D) pole vault --> playing trombone --> playing harp --> jetskiing\n(E) playing harp --> pole vault --> playing trombone --> jetskiing\n(F) playing trombone --> jetskiing --> playing harp --> pole vault", "candidates": ["jetskiing --> pole vault --> playing harp --> playing trombone", "playing harp --> jetskiing --> pole vault --> playing trombone", "playing trombone --> playing harp --> pole vault --> jetskiing", "pole vault --> playing trombone --> playing harp --> jetskiing", "playing harp --> pole vault --> playing trombone --> jetskiing", "playing trombone --> jetskiing --> playing harp --> pole vault"], "answer": "B", "task_type": "order", "question_id": "AO_69", "indices": ["60", "840", "1050", "2010", "2400", "2610", "3480", "3810", "4560", "4830", "5670", "5730", "6360", "6780", "7680", "8220", "8760", "9360", "9570", "10080", "10500", "11310", "11940", "12180", "12600", "13080", "13710", "14400", "14730", "15480", "15930", "16290"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "befb8690d49afee0fc35e5251bfbad8ce3be09d65fcf50cf40a37e0894e46be2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> pole vault --> playing harp --> playing trombone\n(B) playing harp --> jetskiing --> pole vault --> playing trombone\n(C) playing trombone --> playing harp --> pole vault --> jetskiing\n(D) pole vault --> playing trombone --> playing harp --> jetskiing\n(E) playing harp --> pole vault --> playing trombone --> jetskiing\n(F) playing trombone --> jetskiing --> playing harp --> pole vault", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events shown in the video. The", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> pole vault --> playing harp --> playing trombone\n(B) playing harp --> jetskiing --> pole vault --> playing trombone\n(C) playing trombone --> playing harp --> pole vault --> jetskiing\n(D) pole vault --> playing trombone --> playing harp --> jetskiing\n(E) playing harp --> pole vault --> playing trombone --> jetskiing\n(F) playing trombone --> jetskiing --> playing harp --> pole vault\nOnly give the best option.\nBest option: ("}
{"doc_id": 148, "doc": {"video_name": "test_needle_30.mp4", "duration": 459.13, "question": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Jumping jacks\n(B) Jumping rope\n(C) Squats\n(D) Pull-ups\n(E) Sit-ups\n(F) Push-ups", "candidates": ["Jumping jacks", "Jumping rope", "Squats", "Pull-ups", "Sit-ups", "Push-ups"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_3", "indices": ["348", "464", "841", "1392", "2088", "2349", "2668", "3045", "3567", "4031", "4466", "4727", "5278", "5655", "6264", "6815", "7163", "7279", "7975", "8265", "8584", "9309", "9715", "10005", "10643", "10701", "11484", "11571", "12064", "12789", "13253", "13485"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f99521e73cdf33ed931c784a88cf56024e425e8097bc4534b5e411cada2af818", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Jumping jacks\n(B) Jumping rope\n(C) Squats\n(D) Pull-ups\n(E) Sit-ups\n(F) Push-ups", "task_type": "needleQA", "pred_answer": "A", "answer": "A"}, "input": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Jumping jacks\n(B) Jumping rope\n(C) Squats\n(D) Pull-ups\n(E) Sit-ups\n(F) Push-ups\nOnly give the best option.\nBest option: ("}
{"doc_id": 152, "doc": {"video_name": "test_needle_9.mp4", "duration": 457.02000000000004, "question": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) RICE\n(C) VEGETABLE\n(D) FOOD\n(E) FRESH\n(F) SWEET", "candidates": ["MEAT", "RICE", "VEGETABLE", "FOOD", "FRESH", "SWEET"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_7", "indices": ["200", "625", "900", "1100", "1600", "2100", "2350", "2600", "3100", "3400", "3875", "4100", "4275", "4875", "5150", "5625", "6025", "6175", "6425", "7100", "7200", "7475", "8150", "8525", "8550", "9225", "9275", "9675", "10300", "10325", "10950", "11375"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fd57565169f1fa09e0a253814831a3cee8f79acceb4dd81e1b3d026db88142c2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) RICE\n(C) VEGETABLE\n(D) FOOD\n(E) FRESH\n(F) SWEET", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) RICE\n(C) VEGETABLE\n(D) FOOD\n(E) FRESH\n(F) SWEET\nOnly give the best option.\nBest option: ("}
{"doc_id": 156, "doc": {"video_name": "test_needle_25.mp4", "duration": 6979.45, "question": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Singing\n(B) Playing football\n(C) Dancing\n(D) Playing games\n(E) Eating\n(F) Doing nail art", "candidates": ["Singing", "Playing football", "Dancing", "Playing games", "Eating", "Doing nail art"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_11", "indices": ["17516", "46864", "73689", "73718", "73747", "73776", "73805", "73834", "73863", "73892", "73921", "73950", "73979", "74008", "74066", "74095", "74124", "74153", "74182", "74211", "74240", "74269", "74298", "74327", "74356", "74385", "74414", "74443", "74472", "74501", "74530", "75081"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc4e89b069cf333931db39009093a67689a414cde737126f852541032cf27ace", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Singing\n(B) Playing football\n(C) Dancing\n(D) Playing games\n(E) Eating\n(F) Doing nail art", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Singing\n(B) Playing football\n(C) Dancing\n(D) Playing games\n(E) Eating\n(F) Doing nail art\nOnly give the best option.\nBest option: ("}
{"doc_id": 160, "doc": {"video_name": "test_needle_12.mp4", "duration": 7589.6900000000005, "question": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Playing football\n(C) Fighting\n(D) Hugging\n(E) Dancing\n(F) Arguing", "candidates": ["Singing", "Playing football", "Fighting", "Hugging", "Dancing", "Arguing"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_15", "indices": ["47075", "49875", "113250", "113300", "113325", "113475", "113500", "113525", "113575", "113625", "113650", "113675", "113725", "113750", "136250", "136275", "136300", "136325", "136350", "136375", "136400", "136425", "174050", "174075", "174100", "174125", "174150", "174175", "174300", "174350", "174425", "174450"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b1e61216b7d457d0f67dac887152acd75abd3b9e22c2154947307d8d5359216", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Playing football\n(C) Fighting\n(D) Hugging\n(E) Dancing\n(F) Arguing", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Playing football\n(C) Fighting\n(D) Hugging\n(E) Dancing\n(F) Arguing\nOnly give the best option.\nBest option: ("}
{"doc_id": 164, "doc": {"video_name": "test_needle_28.mp4", "duration": 546.4, "question": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Bucket\n(B) Soldier sculpture\n(C) Stone\n(D) Lightning rod\n(E) National flag\n(F) Wooden stick", "candidates": ["Bucket", "Soldier sculpture", "Stone", "Lightning rod", "National flag", "Wooden stick"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_19", "indices": ["275", "450", "1225", "1675", "1950", "2250", "2950", "3375", "3400", "4200", "4500", "4750", "5350", "5925", "6125", "6475", "7025", "7250", "7800", "8200", "8825", "9275", "9475", "10000", "10625", "10700", "11125", "11850", "12125", "12750", "13125", "13325"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a198ac428faae7a6b4089e84b32e601976664ea8d0baa3ce4f04d187f85f5b8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Bucket\n(B) Soldier sculpture\n(C) Stone\n(D) Lightning rod\n(E) National flag\n(F) Wooden stick", "task_type": "needleQA", "pred_answer": "D", "answer": "B"}, "input": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Bucket\n(B) Soldier sculpture\n(C) Stone\n(D) Lightning rod\n(E) National flag\n(F) Wooden stick\nOnly give the best option.\nBest option: ("}
{"doc_id": 168, "doc": {"video_name": "test_needle_52.mp4", "duration": 618.14, "question": "In the collision event of objects in the video, what shape is the moving metal object when the cyan object enters the scene?\n(A) sphere\n(B) cylinder\n(C) cone\n(D) prism\n(E) pyramid\n(F) cube", "candidates": ["sphere", "cylinder", "cone", "prism", "pyramid", "cube"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_23", "indices": ["200", "700", "1125", "1650", "2025", "2700", "3275", "3475", "4300", "4350", "5025", "5750", "5775", "6675", "7050", "7425", "7900", "8200", "8825", "9300", "10075", "10575", "10975", "11550", "11925", "12050", "12850", "13300", "13925", "14450", "14475", "15350"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "628bd40cc04e034b1c376ab88a19362fbc3b536b0701ad6b83564c32c018c93d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what shape is the moving metal object when the cyan object enters the scene?\n(A) sphere\n(B) cylinder\n(C) cone\n(D) prism\n(E) pyramid\n(F) cube", "task_type": "needleQA", "pred_answer": "B", "answer": "A"}, "input": "In the collision event of objects in the video, what shape is the moving metal object when the cyan object enters the scene?\n(A) sphere\n(B) cylinder\n(C) cone\n(D) prism\n(E) pyramid\n(F) cube\nOnly give the best option.\nBest option: ("}
{"doc_id": 172, "doc": {"video_name": "test_needle_57.mp4", "duration": 455.12, "question": "In the collision event of objects in the video, what is the material of the last object to collide with the rubber cube?\n(A) rubber\n(B) wood\n(C) glass\n(D) plastic\n(E) ceramic\n(F) metal", "candidates": ["rubber", "wood", "glass", "plastic", "ceramic", "metal"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_27", "indices": ["150", "475", "800", "1375", "1650", "1950", "2100", "2750", "3150", "3175", "3550", "3875", "4350", "4925", "4950", "5425", "6000", "6025", "6675", "6750", "7350", "7450", "8125", "8150", "8600", "9000", "9550", "9700", "10200", "10625", "10825", "11150"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a5bde315e32b2e25324ebc7e93f07065880b092b1ec019c7d921e8dacccc2472", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what is the material of the last object to collide with the rubber cube?\n(A) rubber\n(B) wood\n(C) glass\n(D) plastic\n(E) ceramic\n(F) metal", "task_type": "needleQA", "pred_answer": "F", "answer": "A"}, "input": "In the collision event of objects in the video, what is the material of the last object to collide with the rubber cube?\n(A) rubber\n(B) wood\n(C) glass\n(D) plastic\n(E) ceramic\n(F) metal\nOnly give the best option.\nBest option: ("}
{"doc_id": 176, "doc": {"video_name": "test_needle_35.mp4", "duration": 652.5, "question": "What is the pink toy the child is playing with in the video?\n(A) Rabbit\n(B) Birthday cake\n(C) Dinosaur\n(D) Cat\n(E) Dog\n(F) Car", "candidates": ["Rabbit", "Birthday cake", "Dinosaur", "Cat", "Dog", "Car"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_31", "indices": ["116", "1102", "1653", "2030", "3016", "3364", "4234", "4669", "4872", "5945", "6119", "6757", "7859", "7975", "9048", "9251", "10353", "10701", "11078", "12035", "12760", "13398", "13427", "14123", "14790", "15283", "16385", "16994", "17255", "18154", "18879", "19401"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f2e2073f83c434f5a8d78e5d12f2708992d6b184271a3304beefaac8a40a356", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the pink toy the child is playing with in the video?\n(A) Rabbit\n(B) Birthday cake\n(C) Dinosaur\n(D) Cat\n(E) Dog\n(F) Car", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "What is the pink toy the child is playing with in the video?\n(A) Rabbit\n(B) Birthday cake\n(C) Dinosaur\n(D) Cat\n(E) Dog\n(F) Car\nOnly give the best option.\nBest option: ("}
{"doc_id": 180, "doc": {"video_name": "test_needle_24.mp4", "duration": 479.94, "question": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Playing football\n(B) Singing\n(C) Playing games\n(D) Doing nail art\n(E) Eating\n(F) Dancing", "candidates": ["Playing football", "Singing", "Playing games", "Doing nail art", "Eating", "Dancing"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_35", "indices": ["6032", "6061", "6090", "6119", "6148", "6177", "6206", "6235", "6264", "6293", "6322", "6351", "6380", "6409", "6438", "6467", "6496", "6525", "6554", "6583", "6612", "6641", "6670", "6699", "6728", "6757", "6786", "6815", "6844", "6873", "6902", "13804"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01fdfe4848ef3f18b8098eb2420f6e66c9b5aa9c131b0f5c5f20034d86d729cf", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Playing football\n(B) Singing\n(C) Playing games\n(D) Doing nail art\n(E) Eating\n(F) Dancing", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the video, what is the long-haired woman in a black skirt doing?\n(A) Playing football\n(B) Singing\n(C) Playing games\n(D) Doing nail art\n(E) Eating\n(F) Dancing\nOnly give the best option.\nBest option: ("}
{"doc_id": 184, "doc": {"video_name": "test_needle_6.mp4", "duration": 303.76, "question": "What are the two women wearing black vests and black pants doing in the video?\n(A) Singing\n(B) Arguing\n(C) Playing football\n(D) Dancing\n(E) Using a mobile phone\n(F) Playing on the computer", "candidates": ["Singing", "Arguing", "Playing football", "Dancing", "Using a mobile phone", "Playing on the computer"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_39", "indices": ["232", "464", "725", "870", "1334", "1624", "1885", "2001", "2320", "2552", "3045", "3277", "3451", "3915", "4031", "4408", "4756", "4785", "5191", "5568", "5684", "5916", "6264", "6554", "6989", "7163", "7453", "7627", "8062", "8352", "8584", "8961"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "465cb76f72b68ccdf8292fce52c13c6129b8fbfcc7ced8aa6804c715d1f73821", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What are the two women wearing black vests and black pants doing in the video?\n(A) Singing\n(B) Arguing\n(C) Playing football\n(D) Dancing\n(E) Using a mobile phone\n(F) Playing on the computer", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "What are the two women wearing black vests and black pants doing in the video?\n(A) Singing\n(B) Arguing\n(C) Playing football\n(D) Dancing\n(E) Using a mobile phone\n(F) Playing on the computer\nOnly give the best option.\nBest option: ("}
{"doc_id": 188, "doc": {"video_name": "test_needle_48.mp4", "duration": 767.14, "question": "In the collision event of objects in the video, which of the following is not responsible for the yellow cylinder's colliding with the purple cylinder?\n(A) the angle of collision\n(B) the friction between surfaces\n(C) the purple cylinder's position\n(D) the force applied to the yellow cylinder\n(E) the yellow cylinder's initial velocity\n(F) the presence of the metal cylinder", "candidates": ["the angle of collision", "the friction between surfaces", "the purple cylinder's position", "the force applied to the yellow cylinder", "the yellow cylinder's initial velocity", "the presence of the metal cylinder"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_43", "indices": ["475", "625", "1350", "2150", "2875", "3050", "4100", "4550", "5300", "5675", "6125", "6850", "7525", "8075", "8775", "9475", "9600", "10450", "11275", "11425", "12325", "12600", "13525", "14000", "14950", "15450", "15650", "16725", "17200", "17800", "18550", "18800"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d40438a41e92db9d8fedede3d57f0a1728fcd5d7fa642a35de74c81bb9ba1460", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, which of the following is not responsible for the yellow cylinder's colliding with the purple cylinder?\n(A) the angle of collision\n(B) the friction between surfaces\n(C) the purple cylinder's position\n(D) the force applied to the yellow cylinder\n(E) the yellow cylinder's initial velocity\n(F) the presence of the metal cylinder", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the collision event of objects in the video, which of the following is not responsible for the yellow cylinder's colliding with the purple cylinder?\n(A) the angle of collision\n(B) the friction between surfaces\n(C) the purple cylinder's position\n(D) the force applied to the yellow cylinder\n(E) the yellow cylinder's initial velocity\n(F) the presence of the metal cylinder\nOnly give the best option.\nBest option: ("}
{"doc_id": 192, "doc": {"video_name": "test_needle_37.mp4", "duration": 613.6400000000001, "question": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Beverage\n(B) Mobile phone\n(C) Apple\n(D) Laptop\n(E) Tablet computer\n(F) Paper materials", "candidates": ["Beverage", "Mobile phone", "Apple", "Laptop", "Tablet computer", "Paper materials"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_47", "indices": ["174", "957", "1305", "1798", "2639", "3306", "3509", "4553", "4901", "5278", "5713", "6293", "7424", "7453", "8584", "8758", "9222", "9889", "10556", "11455", "11803", "12499", "13050", "13340", "14094", "14819", "15225", "16008", "16182", "16733", "17313", "17806"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8caf71ac3e88ed8cf0c7a2bc71f3ba42230433714a7b2e70665b624eed69c08a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Beverage\n(B) Mobile phone\n(C) Apple\n(D) Laptop\n(E) Tablet computer\n(F) Paper materials", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Beverage\n(B) Mobile phone\n(C) Apple\n(D) Laptop\n(E) Tablet computer\n(F) Paper materials\nOnly give the best option.\nBest option: ("}
{"doc_id": 196, "doc": {"video_name": "test_needle_2.mp4", "duration": 5493.71, "question": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Neutral\n(B) Afraid\n(C) Happy\n(D) Nervous\n(E) Sad\n(F) Excited", "candidates": ["Neutral", "Afraid", "Happy", "Nervous", "Sad", "Excited"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_51", "indices": ["107822", "107851", "109591", "109678", "109823", "109852", "110171", "110867", "110896", "111766", "112201", "112433", "112462", "114028", "114057", "114086", "114115", "114144", "114173", "114202", "114231", "114260", "114289", "114318", "114347", "114376", "114405", "114434", "114463", "114492", "127020", "127513"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Happy"]], "filtered_resps": ["(C) Happy"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "73bb4d6deb44c9264547547c9f811232625dbefde538ac7b0d40ef014f38b13d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Neutral\n(B) Afraid\n(C) Happy\n(D) Nervous\n(E) Sad\n(F) Excited", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Neutral\n(B) Afraid\n(C) Happy\n(D) Nervous\n(E) Sad\n(F) Excited\nOnly give the best option.\nBest option: ("}
{"doc_id": 200, "doc": {"video_name": "test_needle_32.mp4", "duration": 569.03, "question": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Talking\n(B) Playing with a phone\n(C) Writing\n(D) Eating\n(E) Drinking coffee\n(F) Drawing", "candidates": ["Talking", "Playing with a phone", "Writing", "Eating", "Drinking coffee", "Drawing"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_55", "indices": ["0", "754", "1305", "2001", "2117", "3074", "3393", "3828", "4756", "4959", "5423", "6264", "6496", "7424", "7772", "8381", "8845", "9048", "9686", "10237", "10643", "11629", "12122", "12673", "12847", "13630", "14007", "14471", "14935", "15631", "15979", "16820"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d25bb86a458ad4a24b8a7108bbf22e92178bf15fbff1a7b5659d830e6c16dd87", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Talking\n(B) Playing with a phone\n(C) Writing\n(D) Eating\n(E) Drinking coffee\n(F) Drawing", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Talking\n(B) Playing with a phone\n(C) Writing\n(D) Eating\n(E) Drinking coffee\n(F) Drawing\nOnly give the best option.\nBest option: ("}
{"doc_id": 204, "doc": {"video_name": "test_needle_26.mp4", "duration": 473.29999999999995, "question": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 3\n(B) 2\n(C) 1\n(D) 5\n(E) 6\n(F) 4", "candidates": ["3", "2", "1", "5", "6", "4"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_59", "indices": ["232", "464", "899", "1566", "2117", "2436", "3045", "3480", "3857", "4031", "4756", "5046", "5684", "5974", "6235", "6960", "7482", "7685", "8265", "8410", "9048", "9570", "9744", "10295", "11020", "11107", "11861", "11948", "12528", "13137", "13340", "14036"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a71ce6ef6fd1472769f7fefc264581063044e620bf97c02670df84ba157fd6ea", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 3\n(B) 2\n(C) 1\n(D) 5\n(E) 6\n(F) 4", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 3\n(B) 2\n(C) 1\n(D) 5\n(E) 6\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 208, "doc": {"video_name": "test_entv_25.mp4", "duration": 595.179419114368, "question": "In the video, what did the woman take out of the refrigerator?\n(A) A dragon fruit\n(B) An eggplant\n(C) A bottle of water\n(D) A bottle of beer\n(E) An apple\n(F) A banana", "candidates": ["A dragon fruit", "An eggplant", "A bottle of water", "A bottle of beer", "An apple", "A banana"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_3", "indices": ["414", "437", "1288", "1541", "1978", "2645", "2668", "3542", "3979", "4025", "4485", "5037", "5566", "5773", "6325", "6785", "7222", "7774", "8280", "8717", "9246", "9591", "9821", "10235", "10971", "11224", "11983", "12098", "12673", "13317", "13547", "13869"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59c65cc8bef5d7c46b74b8effccaf8eec8a6f51deb9b729384bffdeb38bc839f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what did the woman take out of the refrigerator?\n(A) A dragon fruit\n(B) An eggplant\n(C) A bottle of water\n(D) A bottle of beer\n(E) An apple\n(F) A banana", "task_type": "plotQA", "pred_answer": "F", "answer": "C"}, "input": "In the video, what did the woman take out of the refrigerator?\n(A) A dragon fruit\n(B) An eggplant\n(C) A bottle of water\n(D) A bottle of beer\n(E) An apple\n(F) A banana\nOnly give the best option.\nBest option: ("}
{"doc_id": 212, "doc": {"video_name": "test_movie101_97.mp4", "duration": 556.0, "question": "How did the robbers intercept the police car in the video?\n(A) Faked a traffic accident\n(B) Shot with a gun\n(C) Detonated a smoke bomb\n(D) Took hostages\n(E) Detonated a bomb\n(F) Set up a roadblock", "candidates": ["Faked a traffic accident", "Shot with a gun", "Detonated a smoke bomb", "Took hostages", "Detonated a bomb", "Set up a roadblock"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_7", "indices": ["0", "475", "1100", "1275", "2100", "2425", "2950", "3225", "3675", "4000", "4425", "4750", "5425", "5875", "6450", "6625", "7150", "7625", "7875", "8350", "9075", "9275", "9925", "10100", "10525", "11100", "11425", "11700", "12550", "12600", "13050", "13775"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5cc1ba5b2abe1b1cc68aba90829b5f31179215c8edea0cf6f48dce089f55d753", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How did the robbers intercept the police car in the video?\n(A) Faked a traffic accident\n(B) Shot with a gun\n(C) Detonated a smoke bomb\n(D) Took hostages\n(E) Detonated a bomb\n(F) Set up a roadblock", "task_type": "plotQA", "pred_answer": "E", "answer": "E"}, "input": "How did the robbers intercept the police car in the video?\n(A) Faked a traffic accident\n(B) Shot with a gun\n(C) Detonated a smoke bomb\n(D) Took hostages\n(E) Detonated a bomb\n(F) Set up a roadblock\nOnly give the best option.\nBest option: ("}
{"doc_id": 216, "doc": {"video_name": "test_movie101_94.mp4", "duration": 439.0, "question": "What is the man in the black hat sitting on in the video?\n(A) Wheelchair\n(B) Chair\n(C) Floor\n(D) Sofa\n(E) Rock\n(F) Stool", "candidates": ["Wheelchair", "Chair", "Floor", "Sofa", "Rock", "Stool"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_11", "indices": ["150", "525", "825", "1250", "1525", "1775", "2350", "2625", "2850", "3075", "3725", "3750", "4250", "4650", "5000", "5275", "5625", "5950", "6450", "6750", "7100", "7375", "7625", "7900", "8475", "8750", "8925", "9250", "9875", "9975", "10600", "10900"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a0b466c0caff7223738ea8a4b24aaceade32b51932b859353ff77e63aa22c2e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the man in the black hat sitting on in the video?\n(A) Wheelchair\n(B) Chair\n(C) Floor\n(D) Sofa\n(E) Rock\n(F) Stool", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What is the man in the black hat sitting on in the video?\n(A) Wheelchair\n(B) Chair\n(C) Floor\n(D) Sofa\n(E) Rock\n(F) Stool\nOnly give the best option.\nBest option: ("}
{"doc_id": 220, "doc": {"video_name": "test_cartoon_14.mp4", "duration": 453.5, "question": "What did the kitten use to hit the big cat's butt?\n(A) Wooden plank\n(B) Cannonball\n(C) Gun\n(D) Stone\n(E) Stick\n(F) Hand", "candidates": ["Wooden plank", "Cannonball", "Gun", "Stone", "Stick", "Hand"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_15", "indices": ["232", "725", "1131", "1363", "2001", "2262", "2842", "3161", "3451", "4176", "4611", "4930", "5075", "5771", "6264", "6380", "6931", "7337", "7743", "8062", "8816", "9222", "9512", "9976", "10469", "10759", "11136", "11426", "12064", "12441", "12963", "13166"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "55782f36ed1abb211a28704158e5f13cdf2993bb78d5c68ad6ee384975f6e4e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did the kitten use to hit the big cat's butt?\n(A) Wooden plank\n(B) Cannonball\n(C) Gun\n(D) Stone\n(E) Stick\n(F) Hand", "task_type": "plotQA", "pred_answer": "A", "answer": "A"}, "input": "What did the kitten use to hit the big cat's butt?\n(A) Wooden plank\n(B) Cannonball\n(C) Gun\n(D) Stone\n(E) Stick\n(F) Hand\nOnly give the best option.\nBest option: ("}
{"doc_id": 224, "doc": {"video_name": "test_cartoon_21.mp4", "duration": 662.3029651724227, "question": "Why was the turtle carrying the octopus when escaping the cave?\n(A) The octopus was unconscious\n(B) The octopus was sick\n(C) The octopus was too tired\n(D) The octopus didn't want to leave\n(E) The octopus was injured\n(F) The octopus couldn't swim", "candidates": ["The octopus was unconscious", "The octopus was sick", "The octopus was too tired", "The octopus didn't want to leave", "The octopus was injured", "The octopus couldn't swim"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_19", "indices": ["400", "850", "1325", "2000", "2400", "2675", "3075", "3775", "4525", "4800", "5275", "5925", "6425", "7175", "7475", "7875", "8650", "9250", "9700", "9875", "10375", "11100", "11425", "12075", "12450", "13125", "13800", "14100", "14575", "15150", "15575", "16250"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f1a483b43f827dee28d4ddce72a726436fb7a07ebb81f18359dbbdd24a65c22", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why was the turtle carrying the octopus when escaping the cave?\n(A) The octopus was unconscious\n(B) The octopus was sick\n(C) The octopus was too tired\n(D) The octopus didn't want to leave\n(E) The octopus was injured\n(F) The octopus couldn't swim", "task_type": "plotQA", "pred_answer": "F", "answer": "A"}, "input": "Why was the turtle carrying the octopus when escaping the cave?\n(A) The octopus was unconscious\n(B) The octopus was sick\n(C) The octopus was too tired\n(D) The octopus didn't want to leave\n(E) The octopus was injured\n(F) The octopus couldn't swim\nOnly give the best option.\nBest option: ("}
{"doc_id": 228, "doc": {"video_name": "test_cartoon_17.mp4", "duration": 400.58, "question": "At the end of the video, what are the cat and two mice doing?\n(A) Playing soccer\n(B) Playing badminton\n(C) Dancing\n(D) Playing volleyball\n(E) Playing basketball\n(F) Fencing", "candidates": ["Playing soccer", "Playing badminton", "Dancing", "Playing volleyball", "Playing basketball", "Fencing"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_23", "indices": ["203", "406", "754", "1363", "1624", "2001", "2465", "2958", "3045", "3712", "4089", "4147", "4756", "5191", "5365", "5800", "6032", "6670", "6960", "7453", "7569", "8004", "8381", "8613", "9222", "9715", "10063", "10208", "10498", "11223", "11455", "11832"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["Fencing"]], "filtered_resps": ["Fencing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "623f93d741e87d27b19d057d20b4ab22470f0a4c672f1cb042879de1bcd14ccb", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "At the end of the video, what are the cat and two mice doing?\n(A) Playing soccer\n(B) Playing badminton\n(C) Dancing\n(D) Playing volleyball\n(E) Playing basketball\n(F) Fencing", "task_type": "plotQA", "pred_answer": "Fencing", "answer": "F"}, "input": "At the end of the video, what are the cat and two mice doing?\n(A) Playing soccer\n(B) Playing badminton\n(C) Dancing\n(D) Playing volleyball\n(E) Playing basketball\n(F) Fencing\nOnly give the best option.\nBest option: ("}
{"doc_id": 232, "doc": {"video_name": "test_cartoon_2.mp4", "duration": 733.4075768505323, "question": "What is the octopus's phone in the video?\n(A) Pineapple\n(B) Stone\n(C) Shoe\n(D) Shell\n(E) Conch\n(F) Coral", "candidates": ["Pineapple", "Stone", "Shoe", "Shell", "Conch", "Coral"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_27", "indices": ["525", "575", "1400", "1725", "2550", "3250", "3850", "4025", "5075", "5500", "5950", "6500", "7275", "7600", "8325", "9100", "9600", "9750", "10275", "11400", "11625", "12475", "13100", "13425", "14025", "14575", "15225", "15475", "16450", "17150", "17175", "17950"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08b82345d6f4a713a4d3db6428b36f1cab9294590b6a86bf4ded57a5b1d40ee3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the octopus's phone in the video?\n(A) Pineapple\n(B) Stone\n(C) Shoe\n(D) Shell\n(E) Conch\n(F) Coral", "task_type": "plotQA", "pred_answer": "E", "answer": "E"}, "input": "What is the octopus's phone in the video?\n(A) Pineapple\n(B) Stone\n(C) Shoe\n(D) Shell\n(E) Conch\n(F) Coral\nOnly give the best option.\nBest option: ("}
{"doc_id": 236, "doc": {"video_name": "test_movie101_90.mp4", "duration": 378.0, "question": "What color is the umbrella that is opened in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Yellow\n(E) Red\n(F) Green", "candidates": ["Black", "White", "Blue", "Yellow", "Red", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_31", "indices": ["150", "350", "800", "875", "1325", "1575", "1875", "2050", "2425", "2825", "3050", "3400", "3625", "3950", "4325", "4425", "4975", "5200", "5500", "5800", "6050", "6375", "6650", "7000", "7150", "7600", "7750", "8050", "8325", "8650", "8900", "9375"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b7a216e1d5363f16491f5ea0045602630c77aa68e6ab2caee1520a7a3362777", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the umbrella that is opened in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Yellow\n(E) Red\n(F) Green", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What color is the umbrella that is opened in the video?\n(A) Black\n(B) White\n(C) Blue\n(D) Yellow\n(E) Red\n(F) Green\nOnly give the best option.\nBest option: ("}
{"doc_id": 240, "doc": {"video_name": "test_movie101_98.mp4", "duration": 394.0, "question": "After the man in white gives the item to the man in protective clothing, what happens to him?\n(A) Killed\n(B) Imprisoned\n(C) Treated well\n(D) Beaten\n(E) Rewarded\n(F) Released", "candidates": ["Killed", "Imprisoned", "Treated well", "Beaten", "Rewarded", "Released"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_35", "indices": ["25", "350", "775", "1000", "1425", "1675", "2000", "2125", "2500", "2825", "3200", "3375", "3725", "4275", "4525", "4625", "5150", "5400", "5750", "5850", "6250", "6450", "6900", "7250", "7450", "7750", "8225", "8575", "8675", "8950", "9350", "9575"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1bc4a37bcca5f243bd8fe00fbb3608185ccb5cf08fbcaed7907b7a5a9e8b110", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "After the man in white gives the item to the man in protective clothing, what happens to him?\n(A) Killed\n(B) Imprisoned\n(C) Treated well\n(D) Beaten\n(E) Rewarded\n(F) Released", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "After the man in white gives the item to the man in protective clothing, what happens to him?\n(A) Killed\n(B) Imprisoned\n(C) Treated well\n(D) Beaten\n(E) Rewarded\n(F) Released\nOnly give the best option.\nBest option: ("}
{"doc_id": 244, "doc": {"video_name": "test_movie101_90.mp4", "duration": 378.0, "question": "What color clothes is the girl wearing while drinking at the beginning of the video?\n(A) Purple\n(B) White\n(C) Black\n(D) Green\n(E) Blue\n(F) Pink", "candidates": ["Purple", "White", "Black", "Green", "Blue", "Pink"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_39", "indices": ["75", "425", "600", "925", "1425", "1600", "1875", "2325", "2600", "2650", "3175", "3350", "3525", "3950", "4200", "4700", "4925", "5025", "5575", "5600", "6050", "6375", "6650", "7000", "7175", "7400", "7700", "8150", "8500", "8625", "9050", "9425"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Pink"]], "filtered_resps": ["(F) Pink"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8526560a4d80bf52ad2277e6d9ab75182ef1527894057bcb0df7d4357af7e924", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What color clothes is the girl wearing while drinking at the beginning of the video?\n(A) Purple\n(B) White\n(C) Black\n(D) Green\n(E) Blue\n(F) Pink", "task_type": "plotQA", "pred_answer": "F", "answer": "F"}, "input": "What color clothes is the girl wearing while drinking at the beginning of the video?\n(A) Purple\n(B) White\n(C) Black\n(D) Green\n(E) Blue\n(F) Pink\nOnly give the best option.\nBest option: ("}
{"doc_id": 248, "doc": {"video_name": "test_cartoon_5.mp4", "duration": 499.2630615463369, "question": "In the video, why does the fish in the restaurant look very angry?\n(A) Service is too slow\n(B) Arguing with each other\n(C) Food tastes bad\n(D) Poor service\n(E) Fighting with each other\n(F) Prices are too high", "candidates": ["Service is too slow", "Arguing with each other", "Food tastes bad", "Poor service", "Fighting with each other", "Prices are too high"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_43", "indices": ["50", "625", "1025", "1325", "1625", "2125", "2400", "2925", "3225", "3800", "3900", "4400", "4875", "5100", "5425", "5850", "6375", "6775", "7300", "7375", "8125", "8275", "8725", "9150", "9650", "9800", "10200", "10550", "11175", "11550", "11875", "12150"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8bf5c53725e66ecb0aab6eb0a003e3924d352fb3b71a2889f8368318f829b0e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, why does the fish in the restaurant look very angry?\n(A) Service is too slow\n(B) Arguing with each other\n(C) Food tastes bad\n(D) Poor service\n(E) Fighting with each other\n(F) Prices are too high", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "In the video, why does the fish in the restaurant look very angry?\n(A) Service is too slow\n(B) Arguing with each other\n(C) Food tastes bad\n(D) Poor service\n(E) Fighting with each other\n(F) Prices are too high\nOnly give the best option.\nBest option: ("}
{"doc_id": 252, "doc": {"video_name": "test_cartoon_3.mp4", "duration": 572.577543878504, "question": "What is inside the moldy hamburger in the video?\n(A) Rubber skin\n(B) Iron sheet\n(C) Cement\n(D) Glass\n(E) Wooden board\n(F) Plastic", "candidates": ["Rubber skin", "Iron sheet", "Cement", "Glass", "Wooden board", "Plastic"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_47", "indices": ["325", "425", "975", "1700", "2125", "2225", "3100", "3475", "3700", "4050", "4525", "5175", "5375", "6225", "6425", "6725", "7500", "7800", "8425", "8525", "9350", "9375", "10075", "10275", "10775", "11425", "11750", "12125", "12775", "13200", "13825", "14075"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d1674a48202e3d4f84f5741e6cb5db92263fac70ad889418d0b6d5322c105e5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is inside the moldy hamburger in the video?\n(A) Rubber skin\n(B) Iron sheet\n(C) Cement\n(D) Glass\n(E) Wooden board\n(F) Plastic", "task_type": "plotQA", "pred_answer": "A", "answer": "E"}, "input": "What is inside the moldy hamburger in the video?\n(A) Rubber skin\n(B) Iron sheet\n(C) Cement\n(D) Glass\n(E) Wooden board\n(F) Plastic\nOnly give the best option.\nBest option: ("}
{"doc_id": 256, "doc": {"video_name": "test_movie101_90.mp4", "duration": 378.0, "question": "What genre is this film?\n(A) Mystery\n(B) Action\n(C) Romance\n(D) Comedy\n(E) Thriller\n(F) Crime", "candidates": ["Mystery", "Action", "Romance", "Comedy", "Thriller", "Crime"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_1", "indices": ["150", "350", "800", "875", "1300", "1500", "1825", "2300", "2350", "2875", "3100", "3450", "3725", "3925", "4175", "4700", "4900", "5250", "5575", "5875", "6050", "6200", "6650", "7050", "7100", "7625", "7650", "8125", "8425", "8725", "8975", "9375"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Comedy"]], "filtered_resps": ["(D) Comedy"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f854ec1a41d7ea901e763e61fcf7ba52b04b03c09360f0b5255013e9ecd132a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Mystery\n(B) Action\n(C) Romance\n(D) Comedy\n(E) Thriller\n(F) Crime", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "C"}, "input": "What genre is this film?\n(A) Mystery\n(B) Action\n(C) Romance\n(D) Comedy\n(E) Thriller\n(F) Crime\nOnly give the best option.\nBest option: ("}
{"doc_id": 260, "doc": {"video_name": "test_game_5.mp4", "duration": 269.0, "question": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Pickup\n(C) Excavator\n(D) Truck\n(E) Train\n(F) Car", "candidates": ["Bus", "Pickup", "Excavator", "Truck", "Train", "Car"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_5", "indices": ["210", "240", "630", "930", "1050", "1260", "1680", "1860", "2130", "2340", "2550", "2790", "3150", "3270", "3570", "3780", "4170", "4260", "4710", "4890", "5190", "5490", "5610", "6000", "6210", "6300", "6690", "6840", "7200", "7320", "7680", "7980"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d8369909bff0f671eeb9c065432fc532a7de30427b29afd34d287e7e721749a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Pickup\n(C) Excavator\n(D) Truck\n(E) Train\n(F) Car", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Pickup\n(C) Excavator\n(D) Truck\n(E) Train\n(F) Car\nOnly give the best option.\nBest option: ("}
{"doc_id": 264, "doc": {"video_name": "test_cartoon_2.mp4", "duration": 733.4075768505323, "question": "What kind of environment does the entire video take place in?\n(A) Underwater\n(B) Forest\n(C) Supermarket\n(D) Campus\n(E) Desert\n(F) Library", "candidates": ["Underwater", "Forest", "Supermarket", "Campus", "Desert", "Library"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_9", "indices": ["400", "575", "1475", "2250", "2625", "3250", "3975", "4200", "4800", "5175", "5775", "6675", "7000", "7650", "8050", "9050", "9375", "9750", "10725", "11100", "11450", "12475", "12775", "13425", "14125", "14350", "15400", "15450", "16175", "17050", "17700", "18100"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6aae1fad0d7c00d1763ecb72851d20c3f1a74b66c9ad01099650fed148f0584c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What kind of environment does the entire video take place in?\n(A) Underwater\n(B) Forest\n(C) Supermarket\n(D) Campus\n(E) Desert\n(F) Library", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What kind of environment does the entire video take place in?\n(A) Underwater\n(B) Forest\n(C) Supermarket\n(D) Campus\n(E) Desert\n(F) Library\nOnly give the best option.\nBest option: ("}
{"doc_id": 268, "doc": {"video_name": "test_cartoon_22.mp4", "duration": 604.2316257134521, "question": "In what environment does the video take place?\n(A) Underwater\n(B) Forest\n(C) City\n(D) Countryside\n(E) Desert\n(F) Campus", "candidates": ["Underwater", "Forest", "City", "Countryside", "Desert", "Campus"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_13", "indices": ["300", "900", "1000", "1800", "2100", "2775", "2950", "3325", "4000", "4350", "5050", "5325", "6000", "6150", "6850", "7425", "7700", "8250", "8900", "9400", "9700", "10150", "10475", "11000", "11325", "12225", "12550", "13000", "13450", "14075", "14150", "14775"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe54af3e4c6c6bbf068c6f66d5d8b91fcd799b091546ec00deb3ac1ffbb01bba", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what environment does the video take place?\n(A) Underwater\n(B) Forest\n(C) City\n(D) Countryside\n(E) Desert\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what environment does the video take place?\n(A) Underwater\n(B) Forest\n(C) City\n(D) Countryside\n(E) Desert\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 272, "doc": {"video_name": "test_cartoon_14.mp4", "duration": 453.5, "question": "What event is mainly described in the video?\n(A) Social reform\n(B) Dance performance\n(C) Comical chase\n(D) Political struggle\n(E) Mythical story\n(F) Time travel", "candidates": ["Social reform", "Dance performance", "Comical chase", "Political struggle", "Mythical story", "Time travel"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_17", "indices": ["145", "609", "1160", "1624", "1943", "2117", "2871", "3045", "3596", "4205", "4292", "4843", "5336", "5539", "6032", "6757", "7134", "7192", "7714", "8323", "8526", "9222", "9338", "10063", "10469", "10788", "11020", "11426", "12064", "12586", "13050", "13543"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Comical chase"]], "filtered_resps": ["(C) Comical chase"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "382f91a3db0351ee2e72e7c9e1e289285657b694b1b01c9ddcd28d1a8537340c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What event is mainly described in the video?\n(A) Social reform\n(B) Dance performance\n(C) Comical chase\n(D) Political struggle\n(E) Mythical story\n(F) Time travel", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What event is mainly described in the video?\n(A) Social reform\n(B) Dance performance\n(C) Comical chase\n(D) Political struggle\n(E) Mythical story\n(F) Time travel\nOnly give the best option.\nBest option: ("}
{"doc_id": 276, "doc": {"video_name": "test_cartoon_23.mp4", "duration": 727.120353586435, "question": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Grassland\n(C) Snow mountain\n(D) Forest\n(E) Ocean\n(F) City", "candidates": ["Desert", "Grassland", "Snow mountain", "Forest", "Ocean", "City"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_21", "indices": ["250", "700", "1450", "1675", "2775", "2800", "3400", "4300", "4550", "5350", "6175", "6650", "7000", "7650", "7950", "8500", "9375", "10100", "10350", "11150", "11425", "12000", "12950", "13100", "13975", "14200", "15000", "15375", "16225", "16825", "17075", "17975"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e07e0ade3365367192393ba61d46e40fcca9c8e2ff470111eb8c0c669c513ee1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Grassland\n(C) Snow mountain\n(D) Forest\n(E) Ocean\n(F) City", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Grassland\n(C) Snow mountain\n(D) Forest\n(E) Ocean\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 280, "doc": {"video_name": "test_cartoon_10.mp4", "duration": 586.918084061671, "question": "How many cartoon characters appear in the video?\n(A) 3\n(B) 6\n(C) 4\n(D) 2\n(E) 1\n(F) 5", "candidates": ["3", "6", "4", "2", "1", "5"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_25", "indices": ["25", "475", "900", "1775", "2150", "2700", "2875", "3575", "4075", "4175", "4575", "5125", "5875", "6325", "6425", "6925", "7700", "7825", "8475", "9025", "9575", "9850", "10400", "10750", "11400", "11750", "12025", "12325", "13075", "13450", "13925", "14600"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 6"]], "filtered_resps": ["(B) 6"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eccf7abb240abefa937c152cd34a10715987e441005bb4faf191931f3082042c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How many cartoon characters appear in the video?\n(A) 3\n(B) 6\n(C) 4\n(D) 2\n(E) 1\n(F) 5", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "How many cartoon characters appear in the video?\n(A) 3\n(B) 6\n(C) 4\n(D) 2\n(E) 1\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 284, "doc": {"video_name": "test_cartoon_21.mp4", "duration": 662.3029651724227, "question": "In what environment does the video take place?\n(A) Forest\n(B) Countryside\n(C) City\n(D) Underwater\n(E) Desert\n(F) Campus", "candidates": ["Forest", "Countryside", "City", "Underwater", "Desert", "Campus"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_29", "indices": ["125", "750", "1475", "1575", "2500", "2800", "3075", "3600", "4375", "5075", "5625", "5750", "6350", "7125", "7500", "8225", "8375", "8975", "9750", "10275", "10525", "10975", "11500", "12125", "12700", "13275", "13800", "13950", "14700", "15450", "15975", "16300"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "58e6f801f714dfcfff7d7dce9252ba3a182e907f4d1f8b75317f5d4ac3a06c31", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what environment does the video take place?\n(A) Forest\n(B) Countryside\n(C) City\n(D) Underwater\n(E) Desert\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what environment does the video take place?\n(A) Forest\n(B) Countryside\n(C) City\n(D) Underwater\n(E) Desert\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 288, "doc": {"video_name": "test_cartoon_22.mp4", "duration": 604.2316257134521, "question": "What type of film is this?\n(A) Documentary\n(B) War\n(C) Action\n(D) Animated film\n(E) Romance\n(F) Horror", "candidates": ["Documentary", "War", "Action", "Animated film", "Romance", "Horror"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_33", "indices": ["250", "825", "950", "1475", "1900", "2725", "3225", "3300", "3950", "4350", "4950", "5325", "5850", "6225", "6675", "7475", "7875", "8025", "8650", "9050", "9800", "10250", "10400", "11000", "11475", "12100", "12550", "13150", "13275", "14125", "14175", "15000"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4dbbfd4f7ed6b34cbbd971eb9c7686e770a7e3b43f710d46ae554ffcfa750cb", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Documentary\n(B) War\n(C) Action\n(D) Animated film\n(E) Romance\n(F) Horror", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) Documentary\n(B) War\n(C) Action\n(D) Animated film\n(E) Romance\n(F) Horror\nOnly give the best option.\nBest option: ("}
{"doc_id": 292, "doc": {"video_name": "test_entv_6.mp4", "duration": 573.0751841594767, "question": "What type of film is this?\n(A) Action\n(B) Adventure\n(C) Horror\n(D) Romance\n(E) War\n(F) Documentary", "candidates": ["Action", "Adventure", "Horror", "Romance", "War", "Documentary"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_37", "indices": ["322", "621", "920", "1334", "1978", "2162", "2852", "3358", "3726", "3841", "4508", "4968", "5359", "5543", "6256", "6440", "7176", "7452", "8096", "8487", "8763", "9062", "9821", "9867", "10672", "10879", "11546", "11983", "12006", "12811", "12949", "13455"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) War"]], "filtered_resps": ["(E) War"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d01aaac665a6519a98d369d9231cb88beeb19130077785ed09a6003b99972af", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Adventure\n(C) Horror\n(D) Romance\n(E) War\n(F) Documentary", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "B"}, "input": "What type of film is this?\n(A) Action\n(B) Adventure\n(C) Horror\n(D) Romance\n(E) War\n(F) Documentary\nOnly give the best option.\nBest option: ("}
{"doc_id": 296, "doc": {"video_name": "test_cartoon_5.mp4", "duration": 499.2630615463369, "question": "In what setting does the video take place?\n(A) Underwater\n(B) Forest\n(C) Countryside\n(D) Desert\n(E) Campus\n(F) City", "candidates": ["Underwater", "Forest", "Countryside", "Desert", "Campus", "City"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_41", "indices": ["275", "625", "1075", "1275", "1750", "1950", "2450", "2825", "3225", "3475", "4225", "4550", "5000", "5100", "5800", "6100", "6475", "6750", "7275", "7625", "7900", "8275", "8675", "9100", "9500", "10075", "10150", "10625", "11050", "11550", "11900", "12225"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "229a8a8469a749e0dd18b1e1cbe8fe835f3abd9964a2a5f4bdc079cd4ab9eb91", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Underwater\n(B) Forest\n(C) Countryside\n(D) Desert\n(E) Campus\n(F) City", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the video take place?\n(A) Underwater\n(B) Forest\n(C) Countryside\n(D) Desert\n(E) Campus\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 300, "doc": {"video_name": "test_cartoon_1.mp4", "duration": 483.07277143454803, "question": "Who is the main character in the video?\n(A) Crab\n(B) Snail\n(C) Fish\n(D) Pink Starfish\n(E) Octopus\n(F) Yellow Sponge", "candidates": ["Crab", "Snail", "Fish", "Pink Starfish", "Octopus", "Yellow Sponge"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_45", "indices": ["250", "700", "825", "1300", "1700", "2125", "2600", "2800", "3000", "3550", "3850", "4425", "4500", "5000", "5600", "5825", "6025", "6500", "6775", "7325", "7850", "8200", "8275", "8900", "9200", "9450", "9850", "10275", "10700", "11200", "11625", "12000"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6fb7eb337f97fa76f2cd5fe751c4d13f31cc1ace21a8e35504c754f5b236d266", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Who is the main character in the video?\n(A) Crab\n(B) Snail\n(C) Fish\n(D) Pink Starfish\n(E) Octopus\n(F) Yellow Sponge", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "Who is the main character in the video?\n(A) Crab\n(B) Snail\n(C) Fish\n(D) Pink Starfish\n(E) Octopus\n(F) Yellow Sponge\nOnly give the best option.\nBest option: ("}
{"doc_id": 304, "doc": {"video_name": "test_entv_1.mp4", "duration": 594.6644105054177, "question": "In what setting does the entire video take place?\n(A) Desert\n(B) Beach\n(C) City\n(D) Campus\n(E) Library\n(F) Castle", "candidates": ["Desert", "Beach", "City", "Campus", "Library", "Castle"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_49", "indices": ["0", "621", "1242", "1311", "2139", "2438", "2760", "3519", "3749", "4209", "4669", "5129", "5727", "6095", "6440", "7061", "7406", "7613", "8395", "8441", "9246", "9729", "9936", "10626", "10741", "11385", "11684", "12282", "12834", "13317", "13363", "13800"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Castle"]], "filtered_resps": ["(F) Castle"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "041a8b557a87facc9082aff8fa290c14935dd8ccef2622f31b3482a5c189dd8f", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In what setting does the entire video take place?\n(A) Desert\n(B) Beach\n(C) City\n(D) Campus\n(E) Library\n(F) Castle", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "In what setting does the entire video take place?\n(A) Desert\n(B) Beach\n(C) City\n(D) Campus\n(E) Library\n(F) Castle\nOnly give the best option.\nBest option: ("}
{"doc_id": 308, "doc": {"video_name": "test_game_6.mp4", "duration": 227.0, "question": "What type of house is being constructed in the video?\n(A) Cabin\n(B) Attic\n(C) Basement\n(D) Apartment building\n(E) Bungalow\n(F) Palace", "candidates": ["Cabin", "Attic", "Basement", "Apartment building", "Bungalow", "Palace"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_53", "indices": ["174", "261", "493", "609", "899", "1044", "1363", "1479", "1682", "2059", "2146", "2291", "2697", "2784", "2958", "3335", "3509", "3770", "3886", "4002", "4321", "4611", "4727", "5046", "5104", "5278", "5510", "5887", "5945", "6119", "6496", "6583"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9cc38799b602a02554ebd4c2af9ced231f4cf10bb23fd6c2ee63284591e0aff6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of house is being constructed in the video?\n(A) Cabin\n(B) Attic\n(C) Basement\n(D) Apartment building\n(E) Bungalow\n(F) Palace", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of house is being constructed in the video?\n(A) Cabin\n(B) Attic\n(C) Basement\n(D) Apartment building\n(E) Bungalow\n(F) Palace\nOnly give the best option.\nBest option: ("}
{"doc_id": 312, "doc": {"video_name": "test_cartoon_17.mp4", "duration": 400.58, "question": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Animation\n(D) Horror\n(E) Documentary\n(F) War", "candidates": ["Action", "Romance", "Animation", "Horror", "Documentary", "War"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_57", "indices": ["116", "435", "725", "1189", "1711", "2175", "2320", "2900", "3219", "3567", "3915", "4205", "4611", "5220", "5394", "5626", "6235", "6670", "6844", "7366", "7569", "7888", "8526", "8700", "9338", "9483", "10005", "10411", "10759", "11078", "11542", "11948"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9e025ec544b3c3736b1153a302bc91bc1ea89833769f0f55342bd7fd2df5b810", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Animation\n(D) Horror\n(E) Documentary\n(F) War", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of film is this?\n(A) Action\n(B) Romance\n(C) Animation\n(D) Horror\n(E) Documentary\n(F) War\nOnly give the best option.\nBest option: ("}
{"doc_id": 316, "doc": {"video_name": "test_AWC-4.mp4", "duration": 450.0, "question": "How many scenes were transitioned in the video?\n(A) 6\n(B) 1\n(C) 4\n(D) 2\n(E) 3\n(F) 5", "candidates": ["6", "1", "4", "2", "3", "5"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_61", "indices": ["25", "450", "775", "1150", "1700", "1975", "2150", "2450", "2950", "3275", "3525", "4025", "4450", "4550", "5175", "5250", "5875", "6000", "6450", "7000", "7300", "7550", "7775", "8375", "8575", "8950", "9325", "9575", "9925", "10450", "10650", "11175"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 4"]], "filtered_resps": ["(C) 4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c4a41b5663f3ad01276b537d99ddc36c1bd39c38748c85f54929cb00ebe02c2", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How many scenes were transitioned in the video?\n(A) 6\n(B) 1\n(C) 4\n(D) 2\n(E) 3\n(F) 5", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "E"}, "input": "How many scenes were transitioned in the video?\n(A) 6\n(B) 1\n(C) 4\n(D) 2\n(E) 3\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 320, "doc": {"video_name": "test_pet_1.mp4", "duration": 561.2, "question": "What is the main animal shown in the video?\n(A) Pig\n(B) Sheep\n(C) Cow\n(D) Horse\n(E) Dog\n(F) Cat", "candidates": ["Pig", "Sheep", "Cow", "Horse", "Dog", "Cat"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_65", "indices": ["58", "928", "1073", "1740", "2291", "2610", "3277", "3973", "4640", "5162", "5539", "5742", "6728", "6786", "7627", "8352", "8613", "8903", "9599", "10382", "10962", "11049", "11803", "12383", "12847", "13166", "14065", "14181", "15138", "15370", "16095", "16559"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "477faf90bdfcfb3add5d2e6a33eab50db7288ebbbd4595c63b34c1505141e8b4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the main animal shown in the video?\n(A) Pig\n(B) Sheep\n(C) Cow\n(D) Horse\n(E) Dog\n(F) Cat", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What is the main animal shown in the video?\n(A) Pig\n(B) Sheep\n(C) Cow\n(D) Horse\n(E) Dog\n(F) Cat\nOnly give the best option.\nBest option: ("}
{"doc_id": 324, "doc": {"video_name": "test_movie101_97.mp4", "duration": 556.0, "question": "What type of film is this?\n(A) Crime film\n(B) Science fiction film\n(C) War film\n(D) Horror film\n(E) Romance film\n(F) Documentary film", "candidates": ["Crime film", "Science fiction film", "War film", "Horror film", "Romance film", "Documentary film"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_69", "indices": ["0", "825", "1100", "1275", "2025", "2475", "2825", "3075", "3550", "3975", "4625", "5150", "5200", "5975", "6125", "6875", "7300", "7600", "8000", "8600", "8825", "9250", "9625", "10300", "10450", "11225", "11375", "11925", "12200", "12600", "13400", "13675"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) Crime film"]], "filtered_resps": ["(A) Crime film"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a59e3afadf9a4f95bb2e3a3a24d064af87b57dafe4aecee30b50dafe78ca6f36", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Crime film\n(B) Science fiction film\n(C) War film\n(D) Horror film\n(E) Romance film\n(F) Documentary film", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of film is this?\n(A) Crime film\n(B) Science fiction film\n(C) War film\n(D) Horror film\n(E) Romance film\n(F) Documentary film\nOnly give the best option.\nBest option: ("}
{"doc_id": 328, "doc": {"video_name": "test_movie101_99.mp4", "duration": 464.0, "question": "Under what environmental conditions does the scene in the video take place?\n(A) City\n(B) Rainforest\n(C) Snow Mountain\n(D) Desert\n(E) Ocean\n(F) Island", "candidates": ["City", "Rainforest", "Snow Mountain", "Desert", "Ocean", "Island"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_73", "indices": ["75", "675", "725", "1200", "1575", "1900", "2200", "2625", "3125", "3575", "3750", "4325", "4400", "4725", "5400", "5550", "5800", "6450", "6525", "7100", "7250", "7650", "8225", "8475", "9025", "9225", "9650", "10050", "10150", "10675", "11175", "11325"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fd9a043f89124178a335267bbde8b87e0c88ee9fb652523a220379de4c2c096b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Under what environmental conditions does the scene in the video take place?\n(A) City\n(B) Rainforest\n(C) Snow Mountain\n(D) Desert\n(E) Ocean\n(F) Island", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "Under what environmental conditions does the scene in the video take place?\n(A) City\n(B) Rainforest\n(C) Snow Mountain\n(D) Desert\n(E) Ocean\n(F) Island\nOnly give the best option.\nBest option: ("}
{"doc_id": 332, "doc": {"video_name": "test_AWC-4.mp4", "duration": 450.0, "question": "What is the main animal discussed in the video?\n(A) Tiger\n(B) Dinosaur\n(C) Cat\n(D) Lion\n(E) Rabbit\n(F) Dog", "candidates": ["Tiger", "Dinosaur", "Cat", "Lion", "Rabbit", "Dog"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_77", "indices": ["325", "550", "750", "1100", "1475", "1900", "2100", "2675", "2875", "3150", "3625", "4050", "4450", "4875", "5075", "5250", "5950", "5975", "6550", "6800", "7325", "7400", "7975", "8200", "8675", "8850", "9250", "9575", "10100", "10475", "10575", "10900"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f7acd71563f607fddd941aca2af396229070026fd21db8bb7ece7744582a1e2b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main animal discussed in the video?\n(A) Tiger\n(B) Dinosaur\n(C) Cat\n(D) Lion\n(E) Rabbit\n(F) Dog", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main animal discussed in the video?\n(A) Tiger\n(B) Dinosaur\n(C) Cat\n(D) Lion\n(E) Rabbit\n(F) Dog\nOnly give the best option.\nBest option: ("}
{"doc_id": 336, "doc": {"video_name": "test_entv_22.mp4", "duration": 407.116439508766, "question": "What genre is this film?\n(A) Science Fiction\n(B) Comedy\n(C) Documentary\n(D) Action\n(E) Horror\n(F) War", "candidates": ["Science Fiction", "Comedy", "Documentary", "Action", "Horror", "War"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_81", "indices": ["46", "529", "690", "943", "1495", "1564", "2001", "2369", "2645", "2760", "3197", "3565", "3910", "3979", "4301", "4807", "5129", "5198", "5727", "5773", "6371", "6417", "6762", "7061", "7429", "7613", "8004", "8395", "8694", "9085", "9200", "9729"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1e32c6fdf2e1dea4c389841e6faf5db1034e8c41dcde8fdb7f1064959e1a41ff", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Science Fiction\n(B) Comedy\n(C) Documentary\n(D) Action\n(E) Horror\n(F) War", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What genre is this film?\n(A) Science Fiction\n(B) Comedy\n(C) Documentary\n(D) Action\n(E) Horror\n(F) War\nOnly give the best option.\nBest option: ("}
{"doc_id": 340, "doc": {"video_name": "test_food_3.mp4", "duration": 411.41, "question": "What type of video is this?\n(A) Bodybuilding & Fitness\n(B) Classroom Teaching\n(C) Travel Explanation\n(D) Food Cooking\n(E) Daily Office Work\n(F) Sports", "candidates": ["Bodybuilding & Fitness", "Classroom Teaching", "Travel Explanation", "Food Cooking", "Daily Office Work", "Sports"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_85", "indices": ["92", "460", "598", "1081", "1311", "1817", "1978", "2323", "2576", "2806", "3220", "3450", "3910", "4232", "4416", "4807", "4922", "5290", "5727", "6095", "6233", "6555", "6808", "7222", "7613", "7774", "8257", "8349", "8717", "9131", "9499", "9706"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f0c3d7ebcf612cbaed9f5fe0fa597997aa69b756a9df2957beff29bc73f2bfde", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Bodybuilding & Fitness\n(B) Classroom Teaching\n(C) Travel Explanation\n(D) Food Cooking\n(E) Daily Office Work\n(F) Sports", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of video is this?\n(A) Bodybuilding & Fitness\n(B) Classroom Teaching\n(C) Travel Explanation\n(D) Food Cooking\n(E) Daily Office Work\n(F) Sports\nOnly give the best option.\nBest option: ("}
{"doc_id": 344, "doc": {"video_name": "test_entv_18.mp4", "duration": 651.8112954069612, "question": "What is the setting of the video?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Campus\n(F) Forest", "candidates": ["Countryside", "Desert", "Ocean", "Urban", "Campus", "Forest"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_89", "indices": ["203", "1131", "1247", "1885", "3016", "3596", "4002", "4437", "5452", "5974", "6206", "6902", "7453", "7917", "8613", "9599", "10121", "10759", "11426", "12093", "12528", "13079", "13485", "14529", "14616", "15370", "16327", "16559", "17226", "17661", "18386", "19256"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Urban"]], "filtered_resps": ["(D) Urban"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8eba634e60d13c2c401fbd6cc1ff642f5bc1a1ef9a0b6a3ae2c654f6adc63a04", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the setting of the video?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Campus\n(F) Forest", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the setting of the video?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Campus\n(F) Forest\nOnly give the best option.\nBest option: ("}
{"doc_id": 348, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "Where did I keep the game board?\n(A) On the bookshelf\n(B) In the closet\n(C) Under the bed\n(D) On the coffee table\n(E) In the game box.\n(F) In the kitchen drawer", "candidates": ["On the bookshelf", "In the closet", "Under the bed", "On the coffee table", "In the game box.", "In the kitchen drawer"], "answer": "E", "task_type": "ego", "question_id": "ER_2", "indices": ["240", "690", "1020", "1440", "2190", "2430", "3030", "3150", "3990", "4350", "4710", "4980", "5730", "6240", "6540", "6960", "7410", "7710", "8490", "8970", "9090", "9510", "9990", "10530", "11220", "11310", "12090", "12510", "12900", "13350", "13620", "14190"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c0c7aa067088560ad4a8cdd0e60765c5eca0772c17afbabe8937def07411d1a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Where did I keep the game board?\n(A) On the bookshelf\n(B) In the closet\n(C) Under the bed\n(D) On the coffee table\n(E) In the game box.\n(F) In the kitchen drawer", "task_type": "ego", "pred_answer": "D", "answer": "E"}, "input": "Where did I keep the game board?\n(A) On the bookshelf\n(B) In the closet\n(C) Under the bed\n(D) On the coffee table\n(E) In the game box.\n(F) In the kitchen drawer\nOnly give the best option.\nBest option: ("}
{"doc_id": 352, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "How many flat plastic plates were on the table\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n(E) 2\n(F) 5", "candidates": ["3", "6", "1", "4", "2", "5"], "answer": "E", "task_type": "ego", "question_id": "ER_6", "indices": ["270", "630", "990", "1770", "1800", "2400", "2970", "3540", "3630", "4290", "4740", "5100", "5640", "5910", "6450", "7020", "7530", "8070", "8190", "8850", "9390", "9810", "10080", "10470", "11220", "11430", "12090", "12180", "12630", "13230", "13890", "14280"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1bd13375a6a81e7da5f91f940f3c1bee28a85c6892fe5946f3f27d52dc696ff", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How many flat plastic plates were on the table\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n(E) 2\n(F) 5", "task_type": "ego", "pred_answer": "A", "answer": "E"}, "input": "How many flat plastic plates were on the table\n(A) 3\n(B) 6\n(C) 1\n(D) 4\n(E) 2\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 356, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "What colour dustpan did I hold while cleaning the floor?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n(E) White.\n(F) Black", "candidates": ["Green", "Blue", "Red", "Yellow", "White.", "Black"], "answer": "E", "task_type": "ego", "question_id": "ER_10", "indices": ["210", "450", "990", "1770", "1920", "2580", "3060", "3180", "3840", "4410", "4620", "5250", "5820", "5850", "6690", "6960", "7200", "7710", "8490", "8670", "9420", "9630", "9960", "10680", "11220", "11280", "11850", "12540", "12660", "13440", "13890", "14190"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c775607166419af53ab6523161d3112958712c9feeb47d80bce7aaa794ac190b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What colour dustpan did I hold while cleaning the floor?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n(E) White.\n(F) Black", "task_type": "ego", "pred_answer": "E", "answer": "E"}, "input": "What colour dustpan did I hold while cleaning the floor?\n(A) Green\n(B) Blue\n(C) Red\n(D) Yellow\n(E) White.\n(F) Black\nOnly give the best option.\nBest option: ("}
{"doc_id": 360, "doc": {"video_name": "test_ego_10.mp4", "duration": 480.0333333333333, "question": "Did I leave the television on?\n(A) No, you didn't\n(B) Yes, but it was turned off later\n(C) Yes, I did\n(D) I don't know, I wasn't there\n(E) Maybe, I'm not sure\n(F) No, it was never on", "candidates": ["No, you didn't", "Yes, but it was turned off later", "Yes, I did", "I don't know, I wasn't there", "Maybe, I'm not sure", "No, it was never on"], "answer": "C", "task_type": "ego", "question_id": "ER_14", "indices": ["360", "720", "990", "1680", "1860", "2670", "2700", "3270", "3660", "4290", "4920", "5310", "5520", "5940", "6660", "6960", "7260", "7740", "8430", "8880", "9090", "9540", "10170", "10500", "11130", "11610", "11940", "12540", "12660", "13410", "13530", "14070"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) No, you didn't"]], "filtered_resps": ["(A) No, you didn't"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d2f7363d7b3d3b2932df27523e609d536218cc6ea2e31175ffa6ba725f160277", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the television on?\n(A) No, you didn't\n(B) Yes, but it was turned off later\n(C) Yes, I did\n(D) I don't know, I wasn't there\n(E) Maybe, I'm not sure\n(F) No, it was never on", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Did I leave the television on?\n(A) No, you didn't\n(B) Yes, but it was turned off later\n(C) Yes, I did\n(D) I don't know, I wasn't there\n(E) Maybe, I'm not sure\n(F) No, it was never on\nOnly give the best option.\nBest option: ("}
{"doc_id": 364, "doc": {"video_name": "test_ego_10.mp4", "duration": 480.0333333333333, "question": "Where did I put the dirt?\n(A) In the garden\n(B) In the dustbin\n(C) Under the rug\n(D) In the garage\n(E) In the attic\n(F) In the compost bin", "candidates": ["In the garden", "In the dustbin", "Under the rug", "In the garage", "In the attic", "In the compost bin"], "answer": "B", "task_type": "ego", "question_id": "ER_18", "indices": ["120", "510", "1080", "1530", "1980", "2400", "2820", "3480", "3930", "4320", "4800", "5070", "5430", "5910", "6330", "6870", "7440", "7710", "8130", "8970", "9360", "9510", "10200", "10470", "11070", "11310", "11880", "12540", "12690", "13470", "13500", "14100"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b10e0852ef316a65a2a405e5d151e14a9cc192a48e6ca51b30619d737920480", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where did I put the dirt?\n(A) In the garden\n(B) In the dustbin\n(C) Under the rug\n(D) In the garage\n(E) In the attic\n(F) In the compost bin", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where did I put the dirt?\n(A) In the garden\n(B) In the dustbin\n(C) Under the rug\n(D) In the garage\n(E) In the attic\n(F) In the compost bin\nOnly give the best option.\nBest option: ("}
{"doc_id": 368, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "Where did I put the carton of tools\n(A) on the bed\n(B) under the sink\n(C) in the garage\n(D) on the kitchen counter\n(E) in the closet\n(F) in the car trunk", "candidates": ["on the bed", "under the sink", "in the garage", "on the kitchen counter", "in the closet", "in the car trunk"], "answer": "A", "task_type": "ego", "question_id": "ER_22", "indices": ["0", "780", "1260", "1770", "1950", "2280", "2820", "3450", "3840", "4320", "4590", "5010", "5580", "5880", "6330", "7170", "7560", "7830", "8520", "8580", "9270", "9570", "9930", "10470", "11010", "11400", "12090", "12240", "12630", "13170", "13740", "14100"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6ebdcb2c41f0d0af288550023af5fe89f447cbda8202d62c06a627df6ed9371", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the carton of tools\n(A) on the bed\n(B) under the sink\n(C) in the garage\n(D) on the kitchen counter\n(E) in the closet\n(F) in the car trunk", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "Where did I put the carton of tools\n(A) on the bed\n(B) under the sink\n(C) in the garage\n(D) on the kitchen counter\n(E) in the closet\n(F) in the car trunk\nOnly give the best option.\nBest option: ("}
{"doc_id": 372, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "Where was the red ball\n(A) in the passageway\n(B) at the park\n(C) on the rooftop\n(D) under the bed\n(E) in the car\n(F) in the kitchen", "candidates": ["in the passageway", "at the park", "on the rooftop", "under the bed", "in the car", "in the kitchen"], "answer": "A", "task_type": "ego", "question_id": "ER_26", "indices": ["390", "570", "1200", "1470", "2220", "2550", "2820", "3180", "3840", "4170", "4860", "5130", "5640", "6270", "6330", "6960", "7590", "7950", "8160", "8880", "9090", "9810", "9960", "10680", "11220", "11310", "11850", "12540", "12750", "13080", "13590", "14340"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4da19827a7346431f801d0456265d494da93aaa5121a44683cbaf9dcfdbbd2c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the red ball\n(A) in the passageway\n(B) at the park\n(C) on the rooftop\n(D) under the bed\n(E) in the car\n(F) in the kitchen", "task_type": "ego", "pred_answer": "F", "answer": "A"}, "input": "Where was the red ball\n(A) in the passageway\n(B) at the park\n(C) on the rooftop\n(D) under the bed\n(E) in the car\n(F) in the kitchen\nOnly give the best option.\nBest option: ("}
{"doc_id": 376, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "In what location did I first see the laptop\n(A) on the dining table\n(B) on the top shelf of the wardrobe\n(C) next to the sofa\n(D) under the bed\n(E) in the living room cabinet\n(F) in the lower section of the bedside cupboard", "candidates": ["on the dining table", "on the top shelf of the wardrobe", "next to the sofa", "under the bed", "in the living room cabinet", "in the lower section of the bedside cupboard"], "answer": "F", "task_type": "ego", "question_id": "ER_30", "indices": ["90", "540", "1320", "1440", "2220", "2250", "2790", "3390", "4020", "4410", "4530", "5280", "5730", "5940", "6720", "6810", "7320", "7650", "8130", "8610", "9120", "9450", "10260", "10470", "11130", "11670", "11880", "12570", "12810", "13170", "13770", "14250"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) next to the sofa"]], "filtered_resps": ["(C) next to the sofa"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8822f4df4bf6fa3ba95a479bed811987f5fe3bb2ae2e21dfb890725f5839c0fb", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In what location did I first see the laptop\n(A) on the dining table\n(B) on the top shelf of the wardrobe\n(C) next to the sofa\n(D) under the bed\n(E) in the living room cabinet\n(F) in the lower section of the bedside cupboard", "task_type": "ego", "pred_answer": "C", "answer": "F"}, "input": "In what location did I first see the laptop\n(A) on the dining table\n(B) on the top shelf of the wardrobe\n(C) next to the sofa\n(D) under the bed\n(E) in the living room cabinet\n(F) in the lower section of the bedside cupboard\nOnly give the best option.\nBest option: ("}
{"doc_id": 380, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "What food did I first take from the pack of food on the table\n(A) chicken nuggets\n(B) french fries\n(C) cookies\n(D) sandwich\n(E) potato chips\n(F) apple slices", "candidates": ["chicken nuggets", "french fries", "cookies", "sandwich", "potato chips", "apple slices"], "answer": "B", "task_type": "ego", "question_id": "ER_34", "indices": ["150", "450", "1140", "1650", "1980", "2580", "2820", "3450", "4020", "4050", "4770", "5040", "5610", "5880", "6450", "7080", "7500", "7920", "8160", "8880", "9090", "9750", "10050", "10470", "11010", "11400", "12000", "12150", "12780", "13350", "13680", "13950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bef1e2cba56b92c3e0c663789901ea3c1ae6a5c67dc03891e702281b96a9c18f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What food did I first take from the pack of food on the table\n(A) chicken nuggets\n(B) french fries\n(C) cookies\n(D) sandwich\n(E) potato chips\n(F) apple slices", "task_type": "ego", "pred_answer": "E", "answer": "B"}, "input": "What food did I first take from the pack of food on the table\n(A) chicken nuggets\n(B) french fries\n(C) cookies\n(D) sandwich\n(E) potato chips\n(F) apple slices\nOnly give the best option.\nBest option: ("}
{"doc_id": 384, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "Did I leave the Tv on\n(A) not sure\n(B) I don't know\n(C) possibly\n(D) yes\n(E) maybe\n(F) no", "candidates": ["not sure", "I don't know", "possibly", "yes", "maybe", "no"], "answer": "D", "task_type": "ego", "question_id": "ER_38", "indices": ["360", "600", "1230", "1440", "1980", "2340", "3090", "3180", "3600", "4140", "4620", "5190", "5430", "5910", "6720", "7050", "7200", "8010", "8160", "8670", "9120", "9870", "9960", "10740", "11220", "11310", "12030", "12210", "12870", "13470", "13890", "14280"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) yes"]], "filtered_resps": ["(D) yes"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da08ecde42d3985a7ea89e4574db5a86d86208f15ebb7751aa62cc6f7239156d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I leave the Tv on\n(A) not sure\n(B) I don't know\n(C) possibly\n(D) yes\n(E) maybe\n(F) no", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Did I leave the Tv on\n(A) not sure\n(B) I don't know\n(C) possibly\n(D) yes\n(E) maybe\n(F) no\nOnly give the best option.\nBest option: ("}
{"doc_id": 388, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "Where was the knife after I sliced the limes\n(A) in the refrigerator\n(B) on the dining table\n(C) on the kitchen counter\n(D) on the cutting board\n(E) in the dishwasher\n(F) on a bowl in the sink", "candidates": ["in the refrigerator", "on the dining table", "on the kitchen counter", "on the cutting board", "in the dishwasher", "on a bowl in the sink"], "answer": "F", "task_type": "ego", "question_id": "ER_42", "indices": ["420", "570", "960", "1440", "2100", "2370", "3120", "3270", "3630", "4350", "4920", "5220", "5430", "6240", "6300", "6990", "7500", "8070", "8370", "8730", "9360", "9660", "10050", "10530", "10830", "11580", "11910", "12300", "12930", "13470", "13500", "13950"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2b246461fc5db71cbd5c1ecc6c5d4137ecd4545a436a9f21a033473ae2c1d4c", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where was the knife after I sliced the limes\n(A) in the refrigerator\n(B) on the dining table\n(C) on the kitchen counter\n(D) on the cutting board\n(E) in the dishwasher\n(F) on a bowl in the sink", "task_type": "ego", "pred_answer": "D", "answer": "F"}, "input": "Where was the knife after I sliced the limes\n(A) in the refrigerator\n(B) on the dining table\n(C) on the kitchen counter\n(D) on the cutting board\n(E) in the dishwasher\n(F) on a bowl in the sink\nOnly give the best option.\nBest option: ("}
{"doc_id": 392, "doc": {"video_name": "test_ego_0.mp4", "duration": 480.0, "question": "Where was the screw packet after I tore it up?\n(A) In my pocket\n(B) Under the sofa\n(C) On the table\n(D) Next to the toolbox\n(E) In the dustbin\n(F) In the drawer", "candidates": ["In my pocket", "Under the sofa", "On the table", "Next to the toolbox", "In the dustbin", "In the drawer"], "answer": "E", "task_type": "ego", "question_id": "ER_46", "indices": ["360", "870", "1200", "1530", "2160", "2550", "3060", "3150", "3930", "4200", "4530", "5340", "5400", "6270", "6480", "6810", "7500", "7740", "8490", "8940", "9210", "9660", "10200", "10710", "10830", "11340", "11850", "12510", "12900", "13050", "13620", "14100"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "41a5ff70b4fc06de29047e69ed2579b580bb6bf4a0dd46643603969edd3f27da", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Where was the screw packet after I tore it up?\n(A) In my pocket\n(B) Under the sofa\n(C) On the table\n(D) Next to the toolbox\n(E) In the dustbin\n(F) In the drawer", "task_type": "ego", "pred_answer": "C", "answer": "E"}, "input": "Where was the screw packet after I tore it up?\n(A) In my pocket\n(B) Under the sofa\n(C) On the table\n(D) Next to the toolbox\n(E) In the dustbin\n(F) In the drawer\nOnly give the best option.\nBest option: ("}
{"doc_id": 396, "doc": {"video_name": "test_ego_2.mp4", "duration": 388.76666662083335, "question": "What did I pour in the mug?\n(A) Juice\n(B) Soda\n(C) Tea\n(D) Water\n(E) Coffee\n(F) Milk", "candidates": ["Juice", "Soda", "Tea", "Water", "Coffee", "Milk"], "answer": "D", "task_type": "ego", "question_id": "ER_50", "indices": ["330", "570", "870", "1110", "1680", "1800", "2220", "2550", "2940", "3420", "3930", "4020", "4650", "4830", "5250", "5790", "5850", "6510", "6810", "6960", "7260", "7950", "8040", "8700", "8790", "9120", "9780", "9870", "10170", "10530", "10980", "11610"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Coffee"]], "filtered_resps": ["(E) Coffee"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff8b4741f7a839dcd0c9e895967fad859b8ac79622b4e14f2164b2122da65b6e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What did I pour in the mug?\n(A) Juice\n(B) Soda\n(C) Tea\n(D) Water\n(E) Coffee\n(F) Milk", "task_type": "ego", "pred_answer": "E", "answer": "D"}, "input": "What did I pour in the mug?\n(A) Juice\n(B) Soda\n(C) Tea\n(D) Water\n(E) Coffee\n(F) Milk\nOnly give the best option.\nBest option: ("}
{"doc_id": 400, "doc": {"video_name": "test_tutorial_30.mp4", "duration": 704.07, "question": "What operation is performed after opening the music app in the video?\n(A) Playing a song\n(B) Adjusting the volume\n(C) Browsing the music library\n(D) Searching for music\n(E) Selecting a playback mode\n(F) Frequent screen switching", "candidates": ["Playing a song", "Adjusting the volume", "Browsing the music library", "Searching for music", "Selecting a playback mode", "Frequent screen switching"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_1", "indices": ["0", "810", "1650", "2340", "2850", "3330", "4380", "5070", "5520", "6330", "7020", "7800", "7920", "8610", "9360", "10170", "11130", "11220", "12270", "12930", "13320", "14040", "14850", "15750", "16410", "16830", "17430", "18180", "19050", "19470", "20430", "21060"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "57008ec579f83ebb5cc4a1e134a88a33132efc2eab19f53927c002061462795e", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What operation is performed after opening the music app in the video?\n(A) Playing a song\n(B) Adjusting the volume\n(C) Browsing the music library\n(D) Searching for music\n(E) Selecting a playback mode\n(F) Frequent screen switching", "task_type": "tutorialQA", "pred_answer": "F", "answer": "F"}, "input": "What operation is performed after opening the music app in the video?\n(A) Playing a song\n(B) Adjusting the volume\n(C) Browsing the music library\n(D) Searching for music\n(E) Selecting a playback mode\n(F) Frequent screen switching\nOnly give the best option.\nBest option: ("}
{"doc_id": 404, "doc": {"video_name": "test_tutorial_15.mp4", "duration": 537.05, "question": "The sequence of handling the colors of the paper in the video\n(A) Black, Yellow, Yellow, Black, Green\n(B) Black, Green, Yellow, Yellow, Black\n(C) Black, Yellow, Green, Yellow, Black\n(D) Yellow, Green, Black, Yellow, Black\n(E) Black, Yellow, Yellow, Green, Black\n(F) Yellow, Black, Green, Yellow, Black", "candidates": ["Black, Yellow, Yellow, Black, Green", "Black, Green, Yellow, Yellow, Black", "Black, Yellow, Green, Yellow, Black", "Yellow, Green, Black, Yellow, Black", "Black, Yellow, Yellow, Green, Black", "Yellow, Black, Green, Yellow, Black"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_5", "indices": ["350", "575", "1075", "1325", "1775", "2225", "2875", "3000", "3700", "3950", "4500", "4600", "5350", "5725", "5950", "6625", "6700", "7175", "7900", "8150", "8650", "9075", "9525", "9625", "10300", "10650", "11200", "11300", "11875", "12500", "12850", "13275"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "62dee22628a75b56295630686545e261d758cee640f0a6f23bff985bf9486b77", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "The sequence of handling the colors of the paper in the video\n(A) Black, Yellow, Yellow, Black, Green\n(B) Black, Green, Yellow, Yellow, Black\n(C) Black, Yellow, Green, Yellow, Black\n(D) Yellow, Green, Black, Yellow, Black\n(E) Black, Yellow, Yellow, Green, Black\n(F) Yellow, Black, Green, Yellow, Black", "task_type": "tutorialQA", "pred_answer": "E", "answer": "C"}, "input": "The sequence of handling the colors of the paper in the video\n(A) Black, Yellow, Yellow, Black, Green\n(B) Black, Green, Yellow, Yellow, Black\n(C) Black, Yellow, Green, Yellow, Black\n(D) Yellow, Green, Black, Yellow, Black\n(E) Black, Yellow, Yellow, Green, Black\n(F) Yellow, Black, Green, Yellow, Black\nOnly give the best option.\nBest option: ("}
{"doc_id": 408, "doc": {"video_name": "test_tutorial_1.mp4", "duration": 1247.2, "question": "What is the sequence in which the man processes the vegetables?\n(A) Purple onion, red round chili, green vegetables, scallions, carrots, white onion\n(B) Green vegetables, red round chili, purple onion, scallions, carrots, white onion\n(C) Red round chili, purple onion, green vegetables, scallions, carrots, white onion\n(D) White onion, green vegetables, red round chili, scallions, carrots, purple onion\n(E) Scallions, red round chili, green vegetables, purple onion, carrots, white onion\n(F) Carrots, red round chili, green vegetables, scallions, purple onion, white onion", "candidates": ["Purple onion, red round chili, green vegetables, scallions, carrots, white onion", "Green vegetables, red round chili, purple onion, scallions, carrots, white onion", "Red round chili, purple onion, green vegetables, scallions, carrots, white onion", "White onion, green vegetables, red round chili, scallions, carrots, purple onion", "Scallions, red round chili, green vegetables, purple onion, carrots, white onion", "Carrots, red round chili, green vegetables, scallions, purple onion, white onion"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_9", "indices": ["475", "975", "2475", "3600", "3875", "5575", "6125", "7200", "8475", "9375", "9800", "11450", "11675", "13025", "13725", "15325", "16250", "16775", "17800", "18850", "20275", "21400", "21425", "23250", "23975", "25125", "25475", "26375", "27300", "28600", "30025", "30275"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "954e860a1a6a7791f7d1a6c63d0a1208d207b3bb97f9c9c0c5039e0811d5c3ed", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the sequence in which the man processes the vegetables?\n(A) Purple onion, red round chili, green vegetables, scallions, carrots, white onion\n(B) Green vegetables, red round chili, purple onion, scallions, carrots, white onion\n(C) Red round chili, purple onion, green vegetables, scallions, carrots, white onion\n(D) White onion, green vegetables, red round chili, scallions, carrots, purple onion\n(E) Scallions, red round chili, green vegetables, purple onion, carrots, white onion\n(F) Carrots, red round chili, green vegetables, scallions, purple onion, white onion", "task_type": "tutorialQA", "pred_answer": "C", "answer": "A"}, "input": "What is the sequence in which the man processes the vegetables?\n(A) Purple onion, red round chili, green vegetables, scallions, carrots, white onion\n(B) Green vegetables, red round chili, purple onion, scallions, carrots, white onion\n(C) Red round chili, purple onion, green vegetables, scallions, carrots, white onion\n(D) White onion, green vegetables, red round chili, scallions, carrots, purple onion\n(E) Scallions, red round chili, green vegetables, purple onion, carrots, white onion\n(F) Carrots, red round chili, green vegetables, scallions, purple onion, white onion\nOnly give the best option.\nBest option: ("}
{"doc_id": 412, "doc": {"video_name": "test_tutorial_14.mp4", "duration": 445.17, "question": "The sequence of handling the colors of the strips in the video\n(A) Green, White, Pink, Yellow\n(B) White, Pink, Green, Yellow\n(C) Green, Pink, Yellow, White\n(D) Green, Pink, White, Yellow\n(E) White, Pink, Yellow, Green\n(F) White, Green, Pink, Yellow", "candidates": ["Green, White, Pink, Yellow", "White, Pink, Green, Yellow", "Green, Pink, Yellow, White", "Green, Pink, White, Yellow", "White, Pink, Yellow, Green", "White, Green, Pink, Yellow"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_13", "indices": ["25", "375", "900", "1250", "1600", "2025", "2100", "2575", "2775", "3325", "3575", "3850", "4325", "4500", "4925", "5200", "5675", "5900", "6425", "6600", "7075", "7275", "7950", "8125", "8425", "9000", "9075", "9650", "9825", "10150", "10600", "10875"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bc41476f5f77d024c104d7b4999ae0fe8f0b66fe0a3694ee00be484aa7ef59e7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "The sequence of handling the colors of the strips in the video\n(A) Green, White, Pink, Yellow\n(B) White, Pink, Green, Yellow\n(C) Green, Pink, Yellow, White\n(D) Green, Pink, White, Yellow\n(E) White, Pink, Yellow, Green\n(F) White, Green, Pink, Yellow", "task_type": "tutorialQA", "pred_answer": "D", "answer": "A"}, "input": "The sequence of handling the colors of the strips in the video\n(A) Green, White, Pink, Yellow\n(B) White, Pink, Green, Yellow\n(C) Green, Pink, Yellow, White\n(D) Green, Pink, White, Yellow\n(E) White, Pink, Yellow, Green\n(F) White, Green, Pink, Yellow\nOnly give the best option.\nBest option: ("}
{"doc_id": 416, "doc": {"video_name": "test_tutorial_11.mp4", "duration": 191.87, "question": "The sequence of colors used for the strip of paper in the video\n(A) Pink, Yellow, Green, Green, Red\n(B) Green, Yellow, Pink, Green, Red\n(C) Green, Pink, Yellow, Green, Red\n(D) Green, Pink, Green, Red, Yellow\n(E) Green, Pink, Green, Yellow, Red\n(F) Yellow, Green, Pink, Green, Red", "candidates": ["Pink, Yellow, Green, Green, Red", "Green, Yellow, Pink, Green, Red", "Green, Pink, Yellow, Green, Red", "Green, Pink, Green, Red, Yellow", "Green, Pink, Green, Yellow, Red", "Yellow, Green, Pink, Green, Red"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_17", "indices": ["90", "180", "420", "570", "780", "930", "1200", "1350", "1500", "1590", "1890", "2040", "2130", "2310", "2640", "2670", "2850", "3030", "3330", "3390", "3600", "3870", "3990", "4170", "4290", "4620", "4710", "4890", "5100", "5340", "5370", "5580"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "289bbbfec286d09b9e18bd2d24d3bf39fc0913e71190e5ef235342f243b0fe58", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "The sequence of colors used for the strip of paper in the video\n(A) Pink, Yellow, Green, Green, Red\n(B) Green, Yellow, Pink, Green, Red\n(C) Green, Pink, Yellow, Green, Red\n(D) Green, Pink, Green, Red, Yellow\n(E) Green, Pink, Green, Yellow, Red\n(F) Yellow, Green, Pink, Green, Red", "task_type": "tutorialQA", "pred_answer": "F", "answer": "F"}, "input": "The sequence of colors used for the strip of paper in the video\n(A) Pink, Yellow, Green, Green, Red\n(B) Green, Yellow, Pink, Green, Red\n(C) Green, Pink, Yellow, Green, Red\n(D) Green, Pink, Green, Red, Yellow\n(E) Green, Pink, Green, Yellow, Red\n(F) Yellow, Green, Pink, Green, Red\nOnly give the best option.\nBest option: ("}
{"doc_id": 420, "doc": {"video_name": "test_tutorial_19.mp4", "duration": 386.24, "question": "Which action is performed after marking the paper with a pencil\n(A) Cutting\n(B) Scanning\n(C) Storing\n(D) Writing\n(E) Erasing\n(F) Filing", "candidates": ["Cutting", "Scanning", "Storing", "Writing", "Erasing", "Filing"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_21", "indices": ["210", "690", "990", "1080", "1680", "2070", "2400", "2790", "3180", "3450", "3750", "4110", "4650", "4800", "5370", "5610", "6060", "6330", "6810", "6960", "7500", "7680", "8010", "8610", "8760", "9360", "9540", "10050", "10170", "10620", "11100", "11220"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8225222793950e29eee3e341305cc919d32c9d7b439eb7259be2c5329b2a86f1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which action is performed after marking the paper with a pencil\n(A) Cutting\n(B) Scanning\n(C) Storing\n(D) Writing\n(E) Erasing\n(F) Filing", "task_type": "tutorialQA", "pred_answer": "A", "answer": "A"}, "input": "Which action is performed after marking the paper with a pencil\n(A) Cutting\n(B) Scanning\n(C) Storing\n(D) Writing\n(E) Erasing\n(F) Filing\nOnly give the best option.\nBest option: ("}
{"doc_id": 424, "doc": {"video_name": "test_tutorial_28.mp4", "duration": 3553.39, "question": "In the video, after tearing the paper off the surface of the black panels, what operation is performed?\n(A) Testing functionality\n(B) Securing the panels\n(C) Discarding the panels\n(D) Numbering each panel surface\n(E) Installing equipment\n(F) Cleaning the surface", "candidates": ["Testing functionality", "Securing the panels", "Discarding the panels", "Numbering each panel surface", "Installing equipment", "Cleaning the surface"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_25", "indices": ["2117", "3538", "9802", "12151", "15312", "18792", "22243", "23896", "26564", "29928", "35815", "38889", "41760", "45095", "48198", "50170", "56318", "56347", "62727", "66207", "67193", "70180", "74327", "76270", "81896", "82882", "89436", "91379", "93235", "99441", "100253", "103153"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Installing equipment"]], "filtered_resps": ["(E) Installing equipment"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb02bbbd3ce295e3c3bef9748a816a01b3cff7b62a2a3c8b2563bc68f64c6934", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, after tearing the paper off the surface of the black panels, what operation is performed?\n(A) Testing functionality\n(B) Securing the panels\n(C) Discarding the panels\n(D) Numbering each panel surface\n(E) Installing equipment\n(F) Cleaning the surface", "task_type": "tutorialQA", "pred_answer": "E", "answer": "D"}, "input": "In the video, after tearing the paper off the surface of the black panels, what operation is performed?\n(A) Testing functionality\n(B) Securing the panels\n(C) Discarding the panels\n(D) Numbering each panel surface\n(E) Installing equipment\n(F) Cleaning the surface\nOnly give the best option.\nBest option: ("}
{"doc_id": 428, "doc": {"video_name": "test_tutorial_15.mp4", "duration": 537.05, "question": "Which action is performed after cutting the green paper?\n(A) Folding it into a specific shape or pattern\n(B) Gluing the yellow paper onto the green paper\n(C) Making it into a bookmark\n(D) Attaching it to a greeting card\n(E) Putting it into an envelope\n(F) Placing it into a decorative glass bottle", "candidates": ["Folding it into a specific shape or pattern", "Gluing the yellow paper onto the green paper", "Making it into a bookmark", "Attaching it to a greeting card", "Putting it into an envelope", "Placing it into a decorative glass bottle"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_29", "indices": ["6625", "6725", "6750", "6800", "6825", "6850", "6875", "6900", "6925", "6975", "7200", "7225", "7250", "7275", "7300", "7325", "7350", "7400", "7425", "7450", "7475", "7500", "7525", "7850", "7875", "7900", "7925", "7950", "7975", "8050", "8100", "8125"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f48913428d6ecfc2758b08bd7d8e691b02a77e3d82e8d972f87c49cc564bc72", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which action is performed after cutting the green paper?\n(A) Folding it into a specific shape or pattern\n(B) Gluing the yellow paper onto the green paper\n(C) Making it into a bookmark\n(D) Attaching it to a greeting card\n(E) Putting it into an envelope\n(F) Placing it into a decorative glass bottle", "task_type": "tutorialQA", "pred_answer": "B", "answer": "B"}, "input": "Which action is performed after cutting the green paper?\n(A) Folding it into a specific shape or pattern\n(B) Gluing the yellow paper onto the green paper\n(C) Making it into a bookmark\n(D) Attaching it to a greeting card\n(E) Putting it into an envelope\n(F) Placing it into a decorative glass bottle\nOnly give the best option.\nBest option: ("}
{"doc_id": 432, "doc": {"video_name": "test_tutorial_8.mp4", "duration": 506.73, "question": "Which action did the man take after planting the plant in the flower pot?\n(A) Adding fertilizer\n(B) Trimming the plant\n(C) Explanation\n(D) Placing it in a sunny location\n(E) Taking a picture of the plant\n(F) Watering", "candidates": ["Adding fertilizer", "Trimming the plant", "Explanation", "Placing it in a sunny location", "Taking a picture of the plant", "Watering"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_33", "indices": ["348", "725", "1160", "1682", "2001", "2697", "2813", "3712", "4118", "4379", "4727", "5394", "5945", "6496", "6612", "7279", "7859", "8178", "8758", "9106", "9628", "10005", "10498", "11339", "11368", "12209", "12615", "12760", "13514", "13833", "14355", "14877"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "619d1058e2d0b959cdef2b5d582300b60edeedb0280b5cf74c9101e4529c39b9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which action did the man take after planting the plant in the flower pot?\n(A) Adding fertilizer\n(B) Trimming the plant\n(C) Explanation\n(D) Placing it in a sunny location\n(E) Taking a picture of the plant\n(F) Watering", "task_type": "tutorialQA", "pred_answer": "D", "answer": "C"}, "input": "Which action did the man take after planting the plant in the flower pot?\n(A) Adding fertilizer\n(B) Trimming the plant\n(C) Explanation\n(D) Placing it in a sunny location\n(E) Taking a picture of the plant\n(F) Watering\nOnly give the best option.\nBest option: ("}
{"doc_id": 436, "doc": {"video_name": "test_tutorial_22.mp4", "duration": 1182.94, "question": "What is the main technique used when processing the photo of a person wearing a necklace?\n(A) Exposure adjustment\n(B) Color correction\n(C) Sharpening\n(D) Saturation adjustment\n(E) Erasing\n(F) Contrast enhancement", "candidates": ["Exposure adjustment", "Color correction", "Sharpening", "Saturation adjustment", "Erasing", "Contrast enhancement"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_37", "indices": ["1050", "2160", "2190", "3750", "5490", "5580", "6720", "8520", "9930", "10770", "11130", "13200", "14280", "14880", "16560", "17400", "18780", "19320", "20370", "21930", "22530", "23340", "24480", "26370", "27360", "28560", "29040", "30900", "31080", "32130", "33720", "34680"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Erasing"]], "filtered_resps": ["(E) Erasing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "002ae7cd522d1836b32f0b92bb09cec99efeb4b5693520283d6901ab0cf13c0d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the main technique used when processing the photo of a person wearing a necklace?\n(A) Exposure adjustment\n(B) Color correction\n(C) Sharpening\n(D) Saturation adjustment\n(E) Erasing\n(F) Contrast enhancement", "task_type": "tutorialQA", "pred_answer": "E", "answer": "E"}, "input": "What is the main technique used when processing the photo of a person wearing a necklace?\n(A) Exposure adjustment\n(B) Color correction\n(C) Sharpening\n(D) Saturation adjustment\n(E) Erasing\n(F) Contrast enhancement\nOnly give the best option.\nBest option: ("}
{"doc_id": 440, "doc": {"video_name": "test_tutorial_29.mp4", "duration": 775.45, "question": "In the video, after demonstrating the calculator function, what function is shown next?\n(A) Photography feature\n(B) Weather application\n(C) Email client\n(D) Translation function\n(E) Calendar application\n(F) Maps and navigation", "candidates": ["Photography feature", "Weather application", "Email client", "Translation function", "Calendar application", "Maps and navigation"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_41", "indices": ["200", "875", "1675", "2275", "2550", "3325", "3600", "4475", "5350", "5425", "6075", "6950", "7525", "7950", "8550", "9075", "10225", "10275", "11400", "11575", "12275", "12850", "13300", "14375", "14950", "15650", "15975", "16350", "17375", "18100", "18350", "18900"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Calendar application"]], "filtered_resps": ["(E) Calendar application"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2e2591a46ae358e7d47f1d28fe155d66f062309a944b222a60bf563b16c477aa", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, after demonstrating the calculator function, what function is shown next?\n(A) Photography feature\n(B) Weather application\n(C) Email client\n(D) Translation function\n(E) Calendar application\n(F) Maps and navigation", "task_type": "tutorialQA", "pred_answer": "E", "answer": "D"}, "input": "In the video, after demonstrating the calculator function, what function is shown next?\n(A) Photography feature\n(B) Weather application\n(C) Email client\n(D) Translation function\n(E) Calendar application\n(F) Maps and navigation\nOnly give the best option.\nBest option: ("}
{"doc_id": 444, "doc": {"video_name": "test_count_24.mp4", "duration": 627.42, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 0\n(B) 6\n(C) 3\n(D) 5\n(E) 2\n(F) 4", "candidates": ["0", "6", "3", "5", "2", "4"], "answer": "B", "task_type": "count", "question_id": "AC_2", "indices": ["377", "406", "435", "464", "522", "1537", "1566", "1624", "6322", "6409", "9715", "9744", "9773", "9802", "9831", "9860", "9889", "9918", "9947", "9976", "10005", "18444", "18473", "18502", "18531", "18560", "18589", "18618", "18647", "18676", "18705", "18734"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 3"]], "filtered_resps": ["(C) 3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b0d96fd96abacbcb9ba6ad008d4bb6af4d2d8378ac3e1b316d42ff5fba107dca", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 0\n(B) 6\n(C) 3\n(D) 5\n(E) 2\n(F) 4", "task_type": "count", "pred_answer": "C", "answer": "B"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 0\n(B) 6\n(C) 3\n(D) 5\n(E) 2\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 448, "doc": {"video_name": "test_count_0.mp4", "duration": 500.01, "question": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 5\n(E) 1\n(F) 3", "candidates": ["2", "4", "6", "5", "1", "3"], "answer": "A", "task_type": "count", "question_id": "AC_6", "indices": ["261", "493", "1334", "1392", "1914", "2581", "3190", "3248", "3973", "4582", "4814", "5365", "5800", "6090", "6960", "7105", "7917", "8033", "8468", "9280", "9396", "10092", "10672", "11165", "11223", "12006", "12151", "12760", "13369", "13949", "14355", "14703"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 1"]], "filtered_resps": ["(E) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb584a7bf48608dd30b8b232c0e9d8d799110ab3709aff12f85a2e9e9c042202", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 5\n(E) 1\n(F) 3", "task_type": "count", "pred_answer": "E", "answer": "A"}, "input": "In this video, how many times does the scene of the 'abseiling' action appear in total?\n(A) 2\n(B) 4\n(C) 6\n(D) 5\n(E) 1\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 452, "doc": {"video_name": "test_count_56.mp4", "duration": 460.14, "question": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 2\n(E) 1\n(F) 5", "candidates": ["3", "6", "0", "2", "1", "5"], "answer": "E", "task_type": "count", "question_id": "AC_10", "indices": ["348", "406", "1015", "1392", "2059", "2146", "2668", "3277", "3480", "4031", "4350", "4756", "5162", "5829", "6206", "6467", "7134", "7569", "7743", "8294", "8874", "9367", "9541", "10266", "10701", "10991", "11397", "11977", "12238", "12731", "12963", "13601"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3364e659f204ad59b189f00f9bc12025db842cbff8f39691d63c597f7c104c71", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 2\n(E) 1\n(F) 5", "task_type": "count", "pred_answer": "E", "answer": "E"}, "input": "In this video, how many times does the scene of the 'paragliding' action appear in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 2\n(E) 1\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 456, "doc": {"video_name": "test_count_19.mp4", "duration": 314.03999999999996, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "candidates": ["1", "3", "4", "2", "5", "0"], "answer": "D", "task_type": "count", "question_id": "AC_14", "indices": ["116", "464", "841", "899", "1247", "1595", "1972", "2088", "2552", "2639", "3045", "3306", "3654", "4002", "4292", "4437", "4785", "5075", "5423", "5742", "6003", "6235", "6525", "6960", "7163", "7337", "7714", "7946", "8381", "8497", "8903", "9280"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a13a8a978655d3346a575f91eb709b46fa017e4e16ac1f23b82bca76c44df51d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 3\n(C) 4\n(D) 2\n(E) 5\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 460, "doc": {"video_name": "test_count_39.mp4", "duration": 593.1, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 6\n(E) 0\n(F) 2", "candidates": ["5", "3", "4", "6", "0", "2"], "answer": "F", "task_type": "count", "question_id": "AC_18", "indices": ["510", "660", "1290", "1800", "2370", "3000", "3510", "4020", "4560", "5460", "5880", "6360", "6780", "7200", "7890", "8700", "9330", "9690", "10200", "11010", "11610", "11700", "12600", "12900", "13800", "14040", "14850", "15270", "15990", "16290", "16830", "17670"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 0"]], "filtered_resps": ["(E) 0"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae69b9eec4a1d03889ec66cef6e030fec8e6818fee25be3a94ff6038add86838", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 6\n(E) 0\n(F) 2", "task_type": "count", "pred_answer": "E", "answer": "F"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 5\n(B) 3\n(C) 4\n(D) 6\n(E) 0\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 464, "doc": {"video_name": "test_count_48.mp4", "duration": 728.68, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 1\n(C) 2\n(D) 3\n(E) 5\n(F) 4", "candidates": ["0", "1", "2", "3", "5", "4"], "answer": "E", "task_type": "count", "question_id": "AC_22", "indices": ["1740", "1769", "1827", "1856", "1885", "1914", "1943", "1972", "2552", "2581", "2610", "2639", "2668", "2697", "2726", "2755", "2784", "2813", "5220", "5249", "5278", "5307", "5336", "12934", "13050", "18850", "18879", "18908", "18937", "18966", "18995", "19024"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5164974239f302dc67ea784637f588ab425828fbd74eae2dd8c9fa8ef95a2af2", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 1\n(C) 2\n(D) 3\n(E) 5\n(F) 4", "task_type": "count", "pred_answer": "D", "answer": "E"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'cooking sausages' action\n(A) 0\n(B) 1\n(C) 2\n(D) 3\n(E) 5\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 468, "doc": {"video_name": "test_count_49.mp4", "duration": 470.09999999999997, "question": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 6\n(B) 3\n(C) 1\n(D) 2\n(E) 0\n(F) 4", "candidates": ["6", "3", "1", "2", "0", "4"], "answer": "D", "task_type": "count", "question_id": "AC_26", "indices": ["90", "570", "1140", "1590", "2130", "2550", "3000", "3480", "3810", "3930", "4380", "5190", "5520", "5760", "6480", "6780", "7200", "7560", "7920", "8340", "8820", "9240", "9990", "10110", "10560", "11100", "11490", "12090", "12720", "12750", "13200", "13890"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 1"]], "filtered_resps": ["(C) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ea418b3ee12ed65accc0ddba8b8d675faa09c818952bb70980f3a225ac6ac2a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 6\n(B) 3\n(C) 1\n(D) 2\n(E) 0\n(F) 4", "task_type": "count", "pred_answer": "C", "answer": "D"}, "input": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 6\n(B) 3\n(C) 1\n(D) 2\n(E) 0\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 472, "doc": {"video_name": "test_count_28.mp4", "duration": 685.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 4\n(B) 0\n(C) 6\n(D) 3\n(E) 1\n(F) 2", "candidates": ["4", "0", "6", "3", "1", "2"], "answer": "E", "task_type": "count", "question_id": "AC_30", "indices": ["180", "660", "1410", "2160", "2610", "3330", "4170", "5100", "5160", "5760", "6750", "7290", "8190", "8340", "9090", "9750", "10830", "11490", "11970", "12780", "12810", "13650", "14670", "14970", "15810", "16350", "17100", "17370", "18360", "19200", "19470", "19980"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "428f1e8f98be6c657bd1470bc953936812175744d7b8ac6134eb6d9122b31c1b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 4\n(B) 0\n(C) 6\n(D) 3\n(E) 1\n(F) 2", "task_type": "count", "pred_answer": "B", "answer": "E"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing harp' action\n(A) 4\n(B) 0\n(C) 6\n(D) 3\n(E) 1\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 476, "doc": {"video_name": "test_count_45.mp4", "duration": 540.04, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 3\n(E) 4\n(F) 5", "candidates": ["6", "0", "1", "3", "4", "5"], "answer": "A", "task_type": "count", "question_id": "AC_34", "indices": ["3780", "3810", "3840", "3870", "3900", "3960", "3990", "4020", "4050", "6690", "6720", "6750", "6780", "6810", "13020", "13050", "13080", "13110", "13140", "13170", "13200", "13230", "13260", "13290", "14400", "14430", "14460", "14520", "14550", "14580", "14610", "14640"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6ebc163ee885a4c7d0359ab4a62ceb8493eb9b0de45a238315cd042d24575e19", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 3\n(E) 4\n(F) 5", "task_type": "count", "pred_answer": "D", "answer": "A"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 1\n(D) 3\n(E) 4\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 480, "doc": {"video_name": "test_count_52.mp4", "duration": 530.64, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 5\n(F) 0", "candidates": ["6", "2", "1", "4", "5", "0"], "answer": "A", "task_type": "count", "question_id": "AC_38", "indices": ["1350", "1380", "1410", "1440", "3210", "3330", "3360", "3390", "3420", "3450", "5820", "5850", "5880", "5910", "5940", "5970", "6000", "6030", "6060", "6090", "12300", "12330", "12360", "12420", "12450", "12480", "12570", "15240", "15330", "15360", "15390", "15450"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fd794427a12bd30ff65728ba46147d8881c46172b05dfa2b9d41978198a56cf", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 5\n(F) 0", "task_type": "count", "pred_answer": "E", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 5\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 484, "doc": {"video_name": "test_count_38.mp4", "duration": 583.1, "question": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 1\n(C) 6\n(D) 0\n(E) 4\n(F) 3", "candidates": ["2", "1", "6", "0", "4", "3"], "answer": "B", "task_type": "count", "question_id": "AC_42", "indices": ["175", "800", "1175", "1350", "1875", "2250", "3100", "3550", "3700", "4300", "4650", "5050", "5750", "6325", "6625", "7000", "7575", "7800", "8450", "9025", "9275", "9850", "10100", "10750", "11000", "11675", "11875", "12525", "12800", "13625", "13800", "14425"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 1"]], "filtered_resps": ["(B) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d849a3102615402cd271a887b402eb3bc1583461ed0b3bc75f22c9f34f18ab33", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 1\n(C) 6\n(D) 0\n(E) 4\n(F) 3", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing trombone' action scene in total?\n(A) 2\n(B) 1\n(C) 6\n(D) 0\n(E) 4\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 488, "doc": {"video_name": "test_count_5.mp4", "duration": 470.03999999999996, "question": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 2", "candidates": ["5", "3", "1", "0", "4", "2"], "answer": "F", "task_type": "count", "question_id": "AC_46", "indices": ["210", "510", "870", "1470", "2070", "2400", "2970", "3210", "3630", "4260", "4380", "4860", "5550", "5910", "6180", "6840", "7350", "7740", "8100", "8520", "8880", "9540", "9960", "10530", "10800", "10980", "11550", "12240", "12630", "12930", "13290", "13680"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f816fd30b9fb7d45097360117c7b350b3cd1bfc1fdd8cc862fe8ed178b9e545c", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 2", "task_type": "count", "pred_answer": "D", "answer": "F"}, "input": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 5\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 492, "doc": {"video_name": "test_count_6.mp4", "duration": 560.25, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 4\n(E) 0\n(F) 2", "candidates": ["1", "5", "6", "4", "0", "2"], "answer": "B", "task_type": "count", "question_id": "AC_50", "indices": ["0", "780", "1050", "1950", "2520", "2670", "3630", "3690", "4500", "4770", "5490", "5910", "6480", "7260", "7830", "7860", "8550", "9120", "9510", "10170", "10920", "11160", "11880", "12390", "12720", "13530", "13950", "14220", "14760", "15600", "15930", "16770"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b6c6fffd4678992c023433f85dee7100f61b25d1aef5aad18dbaa513fc5f9c3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 4\n(E) 0\n(F) 2", "task_type": "count", "pred_answer": "F", "answer": "B"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 1\n(B) 5\n(C) 6\n(D) 4\n(E) 0\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 496, "doc": {"video_name": "test_count_47.mp4", "duration": 510.03999999999996, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n(E) 3\n(F) 4", "candidates": ["6", "1", "2", "0", "3", "4"], "answer": "A", "task_type": "count", "question_id": "AC_54", "indices": ["420", "540", "1140", "1830", "2250", "2370", "3270", "3720", "3990", "4470", "4950", "5490", "5940", "6210", "6870", "7260", "8010", "8550", "8580", "9420", "9540", "10440", "10920", "11100", "11850", "12090", "12840", "13350", "13380", "14130", "14370", "14820"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82d2144e47a5e2469c214a4be1c142b831b6da52fafd81b5d167ebea2dd7d6a4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n(E) 3\n(F) 4", "task_type": "count", "pred_answer": "B", "answer": "A"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'shredding paper' action\n(A) 6\n(B) 1\n(C) 2\n(D) 0\n(E) 3\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 500, "doc": {"video_name": "test_count_42.mp4", "duration": 453.16999999999996, "question": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n(E) 4\n(F) 2", "candidates": ["5", "6", "3", "0", "4", "2"], "answer": "B", "task_type": "count", "question_id": "AC_58", "indices": ["3300", "3330", "3360", "3390", "3420", "3450", "3480", "3540", "3570", "4470", "4500", "4560", "4590", "4680", "4710", "6120", "6150", "6180", "6210", "6240", "6300", "6330", "6360", "6390", "7470", "7500", "7530", "7560", "8880", "8940", "9000", "9030"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a5e7f5e15729f2de06759e682675015e2575bce26e2d6487b35e8106685d89a9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n(E) 4\n(F) 2", "task_type": "count", "pred_answer": "E", "answer": "B"}, "input": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 5\n(B) 6\n(C) 3\n(D) 0\n(E) 4\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 1, "doc": {"video_name": "test_surveil_30.mp4", "duration": 198.2, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Explosion\n(C) Abuse\n(D) Robbery\n(E) Arrest\n(F) Assault", "candidates": ["RoadAccidents", "Explosion", "Abuse", "Robbery", "Arrest", "Assault"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_1", "indices": ["120", "270", "390", "690", "840", "960", "1110", "1410", "1560", "1710", "1950", "2040", "2340", "2520", "2730", "2850", "3000", "3150", "3330", "3510", "3840", "3930", "4170", "4290", "4530", "4650", "4800", "4980", "5250", "5430", "5670", "5730"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Robbery"]], "filtered_resps": ["(D) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "630ede51542f94076a829385405d1d0ab948c566a33350f10647b21c15ab37d9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Explosion\n(C) Abuse\n(D) Robbery\n(E) Arrest\n(F) Assault", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "A"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) RoadAccidents\n(B) Explosion\n(C) Abuse\n(D) Robbery\n(E) Arrest\n(F) Assault\nOnly give the best option.\nBest option: ("}
{"doc_id": 5, "doc": {"video_name": "test_surveil_10.mp4", "duration": 4730.04, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Shoplifting\n(D) Robbery\n(E) Fighting\n(F) Explosion", "candidates": ["Stealing", "Vandalism", "Shoplifting", "Robbery", "Fighting", "Explosion"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_5", "indices": ["2700", "5010", "10710", "16590", "18660", "22260", "30480", "32010", "38640", "40380", "47910", "49590", "57570", "60630", "62700", "66840", "75000", "79740", "84090", "85620", "89400", "93660", "100230", "102840", "108660", "115050", "116040", "122490", "125520", "130170", "136890", "138960"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Explosion"]], "filtered_resps": ["(F) Explosion"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b896e4f548a8966ffeaba75bb97f325c50df3e2f7904d03dd0c42b1b871e96a9", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Shoplifting\n(D) Robbery\n(E) Fighting\n(F) Explosion", "task_type": "anomaly_reco", "pred_answer": "F", "answer": "F"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Stealing\n(B) Vandalism\n(C) Shoplifting\n(D) Robbery\n(E) Fighting\n(F) Explosion\nOnly give the best option.\nBest option: ("}
{"doc_id": 9, "doc": {"video_name": "test_surveil_33.mp4", "duration": 359.97, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Normal\n(D) Abuse\n(E) Assault\n(F) RoadAccidents", "candidates": ["Arrest", "Arson", "Normal", "Abuse", "Assault", "RoadAccidents"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_9", "indices": ["0", "360", "840", "1230", "1620", "1800", "2160", "2520", "2880", "3060", "3630", "3750", "4290", "4440", "4890", "5130", "5400", "5850", "6240", "6540", "6990", "7200", "7470", "7890", "8370", "8400", "8760", "9060", "9690", "9810", "10290", "10590"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Assault"]], "filtered_resps": ["(E) Assault"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aa6a3696710b40cb56818e914b68b130b0b97b4be4d22e5c9a5861dde11e2d4c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Normal\n(D) Abuse\n(E) Assault\n(F) RoadAccidents", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arrest\n(B) Arson\n(C) Normal\n(D) Abuse\n(E) Assault\n(F) RoadAccidents\nOnly give the best option.\nBest option: ("}
{"doc_id": 13, "doc": {"video_name": "test_surveil_29.mp4", "duration": 781.1, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Assault\n(D) Fighting\n(E) Shooting\n(F) Shooting", "candidates": ["Vandalism", "Shoplifting", "Assault", "Fighting", "Shooting", "Shooting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_13", "indices": ["150", "870", "1950", "2880", "3180", "4200", "4860", "5640", "6180", "6930", "7440", "8400", "8910", "9900", "10890", "11550", "11970", "12570", "13620", "14070", "14910", "15450", "16620", "16950", "18240", "18750", "19560", "19890", "21030", "21690", "22650", "23280"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Shoplifting"]], "filtered_resps": ["(B) Shoplifting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1144981576b08171991231a15a4a1863988828de4a833f304c07f4651693b7b3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Assault\n(D) Fighting\n(E) Shooting\n(F) Shooting", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Vandalism\n(B) Shoplifting\n(C) Assault\n(D) Fighting\n(E) Shooting\n(F) Shooting\nOnly give the best option.\nBest option: ("}
{"doc_id": 17, "doc": {"video_name": "test_surveil_36.mp4", "duration": 722.77, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Abuse\n(C) Stealing\n(D) Fighting\n(E) RoadAccidents\n(F) Explosion", "candidates": ["Shooting", "Abuse", "Stealing", "Fighting", "RoadAccidents", "Explosion"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_17", "indices": ["420", "990", "1860", "2190", "2790", "3600", "4200", "4770", "5910", "6210", "6960", "8010", "8160", "9360", "10080", "10680", "10890", "11850", "12420", "12930", "13980", "14730", "15180", "15600", "16650", "17430", "18090", "18810", "19290", "19950", "20400", "21120"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) RoadAccidents"]], "filtered_resps": ["(E) RoadAccidents"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e5478cf4df873514f9df7aff83ec214afeae93cd1c5f132e043ec8aa9dae1c4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Abuse\n(C) Stealing\n(D) Fighting\n(E) RoadAccidents\n(F) Explosion", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "A"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shooting\n(B) Abuse\n(C) Stealing\n(D) Fighting\n(E) RoadAccidents\n(F) Explosion\nOnly give the best option.\nBest option: ("}
{"doc_id": 21, "doc": {"video_name": "test_surveil_9.mp4", "duration": 543.0, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Robbery\n(E) Explosion\n(F) Fighting", "candidates": ["Burglary", "Shooting", "Normal", "Robbery", "Explosion", "Fighting"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_21", "indices": ["120", "600", "1440", "1560", "2190", "2940", "3510", "3630", "4140", "4650", "5400", "5880", "6420", "6600", "7140", "7920", "8160", "8970", "9360", "9780", "10200", "10650", "11400", "11820", "12180", "12780", "13230", "13800", "14400", "15000", "15600", "15840"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Explosion"]], "filtered_resps": ["(E) Explosion"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b14553f204cfa09cc95d3ac7400705f36c771e60d3105cbf39630185c2469bf8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Robbery\n(E) Explosion\n(F) Fighting", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "E"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Normal\n(D) Robbery\n(E) Explosion\n(F) Fighting\nOnly give the best option.\nBest option: ("}
{"doc_id": 25, "doc": {"video_name": "test_surveil_22.mp4", "duration": 330.94, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Shooting\n(D) Shooting\n(E) Arrest\n(F) Vandalism", "candidates": ["Normal", "Fighting", "Shooting", "Shooting", "Arrest", "Vandalism"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_25", "indices": ["180", "450", "600", "990", "1260", "1770", "1830", "2430", "2460", "2970", "3180", "3630", "3840", "4110", "4350", "4740", "5190", "5400", "5610", "5910", "6210", "6660", "6930", "7290", "7590", "7890", "8100", "8460", "8730", "8940", "9330", "9840"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b122aa61527ca1e97ec464c6bf6cc28966701a15676497579560ebbf1bf6fc2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Shooting\n(D) Shooting\n(E) Arrest\n(F) Vandalism", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Normal\n(B) Fighting\n(C) Shooting\n(D) Shooting\n(E) Arrest\n(F) Vandalism\nOnly give the best option.\nBest option: ("}
{"doc_id": 29, "doc": {"video_name": "test_surveil_2.mp4", "duration": 210.07, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Explosion\n(D) Vandalism\n(E) Robbery\n(F) Assault", "candidates": ["Stealing", "RoadAccidents", "Explosion", "Vandalism", "Robbery", "Assault"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_29", "indices": ["0", "300", "510", "720", "840", "960", "1200", "1380", "1710", "1890", "2010", "2310", "2430", "2610", "2730", "3030", "3300", "3360", "3630", "3900", "3990", "4170", "4380", "4650", "4800", "4950", "5130", "5340", "5610", "5700", "5910", "6120"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Robbery"]], "filtered_resps": ["(E) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fb90209513a2f30f5bb5495d19046b36d4453ab184f92341f60adc70d38f2798", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Explosion\n(D) Vandalism\n(E) Robbery\n(F) Assault", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Stealing\n(B) RoadAccidents\n(C) Explosion\n(D) Vandalism\n(E) Robbery\n(F) Assault\nOnly give the best option.\nBest option: ("}
{"doc_id": 33, "doc": {"video_name": "test_surveil_28.mp4", "duration": 192.37, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Normal\n(C) Burglary\n(D) Abuse\n(E) Shoplifting\n(F) Vandalism", "candidates": ["Explosion", "Normal", "Burglary", "Abuse", "Shoplifting", "Vandalism"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_33", "indices": ["90", "270", "510", "630", "780", "930", "1230", "1350", "1530", "1620", "1860", "2130", "2160", "2490", "2520", "2790", "2910", "3090", "3300", "3540", "3600", "3810", "3990", "4200", "4320", "4590", "4800", "4860", "5160", "5370", "5460", "5700"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Shoplifting"]], "filtered_resps": ["(E) Shoplifting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "822218e572b99a542f369f7c0bb14aa52f58043bed3053eb6205b7c114e6de4c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Normal\n(C) Burglary\n(D) Abuse\n(E) Shoplifting\n(F) Vandalism", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "E"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Explosion\n(B) Normal\n(C) Burglary\n(D) Abuse\n(E) Shoplifting\n(F) Vandalism\nOnly give the best option.\nBest option: ("}
{"doc_id": 37, "doc": {"video_name": "test_surveil_4.mp4", "duration": 288.02, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Arrest\n(E) Abuse\n(F) Robbery", "candidates": ["Arson", "Shoplifting", "Explosion", "Arrest", "Abuse", "Robbery"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_37", "indices": ["210", "510", "720", "870", "1140", "1470", "1800", "1920", "2220", "2430", "2880", "3210", "3480", "3690", "3870", "4260", "4410", "4800", "4890", "5220", "5610", "5760", "6180", "6240", "6720", "6990", "7170", "7320", "7800", "7860", "8130", "8400"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Explosion"]], "filtered_resps": ["(C) Explosion"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dff2d5bf042f42107a0a225d18f1ce13ab81214c8fb623a3e6c0b1813e1b7055", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Arrest\n(E) Abuse\n(F) Robbery", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Arson\n(B) Shoplifting\n(C) Explosion\n(D) Arrest\n(E) Abuse\n(F) Robbery\nOnly give the best option.\nBest option: ("}
{"doc_id": 41, "doc": {"video_name": "test_sports_27.mp4", "duration": 686.56, "question": "How many times is a team member thrown into the air from the water in the video?\n(A) 4 times\n(B) 1 time\n(C) 6 times\n(D) 3 times\n(E) 2 times\n(F) 5 times", "candidates": ["4 times", "1 time", "6 times", "3 times", "2 times", "5 times"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_2", "indices": ["200", "1025", "1350", "1925", "2225", "2975", "3250", "4000", "4575", "5325", "5600", "6050", "6500", "7400", "7800", "8350", "8675", "9575", "9875", "10200", "10875", "11350", "12100", "12800", "13000", "13825", "13975", "14525", "15350", "15675", "16575", "16600"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a3c200397c23949af18a28eb2ccfaf128bf57554219f65e55b4a35e0259dce0", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many times is a team member thrown into the air from the water in the video?\n(A) 4 times\n(B) 1 time\n(C) 6 times\n(D) 3 times\n(E) 2 times\n(F) 5 times", "task_type": "sportsQA", "pred_answer": "E", "answer": "D"}, "input": "How many times is a team member thrown into the air from the water in the video?\n(A) 4 times\n(B) 1 time\n(C) 6 times\n(D) 3 times\n(E) 2 times\n(F) 5 times\nOnly give the best option.\nBest option: ("}
{"doc_id": 45, "doc": {"video_name": "test_sports_36.mp4", "duration": 960.4, "question": "Which country takes the lead after the second baton pass in the video?\n(A) Canada\n(B) Sweden\n(C) Russia\n(D) Africa\n(E) Brazil\n(F) United States", "candidates": ["Canada", "Sweden", "Russia", "Africa", "Brazil", "United States"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_6", "indices": ["375", "1425", "2175", "2400", "3325", "3750", "5225", "5400", "6675", "7075", "7825", "8800", "9675", "10450", "11150", "11825", "12325", "13150", "13675", "14475", "15025", "15925", "16550", "17775", "18725", "18750", "19525", "20725", "21650", "21750", "22900", "23850"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b67d6347feabe984f8b859f095829c6bb0a0f09653d9029f27a6a1125bd4c638", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which country takes the lead after the second baton pass in the video?\n(A) Canada\n(B) Sweden\n(C) Russia\n(D) Africa\n(E) Brazil\n(F) United States", "task_type": "sportsQA", "pred_answer": "F", "answer": "F"}, "input": "Which country takes the lead after the second baton pass in the video?\n(A) Canada\n(B) Sweden\n(C) Russia\n(D) Africa\n(E) Brazil\n(F) United States\nOnly give the best option.\nBest option: ("}
{"doc_id": 49, "doc": {"video_name": "test_sports_36.mp4", "duration": 960.4, "question": "Which country gets the baton first in the video?\n(A) Botswana\n(B) South Africa\n(C) Poland\n(D) United Kingdom\n(E) Germany\n(F) United States", "candidates": ["Botswana", "South Africa", "Poland", "United Kingdom", "Germany", "United States"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_10", "indices": ["425", "1425", "1950", "2400", "3325", "4175", "5225", "5400", "6000", "7075", "7825", "8800", "9675", "10050", "11150", "11925", "12325", "13150", "13675", "14575", "15025", "15825", "16575", "17775", "18325", "18750", "19950", "20475", "21650", "21750", "22625", "23850"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Poland"]], "filtered_resps": ["(C) Poland"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "713e2db476be3447aa9c259f8b6ba51cc822dadc7d24c37b165c07b6c9b6ef43", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which country gets the baton first in the video?\n(A) Botswana\n(B) South Africa\n(C) Poland\n(D) United Kingdom\n(E) Germany\n(F) United States", "task_type": "sportsQA", "pred_answer": "C", "answer": "F"}, "input": "Which country gets the baton first in the video?\n(A) Botswana\n(B) South Africa\n(C) Poland\n(D) United Kingdom\n(E) Germany\n(F) United States\nOnly give the best option.\nBest option: ("}
{"doc_id": 53, "doc": {"video_name": "test_sports_31.mp4", "duration": 696.0, "question": "Which country's team gets the first ball at the start of the video?\n(A) South Korea\n(B) United States\n(C) Sweden\n(D) China\n(E) Japan\n(F) Russia", "candidates": ["South Korea", "United States", "Sweden", "China", "Japan", "Russia"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_14", "indices": ["700", "1800", "2650", "4200", "4600", "6350", "6700", "8250", "9050", "10350", "11300", "13000", "13750", "14450", "15450", "16800", "17800", "19300", "19900", "20900", "22700", "23000", "24650", "25750", "26250", "27550", "28900", "30400", "30800", "31900", "33550", "33900"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) China"]], "filtered_resps": ["(D) China"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65ac4b608a4452f0d73e3de17f52992289343ca873e3640e88153940516dbdfa", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which country's team gets the first ball at the start of the video?\n(A) South Korea\n(B) United States\n(C) Sweden\n(D) China\n(E) Japan\n(F) Russia", "task_type": "sportsQA", "pred_answer": "D", "answer": "D"}, "input": "Which country's team gets the first ball at the start of the video?\n(A) South Korea\n(B) United States\n(C) Sweden\n(D) China\n(E) Japan\n(F) Russia\nOnly give the best option.\nBest option: ("}
{"doc_id": 57, "doc": {"video_name": "test_sports_17.mp4", "duration": 551.57, "question": "What was the state of the players from both teams in the final moments of the game?\n(A) The white team gave up on offense, and the blue team continued to compete\n(B) The white team continued to attack, and the blue team gave up on defense\n(C) The white team gave up on defense, and the blue team gave up on offense\n(D) The white team gave up on defense, and the blue team continued to attack\n(E) The white team continued to attack, and the blue team continued to defend\n(F) The white team gave up on offense, and the blue team gave up on defense", "candidates": ["The white team gave up on offense, and the blue team continued to compete", "The white team continued to attack, and the blue team gave up on defense", "The white team gave up on defense, and the blue team gave up on offense", "The white team gave up on defense, and the blue team continued to attack", "The white team continued to attack, and the blue team continued to defend", "The white team gave up on offense, and the blue team gave up on defense"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_18", "indices": ["464", "638", "1508", "1827", "2378", "2929", "3277", "4002", "4437", "4872", "5452", "5887", "6380", "6960", "7482", "7946", "8729", "8758", "9425", "10150", "10759", "11078", "11832", "12296", "12528", "13137", "13514", "14297", "14819", "15138", "15660", "16269"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) The white team continued to attack, and the blue team gave up on"]], "filtered_resps": ["(E) The white team continued to attack, and the blue team gave up on"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d7b88ac3c99d2a5c2cdb8caf7e2b76233f67e0c3d833011301acf7061b98b39", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What was the state of the players from both teams in the final moments of the game?\n(A) The white team gave up on offense, and the blue team continued to compete\n(B) The white team continued to attack, and the blue team gave up on defense\n(C) The white team gave up on defense, and the blue team gave up on offense\n(D) The white team gave up on defense, and the blue team continued to attack\n(E) The white team continued to attack, and the blue team continued to defend\n(F) The white team gave up on offense, and the blue team gave up on defense", "task_type": "sportsQA", "pred_answer": "E", "answer": "F"}, "input": "What was the state of the players from both teams in the final moments of the game?\n(A) The white team gave up on offense, and the blue team continued to compete\n(B) The white team continued to attack, and the blue team gave up on defense\n(C) The white team gave up on defense, and the blue team gave up on offense\n(D) The white team gave up on defense, and the blue team continued to attack\n(E) The white team continued to attack, and the blue team continued to defend\n(F) The white team gave up on offense, and the blue team gave up on defense\nOnly give the best option.\nBest option: ("}
{"doc_id": 61, "doc": {"video_name": "test_sports_20.mp4", "duration": 848.16, "question": "How does the team in white score the first points?\n(A) A player scores by dunking after a breakthrough\n(B) A player scores from a mid-range jump shot\n(C) A player scores from a free throw\n(D) A player scores by making a layup after a breakthrough\n(E) A player scores with a stop-and-jump shot\n(F) A player scores from a three-point long shot", "candidates": ["A player scores by dunking after a breakthrough", "A player scores from a mid-range jump shot", "A player scores from a free throw", "A player scores by making a layup after a breakthrough", "A player scores with a stop-and-jump shot", "A player scores from a three-point long shot"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_22", "indices": ["348", "1131", "1827", "3074", "3451", "4611", "4872", "6235", "6351", "7627", "8468", "9077", "9657", "10759", "11687", "12325", "13369", "14007", "14993", "15225", "16066", "17429", "17951", "18299", "19111", "20416", "21141", "22156", "22736", "23722", "23838", "24708"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) A player scores by dunking after a breakthrough"]], "filtered_resps": ["(A) A player scores by dunking after a breakthrough"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4e218e5b711910faca324f09096b41e7ce76f33c2c4f66b7fe50cc325df4fd9a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How does the team in white score the first points?\n(A) A player scores by dunking after a breakthrough\n(B) A player scores from a mid-range jump shot\n(C) A player scores from a free throw\n(D) A player scores by making a layup after a breakthrough\n(E) A player scores with a stop-and-jump shot\n(F) A player scores from a three-point long shot", "task_type": "sportsQA", "pred_answer": "A", "answer": "D"}, "input": "How does the team in white score the first points?\n(A) A player scores by dunking after a breakthrough\n(B) A player scores from a mid-range jump shot\n(C) A player scores from a free throw\n(D) A player scores by making a layup after a breakthrough\n(E) A player scores with a stop-and-jump shot\n(F) A player scores from a three-point long shot\nOnly give the best option.\nBest option: ("}
{"doc_id": 65, "doc": {"video_name": "test_sports_1.mp4", "duration": 645.0, "question": "What happened when player number 11 in the white and blue jersey advanced with the ball?\n(A) He was caught in an offside trap\n(B) He passed the ball\n(C) He was sandwiched in defense\n(D) He fell due to the slippery field\n(E) He kicked the ball out of bounds\n(F) He shot at the goal", "candidates": ["He was caught in an offside trap", "He passed the ball", "He was sandwiched in defense", "He fell due to the slippery field", "He kicked the ball out of bounds", "He shot at the goal"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_26", "indices": ["420", "900", "1650", "2190", "2670", "3270", "3900", "4770", "4830", "5940", "6210", "7140", "7800", "8280", "8580", "9300", "9750", "10320", "11190", "11610", "12090", "12990", "13290", "13920", "14700", "15450", "15720", "16410", "17160", "17550", "18150", "18870"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) He was sandwiched in defense"]], "filtered_resps": ["(C) He was sandwiched in defense"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fc09d0dc49384f1a04ddee377ab7893972ff48b9e4bf96551774e84038d6ce51", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What happened when player number 11 in the white and blue jersey advanced with the ball?\n(A) He was caught in an offside trap\n(B) He passed the ball\n(C) He was sandwiched in defense\n(D) He fell due to the slippery field\n(E) He kicked the ball out of bounds\n(F) He shot at the goal", "task_type": "sportsQA", "pred_answer": "C", "answer": "C"}, "input": "What happened when player number 11 in the white and blue jersey advanced with the ball?\n(A) He was caught in an offside trap\n(B) He passed the ball\n(C) He was sandwiched in defense\n(D) He fell due to the slippery field\n(E) He kicked the ball out of bounds\n(F) He shot at the goal\nOnly give the best option.\nBest option: ("}
{"doc_id": 69, "doc": {"video_name": "test_sports_5.mp4", "duration": 539.53, "question": "How did the team in white jerseys score their goal?\n(A) Penalty kick\n(B) Corner kick\n(C) Header\n(D) Indirect free kick\n(E) Drop ball\n(F) Direct free kick", "candidates": ["Penalty kick", "Corner kick", "Header", "Indirect free kick", "Drop ball", "Direct free kick"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_30", "indices": ["0", "840", "1470", "1500", "2400", "2820", "3030", "3600", "4080", "4560", "5040", "5550", "6120", "6870", "7410", "7650", "8250", "8640", "9330", "9690", "10560", "11040", "11430", "11850", "12360", "13050", "13110", "13800", "14160", "14940", "15210", "15720"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac4ee8b2405b89ed1847a5acf375b91785cc830b0c6682987165177712799ff9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How did the team in white jerseys score their goal?\n(A) Penalty kick\n(B) Corner kick\n(C) Header\n(D) Indirect free kick\n(E) Drop ball\n(F) Direct free kick", "task_type": "sportsQA", "pred_answer": "F", "answer": "A"}, "input": "How did the team in white jerseys score their goal?\n(A) Penalty kick\n(B) Corner kick\n(C) Header\n(D) Indirect free kick\n(E) Drop ball\n(F) Direct free kick\nOnly give the best option.\nBest option: ("}
{"doc_id": 73, "doc": {"video_name": "test_sports_14.mp4", "duration": 597.91, "question": "What happened to player number 5 from the white team in the final stages of the game?\n(A) He missed a free throw\n(B) He was knocked off by a defensive player\n(C) He committed a traveling violation\n(D) He made a shot\n(E) He was fouled by a hand-check\n(F) He made a mistake and dribbled the ball out of bounds", "candidates": ["He missed a free throw", "He was knocked off by a defensive player", "He committed a traveling violation", "He made a shot", "He was fouled by a hand-check", "He made a mistake and dribbled the ball out of bounds"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_34", "indices": ["826", "1357", "2832", "4366", "5192", "6136", "7434", "8555", "9086", "10679", "11505", "12390", "14455", "14632", "15871", "17110", "17877", "19470", "20414", "22007", "23128", "24072", "25547", "26314", "27199", "28615", "29854", "30680", "31742", "32804", "34102", "35400"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) He was fouled by a hand-check"]], "filtered_resps": ["(E) He was fouled by a hand-check"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "32e240be948bac913f7bc2a37a1be79418c1d7d635bf8b518e25720e5f2c8cbd", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What happened to player number 5 from the white team in the final stages of the game?\n(A) He missed a free throw\n(B) He was knocked off by a defensive player\n(C) He committed a traveling violation\n(D) He made a shot\n(E) He was fouled by a hand-check\n(F) He made a mistake and dribbled the ball out of bounds", "task_type": "sportsQA", "pred_answer": "E", "answer": "B"}, "input": "What happened to player number 5 from the white team in the final stages of the game?\n(A) He missed a free throw\n(B) He was knocked off by a defensive player\n(C) He committed a traveling violation\n(D) He made a shot\n(E) He was fouled by a hand-check\n(F) He made a mistake and dribbled the ball out of bounds\nOnly give the best option.\nBest option: ("}
{"doc_id": 77, "doc": {"video_name": "test_order_44.mp4", "duration": 520.04, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> baking cookies --> paragliding --> cleaning toilet\n(B) paragliding --> shredding paper --> cleaning toilet --> baking cookies\n(C) cleaning toilet --> shredding paper --> paragliding --> baking cookies\n(D) paragliding --> baking cookies --> shredding paper --> cleaning toilet\n(E) shredding paper --> paragliding --> cleaning toilet --> baking cookies\n(F) cleaning toilet --> paragliding --> baking cookies --> shredding paper", "candidates": ["shredding paper --> baking cookies --> paragliding --> cleaning toilet", "paragliding --> shredding paper --> cleaning toilet --> baking cookies", "cleaning toilet --> shredding paper --> paragliding --> baking cookies", "paragliding --> baking cookies --> shredding paper --> cleaning toilet", "shredding paper --> paragliding --> cleaning toilet --> baking cookies", "cleaning toilet --> paragliding --> baking cookies --> shredding paper"], "answer": "A", "task_type": "order", "question_id": "AO_2", "indices": ["290", "696", "986", "1624", "2349", "2726", "3248", "3712", "3944", "4727", "5075", "5336", "5887", "6467", "7192", "7540", "7946", "8294", "9077", "9599", "10150", "10614", "10701", "11455", "11919", "12383", "12934", "13543", "13688", "14471", "14819", "15109"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) shredding paper --> paragliding --> cleaning toilet --> baking cookies"]], "filtered_resps": ["(E) shredding paper --> paragliding --> cleaning toilet --> baking cookies"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a613d3610e60c6bb5292162484ae617d67149eacf88a565880f058bcb3bbe73a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> baking cookies --> paragliding --> cleaning toilet\n(B) paragliding --> shredding paper --> cleaning toilet --> baking cookies\n(C) cleaning toilet --> shredding paper --> paragliding --> baking cookies\n(D) paragliding --> baking cookies --> shredding paper --> cleaning toilet\n(E) shredding paper --> paragliding --> cleaning toilet --> baking cookies\n(F) cleaning toilet --> paragliding --> baking cookies --> shredding paper", "task_type": "order", "pred_answer": "E", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) shredding paper --> baking cookies --> paragliding --> cleaning toilet\n(B) paragliding --> shredding paper --> cleaning toilet --> baking cookies\n(C) cleaning toilet --> shredding paper --> paragliding --> baking cookies\n(D) paragliding --> baking cookies --> shredding paper --> cleaning toilet\n(E) shredding paper --> paragliding --> cleaning toilet --> baking cookies\n(F) cleaning toilet --> paragliding --> baking cookies --> shredding paper\nOnly give the best option.\nBest option: ("}
{"doc_id": 81, "doc": {"video_name": "test_order_25.mp4", "duration": 640.72, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> stomping grapes --> jetskiing --> tossing coin\n(B) jetskiing --> pole vault --> stomping grapes --> tossing coin\n(C) stomping grapes --> pole vault --> jetskiing --> tossing coin\n(D) stomping grapes --> pole vault --> tossing coin --> jetskiing\n(E) stomping grapes --> tossing coin --> jetskiing --> pole vault\n(F) pole vault --> jetskiing --> stomping grapes --> tossing coin", "candidates": ["pole vault --> stomping grapes --> jetskiing --> tossing coin", "jetskiing --> pole vault --> stomping grapes --> tossing coin", "stomping grapes --> pole vault --> jetskiing --> tossing coin", "stomping grapes --> pole vault --> tossing coin --> jetskiing", "stomping grapes --> tossing coin --> jetskiing --> pole vault", "pole vault --> jetskiing --> stomping grapes --> tossing coin"], "answer": "D", "task_type": "order", "question_id": "AO_6", "indices": ["116", "580", "1189", "1827", "2639", "3074", "3944", "4495", "5162", "5945", "5974", "6583", "7395", "7888", "8439", "9309", "9628", "10440", "10875", "11658", "12383", "12731", "13340", "13862", "14761", "15196", "15805", "16675", "16994", "17371", "18009", "19169"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba1532c0f7e7025327cd9ef8cbc9ebba751d03b113f5da8bf10c768a44372fb2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> stomping grapes --> jetskiing --> tossing coin\n(B) jetskiing --> pole vault --> stomping grapes --> tossing coin\n(C) stomping grapes --> pole vault --> jetskiing --> tossing coin\n(D) stomping grapes --> pole vault --> tossing coin --> jetskiing\n(E) stomping grapes --> tossing coin --> jetskiing --> pole vault\n(F) pole vault --> jetskiing --> stomping grapes --> tossing coin", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) pole vault --> stomping grapes --> jetskiing --> tossing coin\n(B) jetskiing --> pole vault --> stomping grapes --> tossing coin\n(C) stomping grapes --> pole vault --> jetskiing --> tossing coin\n(D) stomping grapes --> pole vault --> tossing coin --> jetskiing\n(E) stomping grapes --> tossing coin --> jetskiing --> pole vault\n(F) pole vault --> jetskiing --> stomping grapes --> tossing coin\nOnly give the best option.\nBest option: ("}
{"doc_id": 85, "doc": {"video_name": "test_order_9.mp4", "duration": 490.04999999999995, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> shredding paper --> carving pumpkin --> tossing coin\n(B) carving pumpkin --> playing harp --> tossing coin --> shredding paper\n(C) playing harp --> carving pumpkin --> shredding paper --> tossing coin\n(D) shredding paper --> playing harp --> carving pumpkin --> tossing coin\n(E) tossing coin --> playing harp --> carving pumpkin --> shredding paper\n(F) tossing coin --> shredding paper --> carving pumpkin --> playing harp", "candidates": ["playing harp --> shredding paper --> carving pumpkin --> tossing coin", "carving pumpkin --> playing harp --> tossing coin --> shredding paper", "playing harp --> carving pumpkin --> shredding paper --> tossing coin", "shredding paper --> playing harp --> carving pumpkin --> tossing coin", "tossing coin --> playing harp --> carving pumpkin --> shredding paper", "tossing coin --> shredding paper --> carving pumpkin --> playing harp"], "answer": "E", "task_type": "order", "question_id": "AO_10", "indices": ["87", "580", "1160", "1653", "2001", "2465", "3103", "3480", "4031", "4234", "4553", "5249", "5742", "6322", "6699", "7192", "7453", "7772", "8410", "8758", "9164", "9744", "10063", "10904", "11136", "11513", "12325", "12760", "13195", "13688", "14152", "14413"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of actions shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of actions shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ccd409f06dc0e352783322faf007f5951dc02b24cbe8d52fcfdb2f2138e5c70", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> shredding paper --> carving pumpkin --> tossing coin\n(B) carving pumpkin --> playing harp --> tossing coin --> shredding paper\n(C) playing harp --> carving pumpkin --> shredding paper --> tossing coin\n(D) shredding paper --> playing harp --> carving pumpkin --> tossing coin\n(E) tossing coin --> playing harp --> carving pumpkin --> shredding paper\n(F) tossing coin --> shredding paper --> carving pumpkin --> playing harp", "task_type": "order", "pred_answer": "None of the options provided match the sequence of actions shown in the video. The", "answer": "E"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> shredding paper --> carving pumpkin --> tossing coin\n(B) carving pumpkin --> playing harp --> tossing coin --> shredding paper\n(C) playing harp --> carving pumpkin --> shredding paper --> tossing coin\n(D) shredding paper --> playing harp --> carving pumpkin --> tossing coin\n(E) tossing coin --> playing harp --> carving pumpkin --> shredding paper\n(F) tossing coin --> shredding paper --> carving pumpkin --> playing harp\nOnly give the best option.\nBest option: ("}
{"doc_id": 89, "doc": {"video_name": "test_order_43.mp4", "duration": 512.6800000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> water sliding --> zumba --> riding mule\n(B) water sliding --> riding mule --> clean and jerk --> zumba\n(C) water sliding --> clean and jerk --> riding mule --> zumba\n(D) zumba --> riding mule --> clean and jerk --> water sliding\n(E) zumba --> clean and jerk --> riding mule --> water sliding\n(F) water sliding --> clean and jerk --> zumba --> riding mule", "candidates": ["clean and jerk --> water sliding --> zumba --> riding mule", "water sliding --> riding mule --> clean and jerk --> zumba", "water sliding --> clean and jerk --> riding mule --> zumba", "zumba --> riding mule --> clean and jerk --> water sliding", "zumba --> clean and jerk --> riding mule --> water sliding", "water sliding --> clean and jerk --> zumba --> riding mule"], "answer": "E", "task_type": "order", "question_id": "AO_14", "indices": ["150", "870", "1260", "1710", "2250", "2520", "3150", "3570", "4290", "4560", "5070", "5430", "5880", "6390", "6780", "7650", "7680", "8190", "8880", "9510", "10020", "10140", "10920", "11040", "11790", "12210", "12870", "13020", "13440", "14370", "14670", "14940"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5193ab1aecd2b710dd9842a297d4781572ee9e216dd2d7f4096ec35a5a292861", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> water sliding --> zumba --> riding mule\n(B) water sliding --> riding mule --> clean and jerk --> zumba\n(C) water sliding --> clean and jerk --> riding mule --> zumba\n(D) zumba --> riding mule --> clean and jerk --> water sliding\n(E) zumba --> clean and jerk --> riding mule --> water sliding\n(F) water sliding --> clean and jerk --> zumba --> riding mule", "task_type": "order", "pred_answer": "E", "answer": "E"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) clean and jerk --> water sliding --> zumba --> riding mule\n(B) water sliding --> riding mule --> clean and jerk --> zumba\n(C) water sliding --> clean and jerk --> riding mule --> zumba\n(D) zumba --> riding mule --> clean and jerk --> water sliding\n(E) zumba --> clean and jerk --> riding mule --> water sliding\n(F) water sliding --> clean and jerk --> zumba --> riding mule\nOnly give the best option.\nBest option: ("}
{"doc_id": 93, "doc": {"video_name": "test_order_24.mp4", "duration": 6071.1, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> playing trombone --> abseiling --> riding mule\n(B) playing trombone --> riding mule --> clean and jerk --> abseiling\n(C) abseiling --> riding mule --> clean and jerk --> playing trombone\n(D) riding mule --> playing trombone --> clean and jerk --> abseiling\n(E) playing trombone --> riding mule --> abseiling --> clean and jerk\n(F) abseiling --> playing trombone --> riding mule --> clean and jerk", "candidates": ["clean and jerk --> playing trombone --> abseiling --> riding mule", "playing trombone --> riding mule --> clean and jerk --> abseiling", "abseiling --> riding mule --> clean and jerk --> playing trombone", "riding mule --> playing trombone --> clean and jerk --> abseiling", "playing trombone --> riding mule --> abseiling --> clean and jerk", "abseiling --> playing trombone --> riding mule --> clean and jerk"], "answer": "B", "task_type": "order", "question_id": "AO_18", "indices": ["4890", "6270", "11460", "19530", "26700", "32520", "39660", "44850", "46860", "55320", "57990", "67170", "68700", "77220", "85020", "89100", "95280", "99870", "102780", "112140", "115770", "123480", "129210", "135000", "138510", "142350", "152280", "154650", "161910", "165030", "175410", "179610"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events described in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events described in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38cf38e01b8416874be1615093088f932c5909db2080f8e0dc7f52fb1831c1f8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> playing trombone --> abseiling --> riding mule\n(B) playing trombone --> riding mule --> clean and jerk --> abseiling\n(C) abseiling --> riding mule --> clean and jerk --> playing trombone\n(D) riding mule --> playing trombone --> clean and jerk --> abseiling\n(E) playing trombone --> riding mule --> abseiling --> clean and jerk\n(F) abseiling --> playing trombone --> riding mule --> clean and jerk", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events described in the video. The", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) clean and jerk --> playing trombone --> abseiling --> riding mule\n(B) playing trombone --> riding mule --> clean and jerk --> abseiling\n(C) abseiling --> riding mule --> clean and jerk --> playing trombone\n(D) riding mule --> playing trombone --> clean and jerk --> abseiling\n(E) playing trombone --> riding mule --> abseiling --> clean and jerk\n(F) abseiling --> playing trombone --> riding mule --> clean and jerk\nOnly give the best option.\nBest option: ("}
{"doc_id": 97, "doc": {"video_name": "test_order_27.mp4", "duration": 418.0499999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cooking sausages --> water sliding --> stomping grapes --> baking cookies\n(B) baking cookies --> water sliding --> cooking sausages --> stomping grapes\n(C) baking cookies --> cooking sausages --> stomping grapes --> water sliding\n(D) baking cookies --> stomping grapes --> cooking sausages --> water sliding\n(E) water sliding --> stomping grapes --> baking cookies --> cooking sausages\n(F) water sliding --> cooking sausages --> baking cookies --> stomping grapes", "candidates": ["cooking sausages --> water sliding --> stomping grapes --> baking cookies", "baking cookies --> water sliding --> cooking sausages --> stomping grapes", "baking cookies --> cooking sausages --> stomping grapes --> water sliding", "baking cookies --> stomping grapes --> cooking sausages --> water sliding", "water sliding --> stomping grapes --> baking cookies --> cooking sausages", "water sliding --> cooking sausages --> baking cookies --> stomping grapes"], "answer": "A", "task_type": "order", "question_id": "AO_22", "indices": ["210", "480", "1140", "1230", "1860", "2190", "2610", "2730", "3450", "3810", "4170", "4440", "4740", "5070", "5490", "5940", "6570", "6750", "7410", "7500", "7950", "8340", "8760", "9150", "9390", "10140", "10170", "10770", "11010", "11430", "11820", "12360"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "907de2e51746d716401be3eeb1d96426e75f9d860d0c22a53864b1f1b6f0a820", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cooking sausages --> water sliding --> stomping grapes --> baking cookies\n(B) baking cookies --> water sliding --> cooking sausages --> stomping grapes\n(C) baking cookies --> cooking sausages --> stomping grapes --> water sliding\n(D) baking cookies --> stomping grapes --> cooking sausages --> water sliding\n(E) water sliding --> stomping grapes --> baking cookies --> cooking sausages\n(F) water sliding --> cooking sausages --> baking cookies --> stomping grapes", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events shown in the video. The", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) cooking sausages --> water sliding --> stomping grapes --> baking cookies\n(B) baking cookies --> water sliding --> cooking sausages --> stomping grapes\n(C) baking cookies --> cooking sausages --> stomping grapes --> water sliding\n(D) baking cookies --> stomping grapes --> cooking sausages --> water sliding\n(E) water sliding --> stomping grapes --> baking cookies --> cooking sausages\n(F) water sliding --> cooking sausages --> baking cookies --> stomping grapes\nOnly give the best option.\nBest option: ("}
{"doc_id": 101, "doc": {"video_name": "test_order_28.mp4", "duration": 7233.53, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> water sliding --> cleaning toilet --> making jewelry\n(B) stomping grapes --> making jewelry --> cleaning toilet --> water sliding\n(C) water sliding --> making jewelry --> cleaning toilet --> stomping grapes\n(D) making jewelry --> stomping grapes --> water sliding --> cleaning toilet\n(E) making jewelry --> water sliding --> stomping grapes --> cleaning toilet\n(F) making jewelry --> water sliding --> cleaning toilet --> stomping grapes", "candidates": ["stomping grapes --> water sliding --> cleaning toilet --> making jewelry", "stomping grapes --> making jewelry --> cleaning toilet --> water sliding", "water sliding --> making jewelry --> cleaning toilet --> stomping grapes", "making jewelry --> stomping grapes --> water sliding --> cleaning toilet", "making jewelry --> water sliding --> stomping grapes --> cleaning toilet", "making jewelry --> water sliding --> cleaning toilet --> stomping grapes"], "answer": "D", "task_type": "order", "question_id": "AO_26", "indices": ["4620", "9690", "18120", "26820", "29700", "39510", "47250", "48390", "59010", "67440", "67830", "74700", "81930", "91320", "95550", "106170", "108600", "119760", "124170", "129420", "138000", "145260", "154050", "160440", "166290", "171540", "178080", "188790", "194460", "199710", "209700", "215370"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["The video does not depict any activities related to stomping grapes, water sliding,"]], "filtered_resps": ["The video does not depict any activities related to stomping grapes, water sliding,"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af8d66a1376a795559740cd75db82d2faf5e045d33c4c302f769afb5f0ea4be5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> water sliding --> cleaning toilet --> making jewelry\n(B) stomping grapes --> making jewelry --> cleaning toilet --> water sliding\n(C) water sliding --> making jewelry --> cleaning toilet --> stomping grapes\n(D) making jewelry --> stomping grapes --> water sliding --> cleaning toilet\n(E) making jewelry --> water sliding --> stomping grapes --> cleaning toilet\n(F) making jewelry --> water sliding --> cleaning toilet --> stomping grapes", "task_type": "order", "pred_answer": "The video does not depict any activities related to stomping grapes, water sliding,", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> water sliding --> cleaning toilet --> making jewelry\n(B) stomping grapes --> making jewelry --> cleaning toilet --> water sliding\n(C) water sliding --> making jewelry --> cleaning toilet --> stomping grapes\n(D) making jewelry --> stomping grapes --> water sliding --> cleaning toilet\n(E) making jewelry --> water sliding --> stomping grapes --> cleaning toilet\n(F) making jewelry --> water sliding --> cleaning toilet --> stomping grapes\nOnly give the best option.\nBest option: ("}
{"doc_id": 105, "doc": {"video_name": "test_order_63.mp4", "duration": 665.42, "question": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->4->1->3\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n(E) 2->1->3->4\n(F) 1->2->3->4", "candidates": ["2->4->1->3", "4->1->2->3", "4->3->2->1", "3->2->1->4", "2->1->3->4", "1->2->3->4"], "answer": "F", "task_type": "order", "question_id": "AO_30", "indices": ["8250", "8300", "8875", "8925", "8950", "8975", "9025", "9050", "9075", "9100", "9125", "9150", "9175", "9225", "9250", "9300", "9425", "9450", "9475", "9500", "9550", "10575", "10600", "10650", "10725", "10750", "10775", "10825", "10875", "10900", "11025", "11050"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 4->3->2->1"]], "filtered_resps": ["(C) 4->3->2->1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9c71aaf3e4bbd275cb090ad3c25f48bc838b8da96e4f0b35dd5c76d8ad49d11d", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->4->1->3\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n(E) 2->1->3->4\n(F) 1->2->3->4", "task_type": "order", "pred_answer": "C", "answer": "F"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Woman tapes her hands with white tape; (2)Woman starts boxing in the ring with a guy; (3)Woman does sit ups on a towel on the beach; (4)Pictures of woman in her bikini are shown.\n(A) 2->4->1->3\n(B) 4->1->2->3\n(C) 4->3->2->1\n(D) 3->2->1->4\n(E) 2->1->3->4\n(F) 1->2->3->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 109, "doc": {"video_name": "test_order_15.mp4", "duration": 486.78, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> stomping grapes --> shredding paper --> water sliding\n(B) water sliding --> shredding paper --> stomping grapes --> riding mule\n(C) water sliding --> stomping grapes --> shredding paper --> riding mule\n(D) stomping grapes --> water sliding --> riding mule --> shredding paper\n(E) riding mule --> water sliding --> stomping grapes --> shredding paper\n(F) stomping grapes --> riding mule --> water sliding --> shredding paper", "candidates": ["riding mule --> stomping grapes --> shredding paper --> water sliding", "water sliding --> shredding paper --> stomping grapes --> riding mule", "water sliding --> stomping grapes --> shredding paper --> riding mule", "stomping grapes --> water sliding --> riding mule --> shredding paper", "riding mule --> water sliding --> stomping grapes --> shredding paper", "stomping grapes --> riding mule --> water sliding --> shredding paper"], "answer": "D", "task_type": "order", "question_id": "AO_34", "indices": ["406", "841", "1044", "1595", "1972", "2262", "3103", "3219", "3625", "4495", "4524", "5423", "5452", "6061", "6757", "6960", "7656", "8120", "8584", "8729", "9135", "9570", "10150", "10904", "10933", "11716", "11890", "12528", "12934", "13456", "13920", "14326"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) water sliding --> stomping grapes --> shredding paper --> riding mule"]], "filtered_resps": ["(C) water sliding --> stomping grapes --> shredding paper --> riding mule"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2294e8b62a7c704f221e695a3473a81585c3461766de4b2f2e1f71c9486ac91f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> stomping grapes --> shredding paper --> water sliding\n(B) water sliding --> shredding paper --> stomping grapes --> riding mule\n(C) water sliding --> stomping grapes --> shredding paper --> riding mule\n(D) stomping grapes --> water sliding --> riding mule --> shredding paper\n(E) riding mule --> water sliding --> stomping grapes --> shredding paper\n(F) stomping grapes --> riding mule --> water sliding --> shredding paper", "task_type": "order", "pred_answer": "C", "answer": "D"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) riding mule --> stomping grapes --> shredding paper --> water sliding\n(B) water sliding --> shredding paper --> stomping grapes --> riding mule\n(C) water sliding --> stomping grapes --> shredding paper --> riding mule\n(D) stomping grapes --> water sliding --> riding mule --> shredding paper\n(E) riding mule --> water sliding --> stomping grapes --> shredding paper\n(F) stomping grapes --> riding mule --> water sliding --> shredding paper\nOnly give the best option.\nBest option: ("}
{"doc_id": 113, "doc": {"video_name": "test_order_35.mp4", "duration": 730.85, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> shredding paper --> playing harp --> playing trombone\n(B) playing harp --> tossing coin --> shredding paper --> playing trombone\n(C) shredding paper --> playing harp --> playing trombone --> tossing coin\n(D) playing trombone --> tossing coin --> playing harp --> shredding paper\n(E) playing harp --> shredding paper --> playing trombone --> tossing coin\n(F) playing trombone --> shredding paper --> tossing coin --> playing harp", "candidates": ["tossing coin --> shredding paper --> playing harp --> playing trombone", "playing harp --> tossing coin --> shredding paper --> playing trombone", "shredding paper --> playing harp --> playing trombone --> tossing coin", "playing trombone --> tossing coin --> playing harp --> shredding paper", "playing harp --> shredding paper --> playing trombone --> tossing coin", "playing trombone --> shredding paper --> tossing coin --> playing harp"], "answer": "E", "task_type": "order", "question_id": "AO_38", "indices": ["60", "960", "1950", "2640", "2970", "3570", "4170", "5100", "5700", "6780", "7110", "7950", "8700", "9150", "9630", "10260", "11460", "11940", "12510", "13320", "13980", "14940", "15600", "15870", "16650", "17130", "18210", "18630", "19530", "20460", "21090", "21420"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) playing harp --> tossing coin --> shredding paper --> playing trombone"]], "filtered_resps": ["(B) playing harp --> tossing coin --> shredding paper --> playing trombone"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2d0f62e13667e25fb9975a9d6029316b545cecc67c9b4ce25ab1e73f8e60f53", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> shredding paper --> playing harp --> playing trombone\n(B) playing harp --> tossing coin --> shredding paper --> playing trombone\n(C) shredding paper --> playing harp --> playing trombone --> tossing coin\n(D) playing trombone --> tossing coin --> playing harp --> shredding paper\n(E) playing harp --> shredding paper --> playing trombone --> tossing coin\n(F) playing trombone --> shredding paper --> tossing coin --> playing harp", "task_type": "order", "pred_answer": "B", "answer": "E"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> shredding paper --> playing harp --> playing trombone\n(B) playing harp --> tossing coin --> shredding paper --> playing trombone\n(C) shredding paper --> playing harp --> playing trombone --> tossing coin\n(D) playing trombone --> tossing coin --> playing harp --> shredding paper\n(E) playing harp --> shredding paper --> playing trombone --> tossing coin\n(F) playing trombone --> shredding paper --> tossing coin --> playing harp\nOnly give the best option.\nBest option: ("}
{"doc_id": 117, "doc": {"video_name": "test_order_29.mp4", "duration": 479.46999999999997, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> zumba --> javelin throw --> clean and jerk\n(B) clean and jerk --> zumba --> javelin throw --> baking cookies\n(C) zumba --> baking cookies --> javelin throw --> clean and jerk\n(D) javelin throw --> baking cookies --> zumba --> clean and jerk\n(E) javelin throw --> clean and jerk --> zumba --> baking cookies\n(F) clean and jerk --> baking cookies --> javelin throw --> zumba", "candidates": ["baking cookies --> zumba --> javelin throw --> clean and jerk", "clean and jerk --> zumba --> javelin throw --> baking cookies", "zumba --> baking cookies --> javelin throw --> clean and jerk", "javelin throw --> baking cookies --> zumba --> clean and jerk", "javelin throw --> clean and jerk --> zumba --> baking cookies", "clean and jerk --> baking cookies --> javelin throw --> zumba"], "answer": "B", "task_type": "order", "question_id": "AO_42", "indices": ["210", "570", "1230", "1590", "2130", "2640", "2700", "3390", "3780", "4080", "4560", "5160", "5610", "5910", "6570", "7050", "7560", "7950", "8460", "8730", "9330", "9750", "9960", "10380", "10860", "11370", "11820", "12480", "12870", "13170", "13770", "13950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events described in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events described in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c2e2e7bbe8765c108e016ec3bbcf554ac67498161525ac82b9a8fe4a6b4d3cc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> zumba --> javelin throw --> clean and jerk\n(B) clean and jerk --> zumba --> javelin throw --> baking cookies\n(C) zumba --> baking cookies --> javelin throw --> clean and jerk\n(D) javelin throw --> baking cookies --> zumba --> clean and jerk\n(E) javelin throw --> clean and jerk --> zumba --> baking cookies\n(F) clean and jerk --> baking cookies --> javelin throw --> zumba", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events described in the video. The", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) baking cookies --> zumba --> javelin throw --> clean and jerk\n(B) clean and jerk --> zumba --> javelin throw --> baking cookies\n(C) zumba --> baking cookies --> javelin throw --> clean and jerk\n(D) javelin throw --> baking cookies --> zumba --> clean and jerk\n(E) javelin throw --> clean and jerk --> zumba --> baking cookies\n(F) clean and jerk --> baking cookies --> javelin throw --> zumba\nOnly give the best option.\nBest option: ("}
{"doc_id": 121, "doc": {"video_name": "test_order_11.mp4", "duration": 490.03999999999996, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> tossing coin --> carving pumpkin --> making jewelry\n(B) tossing coin --> carving pumpkin --> javelin throw --> making jewelry\n(C) making jewelry --> carving pumpkin --> tossing coin --> javelin throw\n(D) carving pumpkin --> tossing coin --> making jewelry --> javelin throw\n(E) making jewelry --> tossing coin --> javelin throw --> carving pumpkin\n(F) javelin throw --> carving pumpkin --> making jewelry --> tossing coin", "candidates": ["javelin throw --> tossing coin --> carving pumpkin --> making jewelry", "tossing coin --> carving pumpkin --> javelin throw --> making jewelry", "making jewelry --> carving pumpkin --> tossing coin --> javelin throw", "carving pumpkin --> tossing coin --> making jewelry --> javelin throw", "making jewelry --> tossing coin --> javelin throw --> carving pumpkin", "javelin throw --> carving pumpkin --> making jewelry --> tossing coin"], "answer": "E", "task_type": "order", "question_id": "AO_46", "indices": ["180", "450", "1170", "1710", "2130", "2340", "3120", "3570", "4050", "4140", "4920", "5430", "5910", "6120", "6510", "6930", "7470", "8100", "8670", "9090", "9330", "9900", "10410", "10890", "11070", "11790", "11910", "12420", "13230", "13620", "14070", "14640"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) javelin throw --> carving pumpkin --> making jewelry --> tossing coin"]], "filtered_resps": ["(F) javelin throw --> carving pumpkin --> making jewelry --> tossing coin"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5f48d4b9da8f01d251e21cb8b641683031d510a2d4ea12ad311d5fc7cdd3d225", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> tossing coin --> carving pumpkin --> making jewelry\n(B) tossing coin --> carving pumpkin --> javelin throw --> making jewelry\n(C) making jewelry --> carving pumpkin --> tossing coin --> javelin throw\n(D) carving pumpkin --> tossing coin --> making jewelry --> javelin throw\n(E) making jewelry --> tossing coin --> javelin throw --> carving pumpkin\n(F) javelin throw --> carving pumpkin --> making jewelry --> tossing coin", "task_type": "order", "pred_answer": "F", "answer": "E"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> tossing coin --> carving pumpkin --> making jewelry\n(B) tossing coin --> carving pumpkin --> javelin throw --> making jewelry\n(C) making jewelry --> carving pumpkin --> tossing coin --> javelin throw\n(D) carving pumpkin --> tossing coin --> making jewelry --> javelin throw\n(E) making jewelry --> tossing coin --> javelin throw --> carving pumpkin\n(F) javelin throw --> carving pumpkin --> making jewelry --> tossing coin\nOnly give the best option.\nBest option: ("}
{"doc_id": 125, "doc": {"video_name": "test_order_52.mp4", "duration": 6297.26, "question": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n(E) 4->3->2->1\n(F) 3->1->2->4", "candidates": ["2->1->4->3", "1->2->4->3", "4->2->1->3", "2->4->1->3", "4->3->2->1", "3->1->2->4"], "answer": "D", "task_type": "order", "question_id": "AO_50", "indices": ["116325", "116400", "116550", "117425", "117675", "117725", "117750", "117775", "117800", "117850", "117875", "117900", "117925", "117950", "117975", "118000", "118025", "118075", "118100", "118125", "118150", "118225", "118250", "118300", "118350", "118375", "118400", "118425", "118475", "118500", "118550", "118700"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "862356c0d168772ddfa02ffb5971acd3c465e9bfac84e5234ea816f153eca44a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n(E) 4->3->2->1\n(F) 3->1->2->4", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)Water is added to the cup and more limes are squeezed in by hand; (2)The small bowls of salt are arranged and limes are sliced in halves; (3)The cup is stirred with more water and a set of cups filled with the refreshment are seen; (4)The limes are juiced into a cup using a hand held press.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 4->2->1->3\n(D) 2->4->1->3\n(E) 4->3->2->1\n(F) 3->1->2->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 129, "doc": {"video_name": "test_order_10.mp4", "duration": 612.63, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> cooking sausages --> baking cookies --> tossing coin\n(B) cooking sausages --> baking cookies --> tossing coin --> jetskiing\n(C) baking cookies --> cooking sausages --> jetskiing --> tossing coin\n(D) jetskiing --> cooking sausages --> tossing coin --> baking cookies\n(E) cooking sausages --> tossing coin --> baking cookies --> jetskiing\n(F) jetskiing --> tossing coin --> baking cookies --> cooking sausages", "candidates": ["jetskiing --> cooking sausages --> baking cookies --> tossing coin", "cooking sausages --> baking cookies --> tossing coin --> jetskiing", "baking cookies --> cooking sausages --> jetskiing --> tossing coin", "jetskiing --> cooking sausages --> tossing coin --> baking cookies", "cooking sausages --> tossing coin --> baking cookies --> jetskiing", "jetskiing --> tossing coin --> baking cookies --> cooking sausages"], "answer": "C", "task_type": "order", "question_id": "AO_54", "indices": ["464", "1102", "1566", "1914", "2523", "3248", "3770", "4350", "4814", "5510", "5713", "6670", "7395", "7540", "8033", "8990", "9686", "9831", "10672", "11020", "11455", "12354", "12760", "13485", "14123", "14674", "15312", "15805", "16037", "17110", "17516", "17835"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) cooking sausages --> tossing coin --> baking cookies --> jetskiing"]], "filtered_resps": ["(E) cooking sausages --> tossing coin --> baking cookies --> jetskiing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a75f41d25454f2ea12e4dca024d565256ec38a8a74c48f0cdfb11d7a4db6e2fc", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> cooking sausages --> baking cookies --> tossing coin\n(B) cooking sausages --> baking cookies --> tossing coin --> jetskiing\n(C) baking cookies --> cooking sausages --> jetskiing --> tossing coin\n(D) jetskiing --> cooking sausages --> tossing coin --> baking cookies\n(E) cooking sausages --> tossing coin --> baking cookies --> jetskiing\n(F) jetskiing --> tossing coin --> baking cookies --> cooking sausages", "task_type": "order", "pred_answer": "E", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> cooking sausages --> baking cookies --> tossing coin\n(B) cooking sausages --> baking cookies --> tossing coin --> jetskiing\n(C) baking cookies --> cooking sausages --> jetskiing --> tossing coin\n(D) jetskiing --> cooking sausages --> tossing coin --> baking cookies\n(E) cooking sausages --> tossing coin --> baking cookies --> jetskiing\n(F) jetskiing --> tossing coin --> baking cookies --> cooking sausages\nOnly give the best option.\nBest option: ("}
{"doc_id": 133, "doc": {"video_name": "test_order_32.mp4", "duration": 718.68, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> riding mule --> carving pumpkin --> pole vault\n(B) pole vault --> jetskiing --> carving pumpkin --> riding mule\n(C) carving pumpkin --> pole vault --> jetskiing --> riding mule\n(D) carving pumpkin --> riding mule --> pole vault --> jetskiing\n(E) pole vault --> riding mule --> jetskiing --> carving pumpkin\n(F) jetskiing --> pole vault --> carving pumpkin --> riding mule", "candidates": ["jetskiing --> riding mule --> carving pumpkin --> pole vault", "pole vault --> jetskiing --> carving pumpkin --> riding mule", "carving pumpkin --> pole vault --> jetskiing --> riding mule", "carving pumpkin --> riding mule --> pole vault --> jetskiing", "pole vault --> riding mule --> jetskiing --> carving pumpkin", "jetskiing --> pole vault --> carving pumpkin --> riding mule"], "answer": "F", "task_type": "order", "question_id": "AO_58", "indices": ["0", "899", "1798", "2639", "3190", "3944", "4060", "5220", "5539", "6322", "6786", "7424", "8091", "8758", "9802", "10643", "11194", "11513", "12470", "12760", "13601", "14442", "15167", "15486", "16762", "16791", "17719", "18125", "19372", "19575", "20184", "21431"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "792234ef549928f317040da7402fcf7c281f8c3641ed6de4945adcecf74c2d48", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> riding mule --> carving pumpkin --> pole vault\n(B) pole vault --> jetskiing --> carving pumpkin --> riding mule\n(C) carving pumpkin --> pole vault --> jetskiing --> riding mule\n(D) carving pumpkin --> riding mule --> pole vault --> jetskiing\n(E) pole vault --> riding mule --> jetskiing --> carving pumpkin\n(F) jetskiing --> pole vault --> carving pumpkin --> riding mule", "task_type": "order", "pred_answer": "F", "answer": "F"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) jetskiing --> riding mule --> carving pumpkin --> pole vault\n(B) pole vault --> jetskiing --> carving pumpkin --> riding mule\n(C) carving pumpkin --> pole vault --> jetskiing --> riding mule\n(D) carving pumpkin --> riding mule --> pole vault --> jetskiing\n(E) pole vault --> riding mule --> jetskiing --> carving pumpkin\n(F) jetskiing --> pole vault --> carving pumpkin --> riding mule\nOnly give the best option.\nBest option: ("}
{"doc_id": 137, "doc": {"video_name": "test_order_13.mp4", "duration": 486.96, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> pole vault --> playing trombone --> playing harp\n(B) zumba --> playing trombone --> playing harp --> pole vault\n(C) zumba --> pole vault --> playing harp --> playing trombone\n(D) zumba --> playing trombone --> pole vault --> playing harp\n(E) playing harp --> pole vault --> zumba --> playing trombone\n(F) playing trombone --> playing harp --> pole vault --> zumba", "candidates": ["zumba --> pole vault --> playing trombone --> playing harp", "zumba --> playing trombone --> playing harp --> pole vault", "zumba --> pole vault --> playing harp --> playing trombone", "zumba --> playing trombone --> pole vault --> playing harp", "playing harp --> pole vault --> zumba --> playing trombone", "playing trombone --> playing harp --> pole vault --> zumba"], "answer": "A", "task_type": "order", "question_id": "AO_62", "indices": ["330", "870", "1200", "1410", "2010", "2610", "3120", "3150", "3750", "4470", "4710", "5430", "5790", "6180", "6420", "7260", "7290", "7800", "8220", "8760", "9090", "9930", "10350", "10770", "10920", "11700", "12090", "12540", "13020", "13440", "13800", "14190"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "22e03335a874827e49d197ea51593dd95830834fbd639365ba230076cd12c5e9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> pole vault --> playing trombone --> playing harp\n(B) zumba --> playing trombone --> playing harp --> pole vault\n(C) zumba --> pole vault --> playing harp --> playing trombone\n(D) zumba --> playing trombone --> pole vault --> playing harp\n(E) playing harp --> pole vault --> zumba --> playing trombone\n(F) playing trombone --> playing harp --> pole vault --> zumba", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) zumba --> pole vault --> playing trombone --> playing harp\n(B) zumba --> playing trombone --> playing harp --> pole vault\n(C) zumba --> pole vault --> playing harp --> playing trombone\n(D) zumba --> playing trombone --> pole vault --> playing harp\n(E) playing harp --> pole vault --> zumba --> playing trombone\n(F) playing trombone --> playing harp --> pole vault --> zumba\nOnly give the best option.\nBest option: ("}
{"doc_id": 141, "doc": {"video_name": "test_order_54.mp4", "duration": 682.25, "question": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 1->3->2->4\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 3->2->1->4", "candidates": ["2->1->4->3", "1->2->4->3", "1->3->2->4", "2->4->1->3", "4->2->1->3", "3->2->1->4"], "answer": "D", "task_type": "order", "question_id": "AO_66", "indices": ["145", "1102", "1421", "1972", "2581", "3480", "4031", "5017", "5307", "5800", "6815", "7482", "8062", "8816", "9135", "9889", "10672", "10846", "11600", "12122", "13224", "13601", "14181", "15109", "15776", "16414", "16791", "17516", "18096", "18850", "19546", "20387"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5a076d3511aac798bf2ad23ebf05da7e9eb239d8cea36b197a5b9c0a42e7a18", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 1->3->2->4\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 3->2->1->4", "task_type": "order", "pred_answer": "A", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The woman starts working on her nails using bottles from a box next to her; (2)The words \"Love Food & Money with Angie Greenup\" appears on screen; (3)Her twitter handle and subscribe screen are shown while she holds her dogs; (4)The woman speaks to the camera from her living room while her dogs play fight behind her.\n(A) 2->1->4->3\n(B) 1->2->4->3\n(C) 1->3->2->4\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 3->2->1->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 145, "doc": {"video_name": "test_needle_17.mp4", "duration": 745.84, "question": "In the video, what did the woman take out from the oven after opening it?\n(A) Chicken leg\n(B) Pizza\n(C) Bread\n(D) Chicken wings\n(E) Sweet potato\n(F) Lamb chops", "candidates": ["Chicken leg", "Pizza", "Bread", "Chicken wings", "Sweet potato", "Lamb chops"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_0", "indices": ["550", "775", "1400", "2050", "2400", "3400", "3575", "4625", "4850", "5400", "6250", "6425", "7075", "7900", "8225", "8975", "9550", "10350", "10450", "11400", "12175", "12225", "13100", "13475", "14400", "14975", "15225", "15700", "16425", "17425", "17450", "18375"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48b2572d35a71f8fa123bc282f6fb934af27e8d075cd25705f160e1c18070de3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what did the woman take out from the oven after opening it?\n(A) Chicken leg\n(B) Pizza\n(C) Bread\n(D) Chicken wings\n(E) Sweet potato\n(F) Lamb chops", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "In the video, what did the woman take out from the oven after opening it?\n(A) Chicken leg\n(B) Pizza\n(C) Bread\n(D) Chicken wings\n(E) Sweet potato\n(F) Lamb chops\nOnly give the best option.\nBest option: ("}
{"doc_id": 149, "doc": {"video_name": "test_needle_21.mp4", "duration": 8013.469999999999, "question": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Drum set\n(B) Piano\n(C) Violin\n(D) Flute\n(E) Guzheng\n(F) Guitar", "candidates": ["Drum set", "Piano", "Violin", "Flute", "Guzheng", "Guitar"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_4", "indices": ["193800", "193875", "193925", "193950", "193975", "194025", "194050", "194100", "194125", "194150", "194175", "194300", "194375", "194425", "194475", "194500", "194525", "194625", "194675", "194750", "194800", "194900", "194925", "194950", "194975", "195000", "195100", "195125", "195150", "195175", "195200", "195225"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6f22801a7b9bf718292a025c524818161183ae09ed97d24380b9c0094f913941", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Drum set\n(B) Piano\n(C) Violin\n(D) Flute\n(E) Guzheng\n(F) Guitar", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Drum set\n(B) Piano\n(C) Violin\n(D) Flute\n(E) Guzheng\n(F) Guitar\nOnly give the best option.\nBest option: ("}
{"doc_id": 153, "doc": {"video_name": "test_needle_53.mp4", "duration": 8205.76, "question": "In the collision event of objects in the video, what shape is the stationary rubber object when the purple cylinder enters the scene?\n(A) cube\n(B) cylinder\n(C) rectangular prism\n(D) pyramid\n(E) sphere\n(F) cone", "candidates": ["cube", "cylinder", "rectangular prism", "pyramid", "sphere", "cone"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_8", "indices": ["4375", "10875", "18225", "22000", "26775", "36450", "38875", "51025", "52050", "60450", "69300", "76150", "77750", "88450", "96000", "101350", "105050", "115075", "115600", "123325", "128200", "136150", "147050", "148450", "158775", "161000", "169600", "177775", "180950", "191050", "196125", "199700"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9047c08c686b946e8498d15a0a73f2c90adcc89addb3a5b935b5e4e3d9225fe6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what shape is the stationary rubber object when the purple cylinder enters the scene?\n(A) cube\n(B) cylinder\n(C) rectangular prism\n(D) pyramid\n(E) sphere\n(F) cone", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the collision event of objects in the video, what shape is the stationary rubber object when the purple cylinder enters the scene?\n(A) cube\n(B) cylinder\n(C) rectangular prism\n(D) pyramid\n(E) sphere\n(F) cone\nOnly give the best option.\nBest option: ("}
{"doc_id": 157, "doc": {"video_name": "test_needle_34.mp4", "duration": 492.78, "question": "What is the pink toy the child is playing with in the video?\n(A) Cat\n(B) Rabbit\n(C) Birthday cake\n(D) Dinosaur\n(E) Dog\n(F) Car", "candidates": ["Cat", "Rabbit", "Birthday cake", "Dinosaur", "Dog", "Car"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_12", "indices": ["0", "870", "1160", "1798", "1885", "2349", "3161", "3538", "3915", "4524", "4640", "5162", "5684", "5974", "6873", "6902", "7366", "8033", "8613", "8787", "9251", "10063", "10469", "10672", "11107", "11832", "12354", "12615", "13195", "13775", "14210", "14297"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5e51aac35e4abdf27b450a79a32954c3c5a34058063a3bf8e6898a7aae5e8c8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the pink toy the child is playing with in the video?\n(A) Cat\n(B) Rabbit\n(C) Birthday cake\n(D) Dinosaur\n(E) Dog\n(F) Car", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "What is the pink toy the child is playing with in the video?\n(A) Cat\n(B) Rabbit\n(C) Birthday cake\n(D) Dinosaur\n(E) Dog\n(F) Car\nOnly give the best option.\nBest option: ("}
{"doc_id": 161, "doc": {"video_name": "test_needle_29.mp4", "duration": 472.28, "question": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Wooden stick\n(B) Bucket\n(C) Soldier sculpture\n(D) Lightning rod\n(E) Stone\n(F) National flag", "candidates": ["Wooden stick", "Bucket", "Soldier sculpture", "Lightning rod", "Stone", "National flag"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_16", "indices": ["50", "400", "925", "1175", "1600", "2050", "2250", "2650", "2975", "3300", "3975", "4300", "4475", "4975", "5325", "5725", "5975", "6475", "6625", "7350", "7525", "7725", "8250", "8700", "8975", "9475", "9650", "10000", "10575", "10750", "11150", "11625"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Soldier sculpture"]], "filtered_resps": ["(C) Soldier sculpture"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3861d0c936503e0da5d24f9422fb2b8f4672e46cde6e282df09ff9a172f57ef2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Wooden stick\n(B) Bucket\n(C) Soldier sculpture\n(D) Lightning rod\n(E) Stone\n(F) National flag", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what is on the tip of the tower next to the building with a red roof?\n(A) Wooden stick\n(B) Bucket\n(C) Soldier sculpture\n(D) Lightning rod\n(E) Stone\n(F) National flag\nOnly give the best option.\nBest option: ("}
{"doc_id": 165, "doc": {"video_name": "test_needle_56.mp4", "duration": 6617.900000000001, "question": "In the collision event of objects in the video, which of the following is not responsible for the collision between the blue cylinder and the blue sphere?\n(A) the trajectory of the blue sphere\n(B) the angle of impact between the objects\n(C) the speed of the blue cylinder\n(D) the surface texture of the blue sphere\n(E) the mass of the blue cylinder\n(F) the presence of the cyan metal object", "candidates": ["the trajectory of the blue sphere", "the angle of impact between the objects", "the speed of the blue cylinder", "the surface texture of the blue sphere", "the mass of the blue cylinder", "the presence of the cyan metal object"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_20", "indices": ["475", "5350", "14500", "15775", "23050", "29950", "31675", "37600", "44875", "51525", "56100", "58550", "63725", "67800", "75600", "80150", "87175", "89575", "96875", "103000", "104150", "111875", "116525", "123175", "128175", "131775", "138700", "140000", "149225", "151325", "157850", "162275"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) the presence of the cyan metal object"]], "filtered_resps": ["(F) the presence of the cyan metal object"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "26df11304152c16fbd74ce113a5a2975945c9c55ffd36825a96ada327eb873f6", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, which of the following is not responsible for the collision between the blue cylinder and the blue sphere?\n(A) the trajectory of the blue sphere\n(B) the angle of impact between the objects\n(C) the speed of the blue cylinder\n(D) the surface texture of the blue sphere\n(E) the mass of the blue cylinder\n(F) the presence of the cyan metal object", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the collision event of objects in the video, which of the following is not responsible for the collision between the blue cylinder and the blue sphere?\n(A) the trajectory of the blue sphere\n(B) the angle of impact between the objects\n(C) the speed of the blue cylinder\n(D) the surface texture of the blue sphere\n(E) the mass of the blue cylinder\n(F) the presence of the cyan metal object\nOnly give the best option.\nBest option: ("}
{"doc_id": 169, "doc": {"video_name": "test_needle_15.mp4", "duration": 460.64, "question": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) Blue\n(B) Gray\n(C) Pink\n(D) Green\n(E) Purple\n(F) White", "candidates": ["Blue", "Gray", "Pink", "Green", "Purple", "White"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_24", "indices": ["236", "1593", "2478", "2596", "3717", "4897", "5192", "6077", "7493", "8142", "8791", "10148", "11033", "11269", "12744", "13393", "14514", "14632", "16107", "16284", "17936", "18762", "19116", "19824", "21417", "21476", "22774", "23246", "24839", "25134", "26550", "27199"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "420d1da4f4b1b575aad0c7ba8471c3c4fb46db5ee04511d6ee8ea10e4e56807c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) Blue\n(B) Gray\n(C) Pink\n(D) Green\n(E) Purple\n(F) White", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) Blue\n(B) Gray\n(C) Pink\n(D) Green\n(E) Purple\n(F) White\nOnly give the best option.\nBest option: ("}
{"doc_id": 173, "doc": {"video_name": "test_needle_3.mp4", "duration": 681.67, "question": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Nervous\n(B) Happy\n(C) Excited\n(D) Neutral\n(E) Afraid\n(F) Sad", "candidates": ["Nervous", "Happy", "Excited", "Neutral", "Afraid", "Sad"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_28", "indices": ["319", "1102", "1740", "2291", "2610", "3335", "3886", "4930", "5423", "6264", "6583", "7047", "8091", "8903", "8961", "9802", "10788", "10991", "11803", "12180", "13108", "13717", "14210", "15109", "15747", "16443", "16762", "17603", "18444", "18589", "19691", "19778"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87c1ace6c389f688fad5a45791f1f1bd92fd620c4ded2560c9a51a4dff3dc2c0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Nervous\n(B) Happy\n(C) Excited\n(D) Neutral\n(E) Afraid\n(F) Sad", "task_type": "needleQA", "pred_answer": "D", "answer": "B"}, "input": "In the video, what kind of emotion does the blue-eyed male doctor reveal in the conversation?\n(A) Nervous\n(B) Happy\n(C) Excited\n(D) Neutral\n(E) Afraid\n(F) Sad\nOnly give the best option.\nBest option: ("}
{"doc_id": 177, "doc": {"video_name": "test_needle_31.mp4", "duration": 7590.060000000001, "question": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Sit-ups\n(B) Jumping jacks\n(C) Jumping rope\n(D) Squats\n(E) Push-ups\n(F) Pull-ups", "candidates": ["Sit-ups", "Jumping jacks", "Jumping rope", "Squats", "Push-ups", "Pull-ups"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_32", "indices": ["6554", "8961", "16878", "23664", "34800", "35612", "46719", "54056", "62669", "65569", "74211", "82447", "90944", "94366", "100920", "112636", "114985", "121858", "128296", "138243", "147494", "153874", "163038", "164285", "176668", "179220", "185658", "194648", "203957", "207814", "219646", "227389"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9cc899fbe7efef69ba95496eec15584ecf6c2132943bc8dd2d83ebe18021743e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Sit-ups\n(B) Jumping jacks\n(C) Jumping rope\n(D) Squats\n(E) Push-ups\n(F) Pull-ups", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "In the video, what is the man wearing a yellow short sleeve shirt and black shorts doing on high ground?\n(A) Sit-ups\n(B) Jumping jacks\n(C) Jumping rope\n(D) Squats\n(E) Push-ups\n(F) Pull-ups\nOnly give the best option.\nBest option: ("}
{"doc_id": 181, "doc": {"video_name": "test_needle_50.mp4", "duration": 455.25, "question": "In the collision event of objects in the video, what color is the object that exits the scene?\n(A) yellow\n(B) blue\n(C) orange\n(D) purple\n(E) red\n(F) green", "candidates": ["yellow", "blue", "orange", "purple", "red", "green"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_36", "indices": ["100", "675", "700", "1200", "1725", "1750", "2350", "2775", "3150", "3275", "3650", "4150", "4525", "4675", "5000", "5400", "5975", "6025", "6475", "6900", "7375", "7625", "7875", "8200", "8525", "9150", "9525", "9625", "9950", "10425", "10750", "11025"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "616e17f31b1e9c0400171228eb7f20c57013a55601767c2d261753cc4d8882b3", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what color is the object that exits the scene?\n(A) yellow\n(B) blue\n(C) orange\n(D) purple\n(E) red\n(F) green", "task_type": "needleQA", "pred_answer": "B", "answer": "F"}, "input": "In the collision event of objects in the video, what color is the object that exits the scene?\n(A) yellow\n(B) blue\n(C) orange\n(D) purple\n(E) red\n(F) green\nOnly give the best option.\nBest option: ("}
{"doc_id": 185, "doc": {"video_name": "test_needle_46.mp4", "duration": 251.14000000000001, "question": "In the collision event of objects in the video, how many red objects are moving?\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 3\n(F) 5", "candidates": ["6", "2", "1", "4", "3", "5"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_40", "indices": ["0", "325", "425", "725", "775", "1025", "1225", "1500", "1550", "1775", "2050", "2300", "2400", "2525", "2800", "3100", "3275", "3325", "3525", "3850", "4025", "4175", "4475", "4500", "4850", "4875", "5075", "5400", "5600", "5675", "5975", "6075"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 2"]], "filtered_resps": ["(B) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b07ced43f5c3c0d8c7482bb547c46572635f8ad6582e7e0325547fee5067a1ef", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, how many red objects are moving?\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 3\n(F) 5", "task_type": "needleQA", "pred_answer": "B", "answer": "E"}, "input": "In the collision event of objects in the video, how many red objects are moving?\n(A) 6\n(B) 2\n(C) 1\n(D) 4\n(E) 3\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 189, "doc": {"video_name": "test_needle_54.mp4", "duration": 725.14, "question": "In the collision event of objects in the video, which of the following is not responsible for the blue sphere's colliding with the cylinder?\n(A) the initial position of the cylinder\n(B) the friction between the blue sphere and the surface\n(C) the momentum of the blue sphere\n(D) the velocity of the cylinder\n(E) the gravitational force acting on the blue sphere\n(F) the presence of the yellow rubber object", "candidates": ["the initial position of the cylinder", "the friction between the blue sphere and the surface", "the momentum of the blue sphere", "the velocity of the cylinder", "the gravitational force acting on the blue sphere", "the presence of the yellow rubber object"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_44", "indices": ["25", "825", "1625", "2075", "2550", "2900", "3525", "3975", "5000", "5175", "5675", "6575", "7175", "7825", "8275", "8925", "9325", "10025", "10475", "10725", "11500", "11875", "12975", "13450", "13750", "14325", "15100", "15275", "15975", "16775", "17000", "18100"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a24a05f9cd0823d0174d5178db01424d7354469804794cd13028dafac68c7197", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, which of the following is not responsible for the blue sphere's colliding with the cylinder?\n(A) the initial position of the cylinder\n(B) the friction between the blue sphere and the surface\n(C) the momentum of the blue sphere\n(D) the velocity of the cylinder\n(E) the gravitational force acting on the blue sphere\n(F) the presence of the yellow rubber object", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the collision event of objects in the video, which of the following is not responsible for the blue sphere's colliding with the cylinder?\n(A) the initial position of the cylinder\n(B) the friction between the blue sphere and the surface\n(C) the momentum of the blue sphere\n(D) the velocity of the cylinder\n(E) the gravitational force acting on the blue sphere\n(F) the presence of the yellow rubber object\nOnly give the best option.\nBest option: ("}
{"doc_id": 193, "doc": {"video_name": "test_needle_41.mp4", "duration": 553.14, "question": "In the collision event of objects in the video, how many rubber cylinders enter the scene after the sphere enters the scene?\n(A) 2\n(B) 0\n(C) 1\n(D) 4\n(E) 5\n(F) 3", "candidates": ["2", "0", "1", "4", "5", "3"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_48", "indices": ["325", "625", "950", "1525", "1800", "2500", "2600", "3100", "3650", "4000", "4375", "5125", "5200", "5900", "6350", "6750", "7200", "7550", "7900", "8275", "8925", "9150", "9850", "9900", "10550", "10975", "11525", "11975", "12400", "12850", "13075", "13500"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf2f1ffe52bdb033b564484ac06e97b3550d602bdfc86ae2e31a141866787734", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, how many rubber cylinders enter the scene after the sphere enters the scene?\n(A) 2\n(B) 0\n(C) 1\n(D) 4\n(E) 5\n(F) 3", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "In the collision event of objects in the video, how many rubber cylinders enter the scene after the sphere enters the scene?\n(A) 2\n(B) 0\n(C) 1\n(D) 4\n(E) 5\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 197, "doc": {"video_name": "test_needle_18.mp4", "duration": 369.47, "question": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Cross\n(B) Square\n(C) Heart shape\n(D) Circle\n(E) Check mark\n(F) Triangle", "candidates": ["Cross", "Square", "Heart shape", "Circle", "Check mark", "Triangle"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_52", "indices": ["0", "609", "725", "1160", "1450", "1711", "2291", "2552", "2755", "3190", "3683", "3770", "4118", "4582", "4930", "5307", "5771", "5887", "6264", "6728", "7105", "7366", "7656", "8033", "8410", "8729", "9019", "9309", "9773", "10324", "10469", "10788"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Heart shape"]], "filtered_resps": ["(C) Heart shape"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cea42e8acc1e70dba2a503875960562916ea351719a702341acffe906aa3e61a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Cross\n(B) Square\n(C) Heart shape\n(D) Circle\n(E) Check mark\n(F) Triangle", "task_type": "needleQA", "pred_answer": "C", "answer": "A"}, "input": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Cross\n(B) Square\n(C) Heart shape\n(D) Circle\n(E) Check mark\n(F) Triangle\nOnly give the best option.\nBest option: ("}
{"doc_id": 201, "doc": {"video_name": "test_needle_58.mp4", "duration": 645.14, "question": "In the collision event of objects in the video, without the brown metal cylinder, which of the following will happen?\n(A) The yellow cylinder passes through the gray object without colliding\n(B) The gray object moves away without any collision\n(C) The yellow cylinder stops moving before reaching the gray object\n(D) The yellow cylinder changes direction and avoids the gray object\n(E) The gray object collides with another object instead of the yellow cylinder\n(F) The yellow cylinder collides with the gray object", "candidates": ["The yellow cylinder passes through the gray object without colliding", "The gray object moves away without any collision", "The yellow cylinder stops moving before reaching the gray object", "The yellow cylinder changes direction and avoids the gray object", "The gray object collides with another object instead of the yellow cylinder", "The yellow cylinder collides with the gray object"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_56", "indices": ["125", "625", "1400", "1650", "2175", "2500", "3125", "3575", "4375", "4725", "5475", "5750", "6050", "6875", "7375", "7975", "8375", "9025", "9500", "9900", "10225", "10800", "11275", "11925", "12350", "13050", "13150", "14050", "14375", "14775", "15525", "15675"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) The yellow cylinder collides with the gray object"]], "filtered_resps": ["(F) The yellow cylinder collides with the gray object"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d570630792894fd200fcb6296e9a68c791f71907a847b10a8ce66e9ef0cb30df", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, without the brown metal cylinder, which of the following will happen?\n(A) The yellow cylinder passes through the gray object without colliding\n(B) The gray object moves away without any collision\n(C) The yellow cylinder stops moving before reaching the gray object\n(D) The yellow cylinder changes direction and avoids the gray object\n(E) The gray object collides with another object instead of the yellow cylinder\n(F) The yellow cylinder collides with the gray object", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the collision event of objects in the video, without the brown metal cylinder, which of the following will happen?\n(A) The yellow cylinder passes through the gray object without colliding\n(B) The gray object moves away without any collision\n(C) The yellow cylinder stops moving before reaching the gray object\n(D) The yellow cylinder changes direction and avoids the gray object\n(E) The gray object collides with another object instead of the yellow cylinder\n(F) The yellow cylinder collides with the gray object\nOnly give the best option.\nBest option: ("}
{"doc_id": 205, "doc": {"video_name": "test_entv_6.mp4", "duration": 573.0751841594767, "question": "What weapon did the person who rescued the man in the forest use to kill the other soldiers?\n(A) Bomb\n(B) Gun\n(C) Knife\n(D) Arrow\n(E) Bow and arrow\n(F) Stick", "candidates": ["Bomb", "Gun", "Knife", "Arrow", "Bow and arrow", "Stick"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_0", "indices": ["230", "621", "943", "1311", "1932", "2185", "2967", "3381", "3703", "3864", "4508", "4692", "5405", "5911", "5980", "6808", "7130", "7291", "8096", "8533", "8556", "9292", "9591", "10074", "10373", "10948", "11385", "11615", "12006", "12811", "12903", "13501"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a1fb567e3fe92f5fb023e66b2c7a9553ecbaff6eae638e50ca8482ee3ddf2b0", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What weapon did the person who rescued the man in the forest use to kill the other soldiers?\n(A) Bomb\n(B) Gun\n(C) Knife\n(D) Arrow\n(E) Bow and arrow\n(F) Stick", "task_type": "plotQA", "pred_answer": "D", "answer": "E"}, "input": "What weapon did the person who rescued the man in the forest use to kill the other soldiers?\n(A) Bomb\n(B) Gun\n(C) Knife\n(D) Arrow\n(E) Bow and arrow\n(F) Stick\nOnly give the best option.\nBest option: ("}
{"doc_id": 209, "doc": {"video_name": "test_cartoon_1.mp4", "duration": 483.07277143454803, "question": "Where did the yellow sponge go after leaving?\n(A) Supermarket\n(B) Mall\n(C) Campus\n(D) Restaurant\n(E) Library\n(F) Bar", "candidates": ["Supermarket", "Mall", "Campus", "Restaurant", "Library", "Bar"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_4", "indices": ["0", "700", "825", "1175", "1700", "2050", "2350", "2750", "3150", "3675", "3750", "4400", "4825", "5225", "5400", "5675", "6075", "6400", "6775", "7150", "7750", "8200", "8425", "8950", "9300", "9450", "10050", "10325", "10700", "11225", "11325", "11975"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "23108f2c1075180aaff939962093830e87ef6b5669f16f4233118da427d6364e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did the yellow sponge go after leaving?\n(A) Supermarket\n(B) Mall\n(C) Campus\n(D) Restaurant\n(E) Library\n(F) Bar", "task_type": "plotQA", "pred_answer": "D", "answer": "A"}, "input": "Where did the yellow sponge go after leaving?\n(A) Supermarket\n(B) Mall\n(C) Campus\n(D) Restaurant\n(E) Library\n(F) Bar\nOnly give the best option.\nBest option: ("}
{"doc_id": 213, "doc": {"video_name": "test_cartoon_21.mp4", "duration": 662.3029651724227, "question": "What did the turtle use to make clothes for itself in the video?\n(A) Leaves\n(B) Plastic Bags\n(C) Iron Sheets\n(D) Branches\n(E) Cloth\n(F) Seaweed", "candidates": ["Leaves", "Plastic Bags", "Iron Sheets", "Branches", "Cloth", "Seaweed"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_8", "indices": ["400", "500", "1050", "1725", "2450", "2900", "3075", "3950", "4525", "5125", "5150", "5750", "6350", "7175", "7475", "8250", "8725", "8975", "9550", "9875", "10600", "11100", "11450", "12075", "12450", "13125", "13800", "14100", "14575", "15175", "15500", "16475"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "971ad7791a0d3f9606c2368e3ec9b86417d8923c208a892e3dd76370ca13f530", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What did the turtle use to make clothes for itself in the video?\n(A) Leaves\n(B) Plastic Bags\n(C) Iron Sheets\n(D) Branches\n(E) Cloth\n(F) Seaweed", "task_type": "plotQA", "pred_answer": "F", "answer": "F"}, "input": "What did the turtle use to make clothes for itself in the video?\n(A) Leaves\n(B) Plastic Bags\n(C) Iron Sheets\n(D) Branches\n(E) Cloth\n(F) Seaweed\nOnly give the best option.\nBest option: ("}
{"doc_id": 217, "doc": {"video_name": "test_movie101_97.mp4", "duration": 556.0, "question": "How many taxis are there at the beginning of the video?\n(A) 1\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 3", "candidates": ["1", "4", "6", "5", "2", "3"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_12", "indices": ["400", "450", "1050", "1425", "2050", "2175", "2925", "3100", "3625", "3975", "4425", "4950", "5275", "6000", "6250", "6525", "7350", "7500", "7875", "8600", "8950", "9375", "9925", "10225", "10775", "10875", "11675", "11700", "12525", "12600", "13425", "13550"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ca3f1023f66cf83ad46f980214de3ac82519ea65863f8f03a93d9dfc633c183c", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "How many taxis are there at the beginning of the video?\n(A) 1\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 3", "task_type": "plotQA", "pred_answer": "E", "answer": "F"}, "input": "How many taxis are there at the beginning of the video?\n(A) 1\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 221, "doc": {"video_name": "test_cartoon_5.mp4", "duration": 499.2630615463369, "question": "In the video, what first hits the cartoon octopus after it goes on stage?\n(A) Tomato\n(B) Cabbage\n(C) Lettuce\n(D) Cucumber\n(E) Green Pepper\n(F) Egg", "candidates": ["Tomato", "Cabbage", "Lettuce", "Cucumber", "Green Pepper", "Egg"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_16", "indices": ["300", "650", "1100", "1525", "1650", "2125", "2475", "2925", "3225", "3800", "3900", "4575", "4825", "5400", "5425", "5850", "6525", "6775", "7000", "7500", "7800", "8375", "8575", "8975", "9350", "9975", "10175", "10600", "10950", "11275", "12050", "12150"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96298f3e13e48740026f7cc03fd1e7ae11951399f31949209832f1feedaa776c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what first hits the cartoon octopus after it goes on stage?\n(A) Tomato\n(B) Cabbage\n(C) Lettuce\n(D) Cucumber\n(E) Green Pepper\n(F) Egg", "task_type": "plotQA", "pred_answer": "C", "answer": "A"}, "input": "In the video, what first hits the cartoon octopus after it goes on stage?\n(A) Tomato\n(B) Cabbage\n(C) Lettuce\n(D) Cucumber\n(E) Green Pepper\n(F) Egg\nOnly give the best option.\nBest option: ("}
{"doc_id": 225, "doc": {"video_name": "test_movie101_93.mp4", "duration": 641.0, "question": "What color are the flowers given by the man in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n(E) Pink\n(F) Purple", "candidates": ["Green", "White", "Blue", "Red", "Pink", "Purple"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_20", "indices": ["325", "600", "1350", "1925", "2275", "2925", "3375", "3675", "4125", "4975", "5425", "5600", "6150", "6575", "7300", "7950", "8300", "8975", "9450", "9600", "10175", "10950", "11475", "11550", "12000", "12600", "13050", "13825", "14425", "14625", "15325", "15975"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20ec455b026eee652df85dc5a420c3b838825740f24859a379be1359b4d5b650", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What color are the flowers given by the man in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n(E) Pink\n(F) Purple", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What color are the flowers given by the man in the video?\n(A) Green\n(B) White\n(C) Blue\n(D) Red\n(E) Pink\n(F) Purple\nOnly give the best option.\nBest option: ("}
{"doc_id": 229, "doc": {"video_name": "test_cartoon_13.mp4", "duration": 379.33, "question": "Where did the cartoon cat hide after being chased by the scissors?\n(A) Bottle\n(B) Mouse hole\n(C) Under the bed\n(D) Doghouse\n(E) Bowl\n(F) Cabinet", "candidates": ["Bottle", "Mouse hole", "Under the bed", "Doghouse", "Bowl", "Cabinet"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_24", "indices": ["290", "348", "783", "1334", "1711", "1885", "2320", "2813", "3103", "3364", "3712", "4089", "4582", "4611", "4988", "5452", "5974", "6177", "6583", "6902", "7134", "7598", "7801", "8381", "8555", "8874", "9280", "9889", "10150", "10556", "10846", "11078"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "217cf15f637add8c0dfbae7ce20346fa05aecd9cf44f7834ea412f93b989297e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did the cartoon cat hide after being chased by the scissors?\n(A) Bottle\n(B) Mouse hole\n(C) Under the bed\n(D) Doghouse\n(E) Bowl\n(F) Cabinet", "task_type": "plotQA", "pred_answer": "B", "answer": "A"}, "input": "Where did the cartoon cat hide after being chased by the scissors?\n(A) Bottle\n(B) Mouse hole\n(C) Under the bed\n(D) Doghouse\n(E) Bowl\n(F) Cabinet\nOnly give the best option.\nBest option: ("}
{"doc_id": 233, "doc": {"video_name": "test_entv_22.mp4", "duration": 407.116439508766, "question": "In the video, what are the men hiding from the women?\n(A) Magazine\n(B) Manual\n(C) Computer\n(D) Letter\n(E) Phone\n(F) Newspaper", "candidates": ["Magazine", "Manual", "Computer", "Letter", "Phone", "Newspaper"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_28", "indices": ["23", "322", "805", "943", "1357", "1748", "2070", "2300", "2645", "2806", "3197", "3634", "3749", "4163", "4393", "4853", "5106", "5221", "5727", "5842", "6371", "6463", "6762", "7061", "7590", "7613", "8004", "8441", "8556", "8855", "9407", "9729"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "da758576017789b4bf4e7940f0f977bce21e3ff0af93be8f4ee3499214c2912d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what are the men hiding from the women?\n(A) Magazine\n(B) Manual\n(C) Computer\n(D) Letter\n(E) Phone\n(F) Newspaper", "task_type": "plotQA", "pred_answer": "D", "answer": "C"}, "input": "In the video, what are the men hiding from the women?\n(A) Magazine\n(B) Manual\n(C) Computer\n(D) Letter\n(E) Phone\n(F) Newspaper\nOnly give the best option.\nBest option: ("}
{"doc_id": 237, "doc": {"video_name": "test_entv_1.mp4", "duration": 594.6644105054177, "question": "What animal do the people in the video bring back?\n(A) Rabbit\n(B) Fox\n(C) Wolf\n(D) Bear\n(E) Cat\n(F) Dog", "candidates": ["Rabbit", "Fox", "Wolf", "Bear", "Cat", "Dog"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_32", "indices": ["299", "736", "1196", "1334", "2139", "2300", "2898", "3519", "3542", "4347", "4439", "5083", "5543", "6164", "6210", "6923", "7498", "7613", "8395", "8418", "9177", "9315", "9821", "10649", "10764", "11339", "11845", "12075", "12489", "12880", "13570", "13800"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Wolf"]], "filtered_resps": ["(C) Wolf"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "341aba44d74c912f224ac58eca68f6daad5861462510faaa92903cc2bbc27247", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What animal do the people in the video bring back?\n(A) Rabbit\n(B) Fox\n(C) Wolf\n(D) Bear\n(E) Cat\n(F) Dog", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "What animal do the people in the video bring back?\n(A) Rabbit\n(B) Fox\n(C) Wolf\n(D) Bear\n(E) Cat\n(F) Dog\nOnly give the best option.\nBest option: ("}
{"doc_id": 241, "doc": {"video_name": "test_entv_21.mp4", "duration": 750.3097253436729, "question": "In the video, what is the woman in the gray t-shirt holding while watching TV with others?\n(A) Toy\n(B) Phone\n(C) Spoon\n(D) Wine glass\n(E) Flashlight\n(F) Snacks", "candidates": ["Toy", "Phone", "Spoon", "Wine glass", "Flashlight", "Snacks"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_36", "indices": ["161", "667", "1633", "1794", "2231", "3105", "3864", "4002", "4508", "5152", "5727", "6647", "6877", "7291", "7981", "8441", "9476", "9683", "10580", "11178", "11661", "12190", "12834", "12903", "13800", "14214", "14720", "15548", "16077", "16813", "17204", "17940"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3fe26fd51066ee47a037536e24a46b33c9421cc00f5e7efb7677240202d53740", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what is the woman in the gray t-shirt holding while watching TV with others?\n(A) Toy\n(B) Phone\n(C) Spoon\n(D) Wine glass\n(E) Flashlight\n(F) Snacks", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what is the woman in the gray t-shirt holding while watching TV with others?\n(A) Toy\n(B) Phone\n(C) Spoon\n(D) Wine glass\n(E) Flashlight\n(F) Snacks\nOnly give the best option.\nBest option: ("}
{"doc_id": 245, "doc": {"video_name": "test_entv_6.mp4", "duration": 573.0751841594767, "question": "Why did the man riding the white horse fall off?\n(A) The horse tripped over a stone\n(B) Knocked off by a soldier\n(C) The horse tripped over a branch\n(D) The horse was shot by an arrow\n(E) He was tripped by a branch\n(F) He was shot by a soldier", "candidates": ["The horse tripped over a stone", "Knocked off by a soldier", "The horse tripped over a branch", "The horse was shot by an arrow", "He was tripped by a branch", "He was shot by a soldier"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_40", "indices": ["46", "621", "943", "1311", "1932", "2530", "2852", "3358", "3657", "4209", "4577", "4853", "5405", "5543", "6187", "6440", "7061", "7452", "7751", "8234", "8648", "9223", "9798", "10143", "10603", "10718", "11316", "11661", "12397", "12811", "12972", "13616"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "642de0a4d0d2f20f8420365c906e0bd5ee76529b3d12fb2b5a17528864e0e313", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why did the man riding the white horse fall off?\n(A) The horse tripped over a stone\n(B) Knocked off by a soldier\n(C) The horse tripped over a branch\n(D) The horse was shot by an arrow\n(E) He was tripped by a branch\n(F) He was shot by a soldier", "task_type": "plotQA", "pred_answer": "A", "answer": "B"}, "input": "Why did the man riding the white horse fall off?\n(A) The horse tripped over a stone\n(B) Knocked off by a soldier\n(C) The horse tripped over a branch\n(D) The horse was shot by an arrow\n(E) He was tripped by a branch\n(F) He was shot by a soldier\nOnly give the best option.\nBest option: ("}
{"doc_id": 249, "doc": {"video_name": "test_cartoon_10.mp4", "duration": 586.918084061671, "question": "Who dirtied the yellow sponge's letter in the video?\n(A) Cartoon starfish\n(B) Cartoon snail\n(C) Cartoon sponge\n(D) Cartoon squirrel\n(E) Cartoon octopus\n(F) Cartoon crab", "candidates": ["Cartoon starfish", "Cartoon snail", "Cartoon sponge", "Cartoon squirrel", "Cartoon octopus", "Cartoon crab"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_44", "indices": ["300", "825", "925", "1400", "1850", "2275", "3000", "3175", "3850", "4325", "4950", "5050", "5800", "6000", "6400", "6850", "7550", "7825", "8600", "9025", "9325", "9775", "10175", "10500", "11400", "11450", "12250", "12325", "13175", "13625", "13825", "14350"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "714f270e3eba9dac6efcf4d2000e86e95072ade7a6a32fe3c1fb9677a9a90004", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who dirtied the yellow sponge's letter in the video?\n(A) Cartoon starfish\n(B) Cartoon snail\n(C) Cartoon sponge\n(D) Cartoon squirrel\n(E) Cartoon octopus\n(F) Cartoon crab", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "Who dirtied the yellow sponge's letter in the video?\n(A) Cartoon starfish\n(B) Cartoon snail\n(C) Cartoon sponge\n(D) Cartoon squirrel\n(E) Cartoon octopus\n(F) Cartoon crab\nOnly give the best option.\nBest option: ("}
{"doc_id": 253, "doc": {"video_name": "test_entv_25.mp4", "duration": 595.179419114368, "question": "In the video, what did the woman in the floral dress hit with a candy?\n(A) A man's mouth\n(B) A mirror\n(C) A woman's hand\n(D) Another woman's head\n(E) A bowl\n(F) A man's arm", "candidates": ["A man's mouth", "A mirror", "A woman's hand", "Another woman's head", "A bowl", "A man's arm"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_48", "indices": ["69", "529", "1265", "1748", "1932", "2369", "3036", "3128", "3956", "4255", "4485", "5290", "5704", "5796", "6624", "6808", "7268", "7590", "8142", "8510", "9085", "9591", "9798", "10626", "10718", "11247", "11914", "12420", "12788", "13018", "13662", "13869"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "615e9e80d6b3d9391f856e546982e81e3b31df22eb56aeb74ce3dc9156512f66", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what did the woman in the floral dress hit with a candy?\n(A) A man's mouth\n(B) A mirror\n(C) A woman's hand\n(D) Another woman's head\n(E) A bowl\n(F) A man's arm", "task_type": "plotQA", "pred_answer": "A", "answer": "D"}, "input": "In the video, what did the woman in the floral dress hit with a candy?\n(A) A man's mouth\n(B) A mirror\n(C) A woman's hand\n(D) Another woman's head\n(E) A bowl\n(F) A man's arm\nOnly give the best option.\nBest option: ("}
{"doc_id": 257, "doc": {"video_name": "test_entv_21.mp4", "duration": 750.3097253436729, "question": "What is the setting of the video?\n(A) Countryside\n(B) Forest\n(C) Campus\n(D) Desert\n(E) Beach\n(F) City", "candidates": ["Countryside", "Forest", "Campus", "Desert", "Beach", "City"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_2", "indices": ["368", "736", "1495", "2093", "2438", "2829", "3381", "4393", "4577", "5152", "6118", "6647", "6923", "7797", "8372", "8487", "9292", "9568", "10235", "10672", "11684", "12190", "12397", "13363", "13754", "14306", "15111", "15157", "16238", "16813", "17273", "17526"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) City"]], "filtered_resps": ["(F) City"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "564b6669cf70460dd827ec4577e7bc51947e9afbe7cc86f7605c40c42370fabf", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What is the setting of the video?\n(A) Countryside\n(B) Forest\n(C) Campus\n(D) Desert\n(E) Beach\n(F) City", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What is the setting of the video?\n(A) Countryside\n(B) Forest\n(C) Campus\n(D) Desert\n(E) Beach\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 261, "doc": {"video_name": "test_entv_25.mp4", "duration": 595.179419114368, "question": "What is the genre of this film?\n(A) Science Fiction\n(B) Horror\n(C) War\n(D) Action\n(E) Comedy\n(F) Documentary", "candidates": ["Science Fiction", "Horror", "War", "Action", "Comedy", "Documentary"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_6", "indices": ["276", "621", "1150", "1748", "2047", "2530", "2944", "3220", "3956", "4301", "4577", "5313", "5405", "5796", "6624", "6670", "7314", "7590", "8073", "8510", "8924", "9591", "9890", "10304", "11017", "11500", "11592", "12029", "12742", "13317", "13524", "13961"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d8eb6232ffec9fadfb67244551ea3d6dd3c19c90fbbc598860fa3df1a41b6777", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the genre of this film?\n(A) Science Fiction\n(B) Horror\n(C) War\n(D) Action\n(E) Comedy\n(F) Documentary", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What is the genre of this film?\n(A) Science Fiction\n(B) Horror\n(C) War\n(D) Action\n(E) Comedy\n(F) Documentary\nOnly give the best option.\nBest option: ("}
{"doc_id": 265, "doc": {"video_name": "test_BWB-4.mp4", "duration": 450.08, "question": "What type of film is this?\n(A) Action\n(B) Comedy\n(C) Animation\n(D) Horror\n(E) Romance\n(F) Documentary", "candidates": ["Action", "Comedy", "Animation", "Horror", "Romance", "Documentary"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_10", "indices": ["150", "425", "925", "1275", "1600", "2025", "2125", "2725", "2900", "3250", "3750", "3850", "4500", "4850", "5025", "5250", "5900", "6200", "6625", "6700", "7100", "7700", "7750", "8400", "8700", "8825", "9275", "9700", "10150", "10175", "10825", "10975"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Documentary"]], "filtered_resps": ["(F) Documentary"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ec5f2297e73bfa6bc727231581df09b99552c4868553fc7c576f3511c2abfed", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Comedy\n(C) Animation\n(D) Horror\n(E) Romance\n(F) Documentary", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What type of film is this?\n(A) Action\n(B) Comedy\n(C) Animation\n(D) Horror\n(E) Romance\n(F) Documentary\nOnly give the best option.\nBest option: ("}
{"doc_id": 269, "doc": {"video_name": "test_travel_0.mp4", "duration": 749.89, "question": "What type of video is this?\n(A) Planting\n(B) Travel\n(C) Education\n(D) Sports\n(E) Medical\n(F) Cooking", "candidates": ["Planting", "Travel", "Education", "Sports", "Medical", "Cooking"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_14", "indices": ["390", "1200", "1980", "2190", "3360", "3660", "4530", "4920", "6240", "6960", "7080", "7710", "8580", "9300", "10440", "10740", "11490", "12420", "13260", "13440", "14160", "15210", "16080", "16170", "17250", "17640", "18540", "19560", "20190", "20970", "21570", "21900"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5bc346361b5bb5eab5efc33205bf12507c3075a8236874f8dd634e728f3d15fb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of video is this?\n(A) Planting\n(B) Travel\n(C) Education\n(D) Sports\n(E) Medical\n(F) Cooking", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of video is this?\n(A) Planting\n(B) Travel\n(C) Education\n(D) Sports\n(E) Medical\n(F) Cooking\nOnly give the best option.\nBest option: ("}
{"doc_id": 273, "doc": {"video_name": "test_entv_5.mp4", "duration": 700.678139986795, "question": "In what setting does the video take place?\n(A) Castle\n(B) Forest\n(C) Desert\n(D) Countryside\n(E) Ocean\n(F) Campus", "candidates": ["Castle", "Forest", "Desert", "Countryside", "Ocean", "Campus"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_18", "indices": ["23", "552", "1173", "1679", "2093", "2990", "3289", "3887", "4186", "4738", "5244", "6026", "6716", "7314", "7544", "7889", "8395", "9154", "9890", "10051", "10649", "11293", "12029", "12075", "12857", "13317", "14053", "14260", "14812", "15479", "15847", "16284"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) Castle"]], "filtered_resps": ["(A) Castle"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1695c51c24dedbf06a7b01a0ed17bc77ffc68ec60cf2b3823cf7d9ec1f781e09", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Castle\n(B) Forest\n(C) Desert\n(D) Countryside\n(E) Ocean\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the video take place?\n(A) Castle\n(B) Forest\n(C) Desert\n(D) Countryside\n(E) Ocean\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 277, "doc": {"video_name": "test_game_3.mp4", "duration": 317.0, "question": "What is the primary mode of transportation the character is using in this video?\n(A) Riding a bus\n(B) Rowing a boat\n(C) Driving a car\n(D) Walking\n(E) Riding a bicycle\n(F) Riding a train", "candidates": ["Riding a bus", "Rowing a boat", "Driving a car", "Walking", "Riding a bicycle", "Riding a train"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_22", "indices": ["203", "493", "638", "1073", "1334", "1711", "1885", "2291", "2639", "2668", "2987", "3480", "3770", "4060", "4292", "4437", "4843", "5104", "5365", "5655", "6090", "6380", "6786", "6960", "7134", "7685", "7859", "8236", "8381", "8642", "9135", "9338"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b03f5b8055e14b60f10749456408c7c53d809fecf433a16bd5c47d0e56c4179a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the primary mode of transportation the character is using in this video?\n(A) Riding a bus\n(B) Rowing a boat\n(C) Driving a car\n(D) Walking\n(E) Riding a bicycle\n(F) Riding a train", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the primary mode of transportation the character is using in this video?\n(A) Riding a bus\n(B) Rowing a boat\n(C) Driving a car\n(D) Walking\n(E) Riding a bicycle\n(F) Riding a train\nOnly give the best option.\nBest option: ("}
{"doc_id": 281, "doc": {"video_name": "test_cartoon_15.mp4", "duration": 423.52, "question": "What genre of movie is the clip in the video?\n(A) Comedy\n(B) War\n(C) Animation\n(D) Documentary\n(E) Romance\n(F) Horror", "candidates": ["Comedy", "War", "Animation", "Documentary", "Romance", "Horror"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_26", "indices": ["58", "580", "783", "1508", "1885", "2320", "2552", "3132", "3364", "3567", "4234", "4321", "4930", "5249", "5684", "6235", "6351", "6757", "7453", "7540", "8178", "8294", "8903", "9309", "9831", "10150", "10585", "10672", "11165", "11832", "12006", "12615"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "489d6f326a8ddc92b48bb61aaa5014a5f62a94ceef4ec2729eed0a4fe4393624", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video?\n(A) Comedy\n(B) War\n(C) Animation\n(D) Documentary\n(E) Romance\n(F) Horror", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What genre of movie is the clip in the video?\n(A) Comedy\n(B) War\n(C) Animation\n(D) Documentary\n(E) Romance\n(F) Horror\nOnly give the best option.\nBest option: ("}
{"doc_id": 285, "doc": {"video_name": "test_AWD-3.mp4", "duration": 450.13, "question": "In what kind of environment does the video take place?\n(A) City\n(B) Station\n(C) Snowy area\n(D) Desert\n(E) School\n(F) Countryside", "candidates": ["City", "Station", "Snowy area", "Desert", "School", "Countryside"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_30", "indices": ["138", "368", "690", "989", "1380", "1610", "2162", "2323", "2829", "2944", "3519", "3588", "3979", "4255", "4554", "4830", "5198", "5612", "6095", "6141", "6716", "6831", "7153", "7705", "7889", "8188", "8602", "8786", "9085", "9545", "9867", "10327"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Snowy area"]], "filtered_resps": ["(C) Snowy area"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6670dcb7c4eda747575606b6fb3972d8b24f2304d907031526b832ddea06ea39", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what kind of environment does the video take place?\n(A) City\n(B) Station\n(C) Snowy area\n(D) Desert\n(E) School\n(F) Countryside", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what kind of environment does the video take place?\n(A) City\n(B) Station\n(C) Snowy area\n(D) Desert\n(E) School\n(F) Countryside\nOnly give the best option.\nBest option: ("}
{"doc_id": 289, "doc": {"video_name": "test_cartoon_3.mp4", "duration": 572.577543878504, "question": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) Snow Mountain\n(C) Forest\n(D) Desert\n(E) Ocean\n(F) City", "candidates": ["Grassland", "Snow Mountain", "Forest", "Desert", "Ocean", "City"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_34", "indices": ["350", "700", "1300", "1600", "2100", "2325", "2750", "3150", "3625", "4000", "4475", "5275", "5700", "6125", "6575", "6825", "7550", "7950", "8400", "8650", "9100", "9375", "10225", "10425", "10775", "11425", "12000", "12425", "12600", "13025", "13675", "14025"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea5ace4e99d9a0a14be6b81019ad9d9e4c1efd0d899cfab3ead1472b03f8ab9c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) Snow Mountain\n(C) Forest\n(D) Desert\n(E) Ocean\n(F) City", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) Snow Mountain\n(C) Forest\n(D) Desert\n(E) Ocean\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 293, "doc": {"video_name": "test_entv_13.mp4", "duration": 678.6181831810065, "question": "What type of film is this?\n(A) Horror\n(B) Documentary\n(C) Romance\n(D) Action\n(E) War\n(F) Comedy", "candidates": ["Horror", "Documentary", "Romance", "Action", "War", "Comedy"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_38", "indices": ["200", "600", "1250", "1650", "2175", "2950", "3625", "4075", "4325", "4825", "5450", "5800", "6575", "7375", "7425", "8025", "8775", "9250", "9725", "10450", "10975", "11250", "12050", "12650", "13200", "13300", "13875", "14550", "15325", "15475", "16225", "16875"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a55a29b0142e4fb5232108e0f2aa3efde60b9906859c466a756d37aa8527e9a0", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Horror\n(B) Documentary\n(C) Romance\n(D) Action\n(E) War\n(F) Comedy", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What type of film is this?\n(A) Horror\n(B) Documentary\n(C) Romance\n(D) Action\n(E) War\n(F) Comedy\nOnly give the best option.\nBest option: ("}
{"doc_id": 297, "doc": {"video_name": "test_game_9.mp4", "duration": 291.0, "question": "What is being constructed in the video?\n(A) Pond\n(B) Swimming pool\n(C) Garden\n(D) Lake\n(E) Yard\n(F) Landscape", "candidates": ["Pond", "Swimming pool", "Garden", "Lake", "Yard", "Landscape"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_42", "indices": ["58", "348", "638", "783", "1131", "1392", "1624", "1827", "2117", "2581", "2639", "3161", "3277", "3683", "3712", "4031", "4321", "4611", "4930", "5046", "5539", "5655", "5974", "6119", "6583", "6699", "7105", "7337", "7511", "7772", "7975", "8352"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Swimming pool"]], "filtered_resps": ["(B) Swimming pool"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b461aeab112081a88afa894e3e1b54d4c19e843d84f19f5a2fe62f8271d0cc9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is being constructed in the video?\n(A) Pond\n(B) Swimming pool\n(C) Garden\n(D) Lake\n(E) Yard\n(F) Landscape", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is being constructed in the video?\n(A) Pond\n(B) Swimming pool\n(C) Garden\n(D) Lake\n(E) Yard\n(F) Landscape\nOnly give the best option.\nBest option: ("}
{"doc_id": 301, "doc": {"video_name": "test_entv_1.mp4", "duration": 594.6644105054177, "question": "What genre is this film?\n(A) Action\n(B) Mystery\n(C) Crime\n(D) Comedy\n(E) Thriller\n(F) Adventure", "candidates": ["Action", "Mystery", "Crime", "Comedy", "Thriller", "Adventure"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_46", "indices": ["0", "621", "874", "1311", "2139", "2254", "2760", "3151", "3542", "4393", "4577", "4899", "5359", "6095", "6417", "6762", "7452", "7636", "8349", "8855", "9223", "9499", "9936", "10465", "10787", "11362", "11684", "12397", "12834", "13110", "13363", "13846"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Adventure"]], "filtered_resps": ["(F) Adventure"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6d8f92108f9e9ffef205b36e8550c73db089f5eb60501eddc8b721360db7366", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Action\n(B) Mystery\n(C) Crime\n(D) Comedy\n(E) Thriller\n(F) Adventure", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What genre is this film?\n(A) Action\n(B) Mystery\n(C) Crime\n(D) Comedy\n(E) Thriller\n(F) Adventure\nOnly give the best option.\nBest option: ("}
{"doc_id": 305, "doc": {"video_name": "test_movie101_93.mp4", "duration": 641.0, "question": "In what setting does the entire video take place?\n(A) Library\n(B) Beach\n(C) Forest\n(D) City\n(E) Campus\n(F) Desert", "candidates": ["Library", "Beach", "Forest", "City", "Campus", "Desert"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_50", "indices": ["0", "925", "1000", "1925", "2300", "2850", "3375", "3600", "4375", "4975", "5475", "5900", "6400", "6775", "7050", "7825", "8150", "8725", "9350", "9575", "10450", "10850", "11125", "11550", "12475", "12650", "13175", "13675", "14325", "14925", "15100", "15975"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) City"]], "filtered_resps": ["(D) City"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a4ab5ff0407d2e2d06d04ee07eccbe983e43a7e025423d92fd7bf35616d1406", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what setting does the entire video take place?\n(A) Library\n(B) Beach\n(C) Forest\n(D) City\n(E) Campus\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what setting does the entire video take place?\n(A) Library\n(B) Beach\n(C) Forest\n(D) City\n(E) Campus\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 309, "doc": {"video_name": "test_entv_25.mp4", "duration": 595.179419114368, "question": "In what setting does the video take place?\n(A) Forest\n(B) Campus\n(C) Countryside\n(D) Ocean\n(E) Urban\n(F) Desert", "candidates": ["Forest", "Campus", "Countryside", "Ocean", "Urban", "Desert"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_54", "indices": ["276", "851", "1035", "1311", "2047", "2553", "3082", "3542", "3956", "4255", "4692", "5198", "5543", "5773", "6624", "6785", "7268", "7636", "8050", "8510", "9108", "9775", "10120", "10649", "10994", "11224", "11845", "12397", "12880", "12903", "13616", "13846"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Urban"]], "filtered_resps": ["(E) Urban"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21bd15257c07c60e252aafd7450ddf866479e20a33cf8de6fdeaa4bc7c17f512", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Forest\n(B) Campus\n(C) Countryside\n(D) Ocean\n(E) Urban\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "In what setting does the video take place?\n(A) Forest\n(B) Campus\n(C) Countryside\n(D) Ocean\n(E) Urban\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 313, "doc": {"video_name": "test_cartoon_14.mp4", "duration": 453.5, "question": "What type of film is this?\n(A) Action\n(B) Documentary\n(C) Animation\n(D) Horror\n(E) War\n(F) Romance", "candidates": ["Action", "Documentary", "Animation", "Horror", "War", "Romance"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_58", "indices": ["377", "638", "1131", "1624", "1943", "2233", "2552", "3219", "3741", "3799", "4234", "4843", "5423", "5742", "6119", "6728", "6960", "7192", "7772", "8323", "8526", "9251", "9628", "9976", "10440", "10730", "11020", "11600", "12064", "12441", "12847", "13543"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0311733fc88daaf1a717c80ffd23c0884ece6f25f36c363ce7fcd8d0a3b6dd87", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Action\n(B) Documentary\n(C) Animation\n(D) Horror\n(E) War\n(F) Romance", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of film is this?\n(A) Action\n(B) Documentary\n(C) Animation\n(D) Horror\n(E) War\n(F) Romance\nOnly give the best option.\nBest option: ("}
{"doc_id": 317, "doc": {"video_name": "test_cartoon_5.mp4", "duration": 499.2630615463369, "question": "What type of film is this?\n(A) Horror\n(B) War\n(C) Documentary\n(D) Action\n(E) Romance\n(F) Animated Film", "candidates": ["Horror", "War", "Documentary", "Action", "Romance", "Animated Film"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_62", "indices": ["325", "625", "900", "1525", "1600", "2125", "2475", "3025", "3200", "3475", "4225", "4250", "4800", "5200", "5500", "6150", "6475", "6725", "7350", "7450", "8125", "8250", "8725", "9325", "9500", "9825", "10225", "10625", "11050", "11525", "11875", "12450"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Animated Film"]], "filtered_resps": ["(F) Animated Film"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a70053c7e95089aaf61f028749d3df2eed4eb3e40f71620cd9f7c165d68f70a", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Horror\n(B) War\n(C) Documentary\n(D) Action\n(E) Romance\n(F) Animated Film", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What type of film is this?\n(A) Horror\n(B) War\n(C) Documentary\n(D) Action\n(E) Romance\n(F) Animated Film\nOnly give the best option.\nBest option: ("}
{"doc_id": 321, "doc": {"video_name": "test_BWG-4.mp4", "duration": 450.0, "question": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Snow Mountain\n(C) Ocean\n(D) Plains\n(E) Rainforest\n(F) City", "candidates": ["Desert", "Snow Mountain", "Ocean", "Plains", "Rainforest", "City"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_66", "indices": ["250", "475", "875", "1100", "1625", "1975", "2300", "2700", "3125", "3250", "3625", "3975", "4500", "4700", "4900", "5375", "5800", "6075", "6475", "6800", "7275", "7550", "7750", "8150", "8700", "8950", "9375", "9525", "10000", "10375", "10625", "11050"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Plains"]], "filtered_resps": ["(D) Plains"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f0478f7a5e4a94c54c418472db6e731a14d67cf23d34819e4d16d5ba2f76d2a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Snow Mountain\n(C) Ocean\n(D) Plains\n(E) Rainforest\n(F) City", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what environment does the scene in the video take place?\n(A) Desert\n(B) Snow Mountain\n(C) Ocean\n(D) Plains\n(E) Rainforest\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 325, "doc": {"video_name": "test_cartoon_23.mp4", "duration": 727.120353586435, "question": "What genre of movie is the clip from?\n(A) Romantic film\n(B) Animated film\n(C) War film\n(D) Comedy film\n(E) Documentary\n(F) Horror film", "candidates": ["Romantic film", "Animated film", "War film", "Comedy film", "Documentary", "Horror film"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_70", "indices": ["350", "700", "1650", "2150", "2450", "3125", "3375", "4075", "4675", "5575", "5825", "6625", "7250", "7350", "8450", "8675", "9275", "9800", "10200", "11050", "11400", "12075", "12975", "13125", "13750", "14250", "14825", "15775", "15950", "16825", "17450", "18025"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09a94e88f88c66d971de02c84b999dc3bc89ed3030a8aed7b42df675d5f2bdbf", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip from?\n(A) Romantic film\n(B) Animated film\n(C) War film\n(D) Comedy film\n(E) Documentary\n(F) Horror film", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What genre of movie is the clip from?\n(A) Romantic film\n(B) Animated film\n(C) War film\n(D) Comedy film\n(E) Documentary\n(F) Horror film\nOnly give the best option.\nBest option: ("}
{"doc_id": 329, "doc": {"video_name": "test_travel_1.mp4", "duration": 614.96, "question": "Where is the video mainly shot?\n(A) River\n(B) Lake\n(C) Plateau\n(D) Grassland\n(E) Mountain\n(F) Ocean", "candidates": ["River", "Lake", "Plateau", "Grassland", "Mountain", "Ocean"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_74", "indices": ["92", "690", "1035", "1725", "1932", "2737", "3151", "3220", "3703", "4232", "4945", "5474", "5727", "6049", "6762", "7084", "7383", "8050", "8326", "9108", "9269", "9729", "10557", "10994", "11431", "11730", "12282", "12719", "12949", "13524", "13984", "14398"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Ocean"]], "filtered_resps": ["(F) Ocean"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c363a185f365e92eb4a967405f6a01e5c01116cdcddeb0d7b3da38b008c26d26", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where is the video mainly shot?\n(A) River\n(B) Lake\n(C) Plateau\n(D) Grassland\n(E) Mountain\n(F) Ocean", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "Where is the video mainly shot?\n(A) River\n(B) Lake\n(C) Plateau\n(D) Grassland\n(E) Mountain\n(F) Ocean\nOnly give the best option.\nBest option: ("}
{"doc_id": 333, "doc": {"video_name": "test_TFS-5.mp4", "duration": 450.0, "question": "Which of the following animals did not appear in the video?\n(A) Elephant\n(B) Lion\n(C) Rhino\n(D) Bird\n(E) Dragonfly\n(F) Crocodile", "candidates": ["Elephant", "Lion", "Rhino", "Bird", "Dragonfly", "Crocodile"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_78", "indices": ["150", "350", "750", "1225", "1625", "2075", "2100", "2750", "3025", "3275", "3600", "4100", "4275", "4750", "5200", "5475", "5900", "6225", "6650", "6750", "7025", "7400", "8050", "8125", "8550", "9000", "9350", "9575", "9825", "10500", "10750", "11050"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e61597296ba717254724ab3db794412f2a08391181fab590fc553f9367fe6c9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following animals did not appear in the video?\n(A) Elephant\n(B) Lion\n(C) Rhino\n(D) Bird\n(E) Dragonfly\n(F) Crocodile", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "Which of the following animals did not appear in the video?\n(A) Elephant\n(B) Lion\n(C) Rhino\n(D) Bird\n(E) Dragonfly\n(F) Crocodile\nOnly give the best option.\nBest option: ("}
{"doc_id": 337, "doc": {"video_name": "test_cartoon_17.mp4", "duration": 400.58, "question": "In what setting does the video take place?\n(A) Countryside\n(B) Campus\n(C) Castle\n(D) Desert\n(E) City\n(F) Forest", "candidates": ["Countryside", "Campus", "Castle", "Desert", "City", "Forest"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_82", "indices": ["29", "435", "783", "1160", "1595", "2117", "2320", "2755", "2987", "3596", "4089", "4408", "4553", "4930", "5394", "5626", "6235", "6670", "6844", "7192", "7511", "7888", "8497", "8613", "9338", "9483", "10005", "10411", "10701", "11107", "11513", "11861"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3fe0393489764b6c3f2afd8949b99baee0ce1b76cd0af72d271f6b8ee42867ac", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Countryside\n(B) Campus\n(C) Castle\n(D) Desert\n(E) City\n(F) Forest", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what setting does the video take place?\n(A) Countryside\n(B) Campus\n(C) Castle\n(D) Desert\n(E) City\n(F) Forest\nOnly give the best option.\nBest option: ("}
{"doc_id": 341, "doc": {"video_name": "test_BWB-5.mp4", "duration": 450.12, "question": "How many types of animals appeared in the video?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n(E) 4\n(F) 1", "candidates": ["3", "2", "5", "6", "4", "1"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_86", "indices": ["225", "575", "925", "1050", "1725", "1925", "2175", "2600", "3025", "3350", "3700", "3950", "4500", "4575", "5225", "5250", "5625", "6175", "6575", "6675", "7350", "7375", "7775", "8300", "8725", "9025", "9275", "9600", "9825", "10225", "10775", "10925"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a4f83ee4d5dc50e2a6834f868d547d6d3f4eb105981503a82109a15379d29614", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many types of animals appeared in the video?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n(E) 4\n(F) 1", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "D"}, "input": "How many types of animals appeared in the video?\n(A) 3\n(B) 2\n(C) 5\n(D) 6\n(E) 4\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 345, "doc": {"video_name": "test_TFS-5.mp4", "duration": 450.0, "question": "How many different types of animals appeared in the video?\n(A) 5\n(B) 4\n(C) 2\n(D) 1\n(E) 3\n(F) 6", "candidates": ["5", "4", "2", "1", "3", "6"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_90", "indices": ["150", "350", "800", "1225", "1625", "2075", "2100", "2750", "3050", "3275", "3600", "4075", "4525", "4750", "5200", "5475", "5900", "6225", "6650", "6750", "7025", "7400", "8050", "8100", "8450", "9000", "9375", "9575", "10100", "10475", "10750", "11050"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6757dad09eb81d2c373b395ba407bfe768d0136b9d29a70c97e9d7eff2e9415c", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "How many different types of animals appeared in the video?\n(A) 5\n(B) 4\n(C) 2\n(D) 1\n(E) 3\n(F) 6", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "A"}, "input": "How many different types of animals appeared in the video?\n(A) 5\n(B) 4\n(C) 2\n(D) 1\n(E) 3\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 349, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "Did I leave the projector on\n(A) not sure\n(B) possibly\n(C) I don't know\n(D) no\n(E) yes\n(F) maybe", "candidates": ["not sure", "possibly", "I don't know", "no", "yes", "maybe"], "answer": "E", "task_type": "ego", "question_id": "ER_3", "indices": ["390", "840", "990", "1740", "1800", "2340", "2700", "3420", "3630", "4380", "4800", "5190", "5400", "6120", "6390", "6960", "7560", "8010", "8400", "8820", "9030", "9780", "9930", "10410", "11010", "11280", "11820", "12180", "12870", "13170", "13620", "13980"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) yes"]], "filtered_resps": ["(E) yes"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80d185c97604219aaaf0c1c2b3266c31ef47a69400f6ae16ed418b3e50c04eeb", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Did I leave the projector on\n(A) not sure\n(B) possibly\n(C) I don't know\n(D) no\n(E) yes\n(F) maybe", "task_type": "ego", "pred_answer": "E", "answer": "E"}, "input": "Did I leave the projector on\n(A) not sure\n(B) possibly\n(C) I don't know\n(D) no\n(E) yes\n(F) maybe\nOnly give the best option.\nBest option: ("}
{"doc_id": 353, "doc": {"video_name": "test_ego_0.mp4", "duration": 480.0, "question": "What did I put on the drum?\n(A) Book\n(B) Machine tool\n(C) Bottle\n(D) Sticker\n(E) Paint\n(F) Cloth", "candidates": ["Book", "Machine tool", "Bottle", "Sticker", "Paint", "Cloth"], "answer": "B", "task_type": "ego", "question_id": "ER_7", "indices": ["150", "570", "1080", "1440", "2100", "2520", "2790", "3510", "3960", "4200", "4770", "5340", "5700", "5910", "6300", "6960", "7230", "8040", "8100", "8850", "9090", "9720", "10020", "10380", "11190", "11610", "11820", "12510", "13020", "13050", "13560", "14310"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Machine tool"]], "filtered_resps": ["(B) Machine tool"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a13dcd25a4b5854107f8cdfe02d8062e6b91c551d93cc911f5d57e39e1226737", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did I put on the drum?\n(A) Book\n(B) Machine tool\n(C) Bottle\n(D) Sticker\n(E) Paint\n(F) Cloth", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What did I put on the drum?\n(A) Book\n(B) Machine tool\n(C) Bottle\n(D) Sticker\n(E) Paint\n(F) Cloth\nOnly give the best option.\nBest option: ("}
{"doc_id": 357, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "Who did I interact with when I carried the plate of food from the kitchen divider\n(A) the elderly gentleman with a cane\n(B) the child with a blue hat\n(C) the lady with a yellow scarf\n(D) the man in black shorts\n(E) the teenager in a green hoodie\n(F) the woman in a red dress", "candidates": ["the elderly gentleman with a cane", "the child with a blue hat", "the lady with a yellow scarf", "the man in black shorts", "the teenager in a green hoodie", "the woman in a red dress"], "answer": "D", "task_type": "ego", "question_id": "ER_11", "indices": ["360", "780", "930", "1710", "1830", "2400", "2820", "3480", "3600", "4260", "4740", "5280", "5610", "5940", "6510", "6750", "7470", "7890", "8220", "8670", "9120", "9600", "10320", "10350", "10800", "11400", "11970", "12240", "12630", "13200", "13530", "14250"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) the man in black shorts"]], "filtered_resps": ["(D) the man in black shorts"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef78c6b7ace343b2305da83af90ed93471797c9e771684017aa00ff8c2ed0a60", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Who did I interact with when I carried the plate of food from the kitchen divider\n(A) the elderly gentleman with a cane\n(B) the child with a blue hat\n(C) the lady with a yellow scarf\n(D) the man in black shorts\n(E) the teenager in a green hoodie\n(F) the woman in a red dress", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Who did I interact with when I carried the plate of food from the kitchen divider\n(A) the elderly gentleman with a cane\n(B) the child with a blue hat\n(C) the lady with a yellow scarf\n(D) the man in black shorts\n(E) the teenager in a green hoodie\n(F) the woman in a red dress\nOnly give the best option.\nBest option: ("}
{"doc_id": 361, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "Where was the scissor?\n(A) In the drawer under the table\n(B) On the window behind the rack\n(C) Next to the lamp on the nightstand\n(D) On the shelf next to the books\n(E) Inside the cabinet above the sink\n(F) Under the couch in the living room", "candidates": ["In the drawer under the table", "On the window behind the rack", "Next to the lamp on the nightstand", "On the shelf next to the books", "Inside the cabinet above the sink", "Under the couch in the living room"], "answer": "B", "task_type": "ego", "question_id": "ER_15", "indices": ["240", "780", "990", "1530", "2130", "2430", "3030", "3360", "3810", "4260", "4860", "5220", "5730", "6270", "6330", "6870", "7200", "7800", "8130", "8670", "9420", "9450", "10290", "10680", "11220", "11430", "11850", "12510", "12660", "13230", "13920", "13950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "25a3e43c4d088f00ca936e2ea14e153bcddbef51b562a8207486908e7552e54b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the scissor?\n(A) In the drawer under the table\n(B) On the window behind the rack\n(C) Next to the lamp on the nightstand\n(D) On the shelf next to the books\n(E) Inside the cabinet above the sink\n(F) Under the couch in the living room", "task_type": "ego", "pred_answer": "F", "answer": "B"}, "input": "Where was the scissor?\n(A) In the drawer under the table\n(B) On the window behind the rack\n(C) Next to the lamp on the nightstand\n(D) On the shelf next to the books\n(E) Inside the cabinet above the sink\n(F) Under the couch in the living room\nOnly give the best option.\nBest option: ("}
{"doc_id": 365, "doc": {"video_name": "test_ego_0.mp4", "duration": 480.0, "question": "Did I leave the drawer door open?\n(A) Not sure\n(B) Possibly\n(C) Maybe\n(D) I don't remember\n(E) Yes\n(F) No", "candidates": ["Not sure", "Possibly", "Maybe", "I don't remember", "Yes", "No"], "answer": "F", "task_type": "ego", "question_id": "ER_19", "indices": ["360", "450", "900", "1350", "2130", "2310", "2790", "3240", "3780", "4050", "4590", "5340", "5820", "6120", "6510", "6780", "7380", "7830", "8250", "8580", "9330", "9690", "10050", "10710", "10980", "11280", "11790", "12510", "12900", "13110", "13650", "14070"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Yes"]], "filtered_resps": ["(E) Yes"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "69ef8f6cc95edabb3f0dbda4d09ad89c760446104f006f94237068695af1afd6", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Did I leave the drawer door open?\n(A) Not sure\n(B) Possibly\n(C) Maybe\n(D) I don't remember\n(E) Yes\n(F) No", "task_type": "ego", "pred_answer": "E", "answer": "F"}, "input": "Did I leave the drawer door open?\n(A) Not sure\n(B) Possibly\n(C) Maybe\n(D) I don't remember\n(E) Yes\n(F) No\nOnly give the best option.\nBest option: ("}
{"doc_id": 369, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "Did I  close the cupboard I took the plastic jar from\n(A) Maybe\n(B) I think so\n(C) Yes\n(D) Not sure\n(E) I don't remember\n(F) No", "candidates": ["Maybe", "I think so", "Yes", "Not sure", "I don't remember", "No"], "answer": "C", "task_type": "ego", "question_id": "ER_23", "indices": ["360", "780", "1260", "1440", "2100", "2370", "3090", "3180", "3600", "4380", "4860", "5220", "5400", "6240", "6330", "6990", "7260", "7740", "8430", "8910", "9120", "9810", "9900", "10710", "11220", "11280", "12030", "12480", "12750", "13410", "13860", "14310"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9657a4010af23b994d7c7e259aef09cc32f5f382690232d4215d469e76dfc954", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I  close the cupboard I took the plastic jar from\n(A) Maybe\n(B) I think so\n(C) Yes\n(D) Not sure\n(E) I don't remember\n(F) No", "task_type": "ego", "pred_answer": "F", "answer": "C"}, "input": "Did I  close the cupboard I took the plastic jar from\n(A) Maybe\n(B) I think so\n(C) Yes\n(D) Not sure\n(E) I don't remember\n(F) No\nOnly give the best option.\nBest option: ("}
{"doc_id": 373, "doc": {"video_name": "test_ego_1.mp4", "duration": 480.0, "question": "Where was the Russell Stover before I picked it up ?\n(A) The Russell Stover was in the refrigerator before I picked it up.\n(B) The Russell Stover was on the counter before I picked it up.\n(C) The Russell Stover was on the shelf before I picked it up.\n(D) The Russell Stover was on the table before I picked it up.\n(E) The Russell Stover was on the rack before I picked it up.\n(F) The Russell Stover was in the drawer before I picked it up.", "candidates": ["The Russell Stover was in the refrigerator before I picked it up.", "The Russell Stover was on the counter before I picked it up.", "The Russell Stover was on the shelf before I picked it up.", "The Russell Stover was on the table before I picked it up.", "The Russell Stover was on the rack before I picked it up.", "The Russell Stover was in the drawer before I picked it up."], "answer": "E", "task_type": "ego", "question_id": "ER_27", "indices": ["420", "570", "1050", "1350", "2130", "2370", "2760", "3570", "3600", "4140", "4770", "5160", "5640", "6000", "6600", "6810", "7230", "7770", "8100", "8880", "9030", "9540", "10320", "10710", "10800", "11550", "11970", "12150", "12930", "13470", "13530", "14040"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1296db13b2c92a9dabe3eeccd1b3f8dfbc29c50ef590c3bf3298dd1dbcf9f5b6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Where was the Russell Stover before I picked it up ?\n(A) The Russell Stover was in the refrigerator before I picked it up.\n(B) The Russell Stover was on the counter before I picked it up.\n(C) The Russell Stover was on the shelf before I picked it up.\n(D) The Russell Stover was on the table before I picked it up.\n(E) The Russell Stover was on the rack before I picked it up.\n(F) The Russell Stover was in the drawer before I picked it up.", "task_type": "ego", "pred_answer": "E", "answer": "E"}, "input": "Where was the Russell Stover before I picked it up ?\n(A) The Russell Stover was in the refrigerator before I picked it up.\n(B) The Russell Stover was on the counter before I picked it up.\n(C) The Russell Stover was on the shelf before I picked it up.\n(D) The Russell Stover was on the table before I picked it up.\n(E) The Russell Stover was on the rack before I picked it up.\n(F) The Russell Stover was in the drawer before I picked it up.\nOnly give the best option.\nBest option: ("}
{"doc_id": 377, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "Where did I put the aluminum foil plate\n(A) in the microwave\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) in the oven\n(E) in the  fridge\n(F) in the pantry", "candidates": ["in the microwave", "on the kitchen counter", "in the dishwasher", "in the oven", "in the  fridge", "in the pantry"], "answer": "E", "task_type": "ego", "question_id": "ER_31", "indices": ["270", "570", "1290", "1590", "2220", "2580", "2820", "3450", "3930", "4260", "4680", "4950", "5700", "6000", "6540", "7080", "7530", "7950", "8160", "8940", "9120", "9600", "10080", "10350", "11190", "11430", "12090", "12210", "12600", "13290", "13620", "14250"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c9a7b1fd4f0ed22c0e6a46fc148e7d9eab26b02a16c2a01feff1f1c10f1d6e0d", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Where did I put the aluminum foil plate\n(A) in the microwave\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) in the oven\n(E) in the  fridge\n(F) in the pantry", "task_type": "ego", "pred_answer": "B", "answer": "E"}, "input": "Where did I put the aluminum foil plate\n(A) in the microwave\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) in the oven\n(E) in the  fridge\n(F) in the pantry\nOnly give the best option.\nBest option: ("}
{"doc_id": 381, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "In what location did I last see the dog\n(A) in the front yard\n(B) in the kitchen\n(C) in the backyard\n(D) in the garage\n(E) in the bedroom\n(F) in the living room", "candidates": ["in the front yard", "in the kitchen", "in the backyard", "in the garage", "in the bedroom", "in the living room"], "answer": "B", "task_type": "ego", "question_id": "ER_35", "indices": ["0", "720", "1020", "1710", "2100", "2340", "2820", "3540", "3840", "4410", "4830", "5070", "5580", "6270", "6720", "6810", "7500", "8070", "8220", "8760", "9120", "9810", "9960", "10650", "11220", "11400", "11970", "12540", "12870", "13380", "13500", "14340"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de9be3436eebb568a38d6b9e2d8cc858cf99665d4a7a7f35a6284eee48d8e93f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what location did I last see the dog\n(A) in the front yard\n(B) in the kitchen\n(C) in the backyard\n(D) in the garage\n(E) in the bedroom\n(F) in the living room", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "In what location did I last see the dog\n(A) in the front yard\n(B) in the kitchen\n(C) in the backyard\n(D) in the garage\n(E) in the bedroom\n(F) in the living room\nOnly give the best option.\nBest option: ("}
{"doc_id": 385, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "What word was printed on the cap of the bottle with yellow liquid on the window?\n(A) Nestea\n(B) Snapple\n(C) Pure Leaf\n(D) Tejava.\n(E) Arizona\n(F) Lipton", "candidates": ["Nestea", "Snapple", "Pure Leaf", "Tejava.", "Arizona", "Lipton"], "answer": "D", "task_type": "ego", "question_id": "ER_39", "indices": ["180", "540", "1230", "1350", "1920", "2460", "3000", "3180", "3630", "4290", "4710", "5220", "5730", "6180", "6420", "7170", "7440", "7710", "8490", "8970", "9420", "9720", "10080", "10410", "10800", "11370", "12030", "12510", "12720", "13080", "13860", "14370"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54cfbbb37414564c668915d652c09bb7a4ad939431ef3caca1fad974363cfccd", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What word was printed on the cap of the bottle with yellow liquid on the window?\n(A) Nestea\n(B) Snapple\n(C) Pure Leaf\n(D) Tejava.\n(E) Arizona\n(F) Lipton", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What word was printed on the cap of the bottle with yellow liquid on the window?\n(A) Nestea\n(B) Snapple\n(C) Pure Leaf\n(D) Tejava.\n(E) Arizona\n(F) Lipton\nOnly give the best option.\nBest option: ("}
{"doc_id": 389, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "What word was printed on the white box kept in the middle rack of the white stand?\n(A) Triscuit.\n(B) Biscuit\n(C) Cracker\n(D) Wheat Thin\n(E) Cheez-It\n(F) Snack", "candidates": ["Triscuit.", "Biscuit", "Cracker", "Wheat Thin", "Cheez-It", "Snack"], "answer": "A", "task_type": "ego", "question_id": "ER_43", "indices": ["90", "630", "1050", "1410", "1920", "2430", "2790", "3300", "3630", "4170", "4560", "5100", "5430", "6180", "6330", "7170", "7470", "7650", "8400", "8880", "9420", "9450", "9990", "10680", "11040", "11580", "11790", "12510", "12690", "13470", "13590", "14310"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0daaf8efd999af2efc20ae3e678fcaad3410408bd32491cbc19ed4c77f28d8b9", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What word was printed on the white box kept in the middle rack of the white stand?\n(A) Triscuit.\n(B) Biscuit\n(C) Cracker\n(D) Wheat Thin\n(E) Cheez-It\n(F) Snack", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What word was printed on the white box kept in the middle rack of the white stand?\n(A) Triscuit.\n(B) Biscuit\n(C) Cracker\n(D) Wheat Thin\n(E) Cheez-It\n(F) Snack\nOnly give the best option.\nBest option: ("}
{"doc_id": 393, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "In what location did I first see the yellow ball\n(A) in the closet\n(B) behind the door\n(C) under the chair\n(D) next to the couch\n(E) inside the drawer\n(F) on the table", "candidates": ["in the closet", "behind the door", "under the chair", "next to the couch", "inside the drawer", "on the table"], "answer": "C", "task_type": "ego", "question_id": "ER_47", "indices": ["150", "840", "1080", "1740", "2190", "2430", "2700", "3480", "3960", "4230", "4650", "5370", "5700", "6090", "6690", "6780", "7590", "7680", "8460", "8970", "9060", "9840", "9960", "10680", "10890", "11490", "11700", "12180", "12930", "13290", "13740", "14160"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ed270778c974156bcd1566affd2ca1ac084df2b532cf5a3a1bae001140cff981", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what location did I first see the yellow ball\n(A) in the closet\n(B) behind the door\n(C) under the chair\n(D) next to the couch\n(E) inside the drawer\n(F) on the table", "task_type": "ego", "pred_answer": "D", "answer": "C"}, "input": "In what location did I first see the yellow ball\n(A) in the closet\n(B) behind the door\n(C) under the chair\n(D) next to the couch\n(E) inside the drawer\n(F) on the table\nOnly give the best option.\nBest option: ("}
{"doc_id": 397, "doc": {"video_name": "test_ego_1.mp4", "duration": 480.0, "question": "Where was the ballast point brewing before I picked it up?\n\n\n(A) The ballast point brewing was on the counter before I picked it up.\n(B) The ballast point brewing was on the porch before I picked it up.\n(C) The ballast point brewing was in the garage before I picked it up.\n(D) The ballast point brewing was in the pantry before I picked it up.\n(E) The ballast point brewing was in the basement before I picked it up.\n(F) The ballast point brewing was in the fridge before I picked it up.", "candidates": ["The ballast point brewing was on the counter before I picked it up.", "The ballast point brewing was on the porch before I picked it up.", "The ballast point brewing was in the garage before I picked it up.", "The ballast point brewing was in the pantry before I picked it up.", "The ballast point brewing was in the basement before I picked it up.", "The ballast point brewing was in the fridge before I picked it up."], "answer": "F", "task_type": "ego", "question_id": "ER_51", "indices": ["210", "720", "1110", "1440", "1890", "2370", "3060", "3360", "3600", "4260", "4740", "4980", "5610", "6120", "6660", "6810", "7200", "8040", "8340", "8880", "9090", "9540", "9960", "10590", "10860", "11610", "11970", "12390", "12930", "13230", "13590", "14040"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b52067945d50d7b0287b7693df2ea86bea040c92bcc6a266adb4c0481e2b47d8", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where was the ballast point brewing before I picked it up?\n\n\n(A) The ballast point brewing was on the counter before I picked it up.\n(B) The ballast point brewing was on the porch before I picked it up.\n(C) The ballast point brewing was in the garage before I picked it up.\n(D) The ballast point brewing was in the pantry before I picked it up.\n(E) The ballast point brewing was in the basement before I picked it up.\n(F) The ballast point brewing was in the fridge before I picked it up.", "task_type": "ego", "pred_answer": "A", "answer": "F"}, "input": "Where was the ballast point brewing before I picked it up?\n\n\n(A) The ballast point brewing was on the counter before I picked it up.\n(B) The ballast point brewing was on the porch before I picked it up.\n(C) The ballast point brewing was in the garage before I picked it up.\n(D) The ballast point brewing was in the pantry before I picked it up.\n(E) The ballast point brewing was in the basement before I picked it up.\n(F) The ballast point brewing was in the fridge before I picked it up.\nOnly give the best option.\nBest option: ("}
{"doc_id": 401, "doc": {"video_name": "test_tutorial_6.mp4", "duration": 1105.96, "question": "The sequence of actions the man takes while cooking\n(A) Cutting food, adding condiments, stir-frying food, plating the food, sprinkling cilantro and lemon juice\n(B) Cutting food, sprinkling cilantro and lemon juice, stir-frying food, adding condiments, plating the food\n(C) Cutting food, adding condiments, sprinkling cilantro and lemon juice, stir-frying food, plating the food\n(D) Cutting food, stir-frying food, adding condiments, plating the food, sprinkling cilantro and lemon juice\n(E) Cutting food, stir-frying food, adding condiments, sprinkling cilantro and lemon juice, plating the food\n(F) Cutting food, stir-frying food, sprinkling cilantro and lemon juice, adding condiments, plating the food", "candidates": ["Cutting food, adding condiments, stir-frying food, plating the food, sprinkling cilantro and lemon juice", "Cutting food, sprinkling cilantro and lemon juice, stir-frying food, adding condiments, plating the food", "Cutting food, adding condiments, sprinkling cilantro and lemon juice, stir-frying food, plating the food", "Cutting food, stir-frying food, adding condiments, plating the food, sprinkling cilantro and lemon juice", "Cutting food, stir-frying food, adding condiments, sprinkling cilantro and lemon juice, plating the food", "Cutting food, stir-frying food, sprinkling cilantro and lemon juice, adding condiments, plating the food"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_2", "indices": ["825", "1050", "2425", "3400", "3500", "4850", "5725", "6025", "7150", "8375", "9150", "10200", "10675", "11525", "12450", "13000", "14100", "15000", "16050", "16575", "17800", "18575", "19325", "20025", "20825", "21850", "23175", "23275", "24500", "25675", "26600", "27225"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e369a3135005f29ad85577fd64734c9eba9e73ff56aa1e387928c13489aca420", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "The sequence of actions the man takes while cooking\n(A) Cutting food, adding condiments, stir-frying food, plating the food, sprinkling cilantro and lemon juice\n(B) Cutting food, sprinkling cilantro and lemon juice, stir-frying food, adding condiments, plating the food\n(C) Cutting food, adding condiments, sprinkling cilantro and lemon juice, stir-frying food, plating the food\n(D) Cutting food, stir-frying food, adding condiments, plating the food, sprinkling cilantro and lemon juice\n(E) Cutting food, stir-frying food, adding condiments, sprinkling cilantro and lemon juice, plating the food\n(F) Cutting food, stir-frying food, sprinkling cilantro and lemon juice, adding condiments, plating the food", "task_type": "tutorialQA", "pred_answer": "E", "answer": "D"}, "input": "The sequence of actions the man takes while cooking\n(A) Cutting food, adding condiments, stir-frying food, plating the food, sprinkling cilantro and lemon juice\n(B) Cutting food, sprinkling cilantro and lemon juice, stir-frying food, adding condiments, plating the food\n(C) Cutting food, adding condiments, sprinkling cilantro and lemon juice, stir-frying food, plating the food\n(D) Cutting food, stir-frying food, adding condiments, plating the food, sprinkling cilantro and lemon juice\n(E) Cutting food, stir-frying food, adding condiments, sprinkling cilantro and lemon juice, plating the food\n(F) Cutting food, stir-frying food, sprinkling cilantro and lemon juice, adding condiments, plating the food\nOnly give the best option.\nBest option: ("}
{"doc_id": 405, "doc": {"video_name": "test_tutorial_16.mp4", "duration": 500.02, "question": "In the video, which action is performed after drawing the general outline of the character\n(A) Erasing unnecessary lines\n(B) Adding light and shadow effects\n(C) Drawing the position of the ears\n(D) Defining facial expression\n(E) Adding specific features\n(F) Adjusting facial proportions", "candidates": ["Erasing unnecessary lines", "Adding light and shadow effects", "Drawing the position of the ears", "Defining facial expression", "Adding specific features", "Adjusting facial proportions"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_6", "indices": ["192", "360", "888", "1464", "1824", "2016", "2376", "2784", "3000", "3600", "3936", "4368", "4536", "4896", "5496", "5784", "6048", "6408", "6768", "7344", "7704", "7944", "8520", "8736", "9312", "9672", "9744", "10176", "10632", "11016", "11400", "11784"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "542615c9e76c82c01f4ff5065ef4bd75e399c7ee9002dace873664abb3ae7f6b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, which action is performed after drawing the general outline of the character\n(A) Erasing unnecessary lines\n(B) Adding light and shadow effects\n(C) Drawing the position of the ears\n(D) Defining facial expression\n(E) Adding specific features\n(F) Adjusting facial proportions", "task_type": "tutorialQA", "pred_answer": "E", "answer": "A"}, "input": "In the video, which action is performed after drawing the general outline of the character\n(A) Erasing unnecessary lines\n(B) Adding light and shadow effects\n(C) Drawing the position of the ears\n(D) Defining facial expression\n(E) Adding specific features\n(F) Adjusting facial proportions\nOnly give the best option.\nBest option: ("}
{"doc_id": 409, "doc": {"video_name": "test_tutorial_12.mp4", "duration": 279.38, "question": "The sequence of tools used in the video\n(A) Glue, Scissors, Creaser, Small stick, Tweezers\n(B) Glue, Creaser, Small stick, Tweezers, Scissors\n(C) Scissors, Glue, Creaser, Small stick, Tweezers\n(D) Glue, Creaser, Scissors, Small stick, Tweezers\n(E) Tweezers, Scissors, Glue, Creaser, Small stick\n(F) Glue, Creaser, Small stick, Scissors, Tweezers", "candidates": ["Glue, Scissors, Creaser, Small stick, Tweezers", "Glue, Creaser, Small stick, Tweezers, Scissors", "Scissors, Glue, Creaser, Small stick, Tweezers", "Glue, Creaser, Scissors, Small stick, Tweezers", "Tweezers, Scissors, Glue, Creaser, Small stick", "Glue, Creaser, Small stick, Scissors, Tweezers"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_10", "indices": ["210", "450", "690", "750", "1050", "1350", "1770", "1950", "2070", "2520", "2580", "2940", "3210", "3450", "3870", "4110", "4290", "4500", "4740", "5160", "5250", "5670", "5880", "6120", "6450", "6510", "6780", "7170", "7350", "7770", "8070", "8220"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a7fbdfaa718d5d9bcd83520e328e0c1119dca0d155ac4450ab6df79078bedf03", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "The sequence of tools used in the video\n(A) Glue, Scissors, Creaser, Small stick, Tweezers\n(B) Glue, Creaser, Small stick, Tweezers, Scissors\n(C) Scissors, Glue, Creaser, Small stick, Tweezers\n(D) Glue, Creaser, Scissors, Small stick, Tweezers\n(E) Tweezers, Scissors, Glue, Creaser, Small stick\n(F) Glue, Creaser, Small stick, Scissors, Tweezers", "task_type": "tutorialQA", "pred_answer": "D", "answer": "C"}, "input": "The sequence of tools used in the video\n(A) Glue, Scissors, Creaser, Small stick, Tweezers\n(B) Glue, Creaser, Small stick, Tweezers, Scissors\n(C) Scissors, Glue, Creaser, Small stick, Tweezers\n(D) Glue, Creaser, Scissors, Small stick, Tweezers\n(E) Tweezers, Scissors, Glue, Creaser, Small stick\n(F) Glue, Creaser, Small stick, Scissors, Tweezers\nOnly give the best option.\nBest option: ("}
{"doc_id": 413, "doc": {"video_name": "test_tutorial_21.mp4", "duration": 1765.25, "question": "Which technique is primarily used when editing the landscape photo in the video?\n(A) Sharpening\n(B) Color correction\n(C) Compositing\n(D) Contrast enhancement\n(E) Saturation adjustment\n(F) Exposure adjustment", "candidates": ["Sharpening", "Color correction", "Compositing", "Contrast enhancement", "Saturation adjustment", "Exposure adjustment"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_14", "indices": ["810", "2730", "3330", "5190", "7110", "8610", "10260", "12780", "13590", "15570", "16980", "19770", "21240", "22920", "24540", "25200", "27240", "28110", "30690", "32580", "34080", "34830", "36450", "38010", "40470", "42930", "44520", "44820", "47940", "47970", "50730", "52440"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Compositing"]], "filtered_resps": ["(C) Compositing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a7ddb472207617e59c42073f22c86791f91f8ac0303a68df623227aee8181c5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which technique is primarily used when editing the landscape photo in the video?\n(A) Sharpening\n(B) Color correction\n(C) Compositing\n(D) Contrast enhancement\n(E) Saturation adjustment\n(F) Exposure adjustment", "task_type": "tutorialQA", "pred_answer": "C", "answer": "C"}, "input": "Which technique is primarily used when editing the landscape photo in the video?\n(A) Sharpening\n(B) Color correction\n(C) Compositing\n(D) Contrast enhancement\n(E) Saturation adjustment\n(F) Exposure adjustment\nOnly give the best option.\nBest option: ("}
{"doc_id": 417, "doc": {"video_name": "test_tutorial_17.mp4", "duration": 528.23, "question": "Sequence of tools used in the handcraft process in the video\n(A) Tape, glue, scissors, carving knife, spray paint\n(B) Tape, glue, carving knife, spray paint, scissors\n(C) Carving knife, scissors, tape, glue, spray paint\n(D) Tape, scissors, glue, carving knife, spray paint\n(E) Scissors, tape, glue, carving knife, spray paint\n(F) Tape, glue, carving knife, scissors, spray paint", "candidates": ["Tape, glue, scissors, carving knife, spray paint", "Tape, glue, carving knife, spray paint, scissors", "Carving knife, scissors, tape, glue, spray paint", "Tape, scissors, glue, carving knife, spray paint", "Scissors, tape, glue, carving knife, spray paint", "Tape, glue, carving knife, scissors, spray paint"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_18", "indices": ["531", "944", "2242", "3894", "4838", "5074", "6431", "6903", "8142", "9086", "10561", "10974", "12567", "13393", "14101", "15694", "16166", "17700", "18703", "18821", "19942", "21063", "22007", "23659", "24013", "24662", "25724", "27435", "27966", "29028", "29618", "30857"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19845b7ec9d13bab428fd0d7fea1f006f136e237b2b86e7e632146de2d3ec77e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Sequence of tools used in the handcraft process in the video\n(A) Tape, glue, scissors, carving knife, spray paint\n(B) Tape, glue, carving knife, spray paint, scissors\n(C) Carving knife, scissors, tape, glue, spray paint\n(D) Tape, scissors, glue, carving knife, spray paint\n(E) Scissors, tape, glue, carving knife, spray paint\n(F) Tape, glue, carving knife, scissors, spray paint", "task_type": "tutorialQA", "pred_answer": "D", "answer": "E"}, "input": "Sequence of tools used in the handcraft process in the video\n(A) Tape, glue, scissors, carving knife, spray paint\n(B) Tape, glue, carving knife, spray paint, scissors\n(C) Carving knife, scissors, tape, glue, spray paint\n(D) Tape, scissors, glue, carving knife, spray paint\n(E) Scissors, tape, glue, carving knife, spray paint\n(F) Tape, glue, carving knife, scissors, spray paint\nOnly give the best option.\nBest option: ("}
{"doc_id": 421, "doc": {"video_name": "test_tutorial_27.mp4", "duration": 833.02, "question": "In the video, what technique does the man use when photographing the teddy bear?\n(A) Exposure compensation\n(B) Changing focal length\n(C) Changing shutter speed\n(D) Lens selection\n(E) Adjusting aperture\n(F) Adjusting ISO sensitivity", "candidates": ["Exposure compensation", "Changing focal length", "Changing shutter speed", "Lens selection", "Adjusting aperture", "Adjusting ISO sensitivity"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_22", "indices": ["138", "897", "1725", "1932", "2645", "3542", "3795", "4485", "5198", "6003", "6624", "7107", "7751", "8326", "9246", "9959", "9982", "10626", "11546", "11983", "12604", "13662", "14053", "14513", "15065", "16077", "16629", "17066", "17986", "18216", "18722", "19596"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "66c1eff507cd582eaa8cb3ce8c1cdda3a6b5d4c366fc1170983c7aae546fcdca", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, what technique does the man use when photographing the teddy bear?\n(A) Exposure compensation\n(B) Changing focal length\n(C) Changing shutter speed\n(D) Lens selection\n(E) Adjusting aperture\n(F) Adjusting ISO sensitivity", "task_type": "tutorialQA", "pred_answer": "F", "answer": "F"}, "input": "In the video, what technique does the man use when photographing the teddy bear?\n(A) Exposure compensation\n(B) Changing focal length\n(C) Changing shutter speed\n(D) Lens selection\n(E) Adjusting aperture\n(F) Adjusting ISO sensitivity\nOnly give the best option.\nBest option: ("}
{"doc_id": 425, "doc": {"video_name": "test_tutorial_13.mp4", "duration": 676.68, "question": "The sequence of actions in the origami process in the video\n(A) Joining the strips, cutting the strip, adjusting the shape of the strip, cutting other strips, fixing the strip, joining the strips\n(B) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, fixing the strip, joining the strips\n(C) Adjusting the shape of the strip, cutting the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(D) Cutting the strip, joining the strips, adjusting the shape of the strip, cutting other strips, joining the strips, fixing the strip\n(E) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(F) Adjusting the shape of the strip, joining the strips, cutting the strip, fixing the strip, cutting other strips, joining the strips", "candidates": ["Joining the strips, cutting the strip, adjusting the shape of the strip, cutting other strips, fixing the strip, joining the strips", "Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, fixing the strip, joining the strips", "Adjusting the shape of the strip, cutting the strip, joining the strips, cutting other strips, joining the strips, fixing the strip", "Cutting the strip, joining the strips, adjusting the shape of the strip, cutting other strips, joining the strips, fixing the strip", "Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, joining the strips, fixing the strip", "Adjusting the shape of the strip, joining the strips, cutting the strip, fixing the strip, cutting other strips, joining the strips"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_26", "indices": ["408", "792", "1032", "1584", "2448", "2712", "3288", "3672", "4416", "4944", "5256", "5688", "6552", "6624", "7512", "7872", "8544", "8856", "9408", "9912", "10536", "10656", "11568", "11832", "12288", "13056", "13368", "14136", "14256", "15096", "15408", "16104"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5c778047eb663f6dcbb40bc26d04302a26d6ae99bc30ec47f630397e0dc0cdc", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "The sequence of actions in the origami process in the video\n(A) Joining the strips, cutting the strip, adjusting the shape of the strip, cutting other strips, fixing the strip, joining the strips\n(B) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, fixing the strip, joining the strips\n(C) Adjusting the shape of the strip, cutting the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(D) Cutting the strip, joining the strips, adjusting the shape of the strip, cutting other strips, joining the strips, fixing the strip\n(E) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(F) Adjusting the shape of the strip, joining the strips, cutting the strip, fixing the strip, cutting other strips, joining the strips", "task_type": "tutorialQA", "pred_answer": "E", "answer": "E"}, "input": "The sequence of actions in the origami process in the video\n(A) Joining the strips, cutting the strip, adjusting the shape of the strip, cutting other strips, fixing the strip, joining the strips\n(B) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, fixing the strip, joining the strips\n(C) Adjusting the shape of the strip, cutting the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(D) Cutting the strip, joining the strips, adjusting the shape of the strip, cutting other strips, joining the strips, fixing the strip\n(E) Cutting the strip, adjusting the shape of the strip, joining the strips, cutting other strips, joining the strips, fixing the strip\n(F) Adjusting the shape of the strip, joining the strips, cutting the strip, fixing the strip, cutting other strips, joining the strips\nOnly give the best option.\nBest option: ("}
{"doc_id": 429, "doc": {"video_name": "test_tutorial_5.mp4", "duration": 1751.34, "question": "What does the man do before frying the food at the end of the video?\n(A) Adjusts the kitchen timer\n(B) Cuts some vegetables\n(C) Cleans the frying pan\n(D) Melts butter\n(E) Sprinkles some pepper on the food\n(F) Turns on the range hood", "candidates": ["Adjusts the kitchen timer", "Cuts some vegetables", "Cleans the frying pan", "Melts butter", "Sprinkles some pepper on the food", "Turns on the range hood"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_30", "indices": ["750", "1450", "2775", "5400", "5450", "7900", "9525", "10875", "11575", "12950", "14850", "16325", "17175", "17850", "20100", "21150", "22500", "24475", "25450", "26675", "27900", "29300", "30925", "31500", "33150", "34300", "36200", "38175", "38450", "40900", "42025", "43075"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0649e15ac3ef5004d89af28c37b044455f4a667fc8a29613f22e085137753178", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What does the man do before frying the food at the end of the video?\n(A) Adjusts the kitchen timer\n(B) Cuts some vegetables\n(C) Cleans the frying pan\n(D) Melts butter\n(E) Sprinkles some pepper on the food\n(F) Turns on the range hood", "task_type": "tutorialQA", "pred_answer": "D", "answer": "D"}, "input": "What does the man do before frying the food at the end of the video?\n(A) Adjusts the kitchen timer\n(B) Cuts some vegetables\n(C) Cleans the frying pan\n(D) Melts butter\n(E) Sprinkles some pepper on the food\n(F) Turns on the range hood\nOnly give the best option.\nBest option: ("}
{"doc_id": 433, "doc": {"video_name": "test_tutorial_24.mp4", "duration": 242.58, "question": "What types of charts are made in the video?\n(A) Scatter plot, box plot, word cloud, bar chart\n(B) Line chart, box plot, word cloud, bar chart\n(C) Scatter plot, line chart, box plot, word cloud\n(D) Scatter plot, line chart, box plot\n(E) Scatter plot, line chart, box plot, word cloud, bar chart\n(F) Scatter plot, line chart, box plot, bar chart", "candidates": ["Scatter plot, box plot, word cloud, bar chart", "Line chart, box plot, word cloud, bar chart", "Scatter plot, line chart, box plot, word cloud", "Scatter plot, line chart, box plot", "Scatter plot, line chart, box plot, word cloud, bar chart", "Scatter plot, line chart, box plot, bar chart"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_34", "indices": ["30", "210", "570", "660", "990", "1260", "1380", "1620", "1830", "2130", "2340", "2640", "2880", "3030", "3240", "3600", "3630", "3930", "4260", "4410", "4590", "4890", "4980", "5280", "5520", "5730", "6060", "6270", "6330", "6750", "6870", "7230"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "065b64e2acc70d97c94157a7fdc769e11ceebdeafdc5f6d3fd27cc309089a24d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What types of charts are made in the video?\n(A) Scatter plot, box plot, word cloud, bar chart\n(B) Line chart, box plot, word cloud, bar chart\n(C) Scatter plot, line chart, box plot, word cloud\n(D) Scatter plot, line chart, box plot\n(E) Scatter plot, line chart, box plot, word cloud, bar chart\n(F) Scatter plot, line chart, box plot, bar chart", "task_type": "tutorialQA", "pred_answer": "D", "answer": "C"}, "input": "What types of charts are made in the video?\n(A) Scatter plot, box plot, word cloud, bar chart\n(B) Line chart, box plot, word cloud, bar chart\n(C) Scatter plot, line chart, box plot, word cloud\n(D) Scatter plot, line chart, box plot\n(E) Scatter plot, line chart, box plot, word cloud, bar chart\n(F) Scatter plot, line chart, box plot, bar chart\nOnly give the best option.\nBest option: ("}
{"doc_id": 437, "doc": {"video_name": "test_tutorial_27.mp4", "duration": 833.02, "question": "In the video, after the man adjusts the aperture of the camera pointed at the lamp on the table, what does he photograph next?\n(A) Flower pot and desk lamp\n(B) Building blocks and Rubik's cube\n(C) Building blocks and flower pot\n(D) Building blocks and desk lamp\n(E) Flower pot and Rubik's cube\n(F) Desk lamp and Rubik's cube", "candidates": ["Flower pot and desk lamp", "Building blocks and Rubik's cube", "Building blocks and flower pot", "Building blocks and desk lamp", "Flower pot and Rubik's cube", "Desk lamp and Rubik's cube"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_38", "indices": ["230", "851", "1748", "1909", "2645", "3565", "4301", "4370", "5106", "6049", "6555", "6969", "7521", "8395", "8970", "9890", "10189", "10626", "11684", "12029", "12949", "13501", "14076", "14513", "15065", "16077", "16307", "16859", "17871", "18676", "18699", "19849"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "907cb7231d05147b1cb5ad128ad5faf01fb8c57477f7011dc566d3ecb281fd0b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, after the man adjusts the aperture of the camera pointed at the lamp on the table, what does he photograph next?\n(A) Flower pot and desk lamp\n(B) Building blocks and Rubik's cube\n(C) Building blocks and flower pot\n(D) Building blocks and desk lamp\n(E) Flower pot and Rubik's cube\n(F) Desk lamp and Rubik's cube", "task_type": "tutorialQA", "pred_answer": "D", "answer": "B"}, "input": "In the video, after the man adjusts the aperture of the camera pointed at the lamp on the table, what does he photograph next?\n(A) Flower pot and desk lamp\n(B) Building blocks and Rubik's cube\n(C) Building blocks and flower pot\n(D) Building blocks and desk lamp\n(E) Flower pot and Rubik's cube\n(F) Desk lamp and Rubik's cube\nOnly give the best option.\nBest option: ("}
{"doc_id": 441, "doc": {"video_name": "test_tutorial_23.mp4", "duration": 1471.75, "question": "What is the main technique used in processing the landscape photo in the video?\n(A) Depth of field adjustment\n(B) Filter application\n(C) Exposure adjustment\n(D) Contrast enhancement\n(E) Cropping and composition\n(F) Saturation adjustment", "candidates": ["Depth of field adjustment", "Filter application", "Exposure adjustment", "Contrast enhancement", "Cropping and composition", "Saturation adjustment"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_42", "indices": ["540", "1440", "3300", "4110", "6720", "7110", "9600", "9630", "11280", "12900", "15030", "15180", "17670", "18510", "20160", "21180", "23460", "23490", "24030", "24060", "24090", "24600", "24720", "24780", "24840", "24870", "24960", "25470", "25500", "25530", "25560", "28590"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Filter application"]], "filtered_resps": ["(B) Filter application"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fd3591323efc3ed0fe9f281abcaae3875176d7f08d1a8788bee42dc9b77d5c7", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main technique used in processing the landscape photo in the video?\n(A) Depth of field adjustment\n(B) Filter application\n(C) Exposure adjustment\n(D) Contrast enhancement\n(E) Cropping and composition\n(F) Saturation adjustment", "task_type": "tutorialQA", "pred_answer": "B", "answer": "B"}, "input": "What is the main technique used in processing the landscape photo in the video?\n(A) Depth of field adjustment\n(B) Filter application\n(C) Exposure adjustment\n(D) Contrast enhancement\n(E) Cropping and composition\n(F) Saturation adjustment\nOnly give the best option.\nBest option: ("}
{"doc_id": 445, "doc": {"video_name": "test_count_44.mp4", "duration": 409.34999999999997, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 3\n(B) 5\n(C) 4\n(D) 2\n(E) 6\n(F) 1", "candidates": ["3", "5", "4", "2", "6", "1"], "answer": "F", "task_type": "count", "question_id": "AC_3", "indices": ["29", "638", "899", "1189", "1682", "2088", "2320", "2784", "3335", "3509", "4002", "4553", "4814", "5075", "5568", "5742", "6148", "6612", "7221", "7250", "7975", "8381", "8613", "8874", "9280", "9628", "10237", "10672", "10759", "11107", "11803", "12093"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac7b7f75350025c04e89d6b39ac2beee884bfb481d18c04b4b8f21f1da98f106", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 3\n(B) 5\n(C) 4\n(D) 2\n(E) 6\n(F) 1", "task_type": "count", "pred_answer": "D", "answer": "F"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'javelin throw' action\n(A) 3\n(B) 5\n(C) 4\n(D) 2\n(E) 6\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 449, "doc": {"video_name": "test_count_2.mp4", "duration": 609.5899999999999, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 4", "candidates": ["1", "6", "3", "5", "0", "4"], "answer": "F", "task_type": "count", "question_id": "AC_7", "indices": ["3780", "3810", "7920", "7950", "7980", "8010", "8040", "8070", "8100", "8130", "8160", "8190", "11910", "11940", "11970", "12000", "12030", "12060", "12090", "12120", "12150", "12180", "15180", "15210", "15240", "15270", "15300", "15330", "15360", "15390", "15420", "15450"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 3"]], "filtered_resps": ["(C) 3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7df6a6d3ae597a866e07d39d57c93c78d689dc7ade39c65d7c7ec759e2cf075f", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 4", "task_type": "count", "pred_answer": "C", "answer": "F"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 453, "doc": {"video_name": "test_count_22.mp4", "duration": 470.02, "question": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 6\n(E) 5\n(F) 2", "candidates": ["3", "0", "1", "6", "5", "2"], "answer": "F", "task_type": "count", "question_id": "AC_11", "indices": ["203", "754", "1218", "1653", "2117", "2262", "2755", "3132", "3654", "4234", "4379", "5017", "5626", "5916", "6438", "6757", "7105", "7714", "8033", "8642", "8903", "9541", "10034", "10179", "10527", "11165", "11716", "12267", "12296", "12963", "13253", "13775"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 1"]], "filtered_resps": ["(C) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93624625e639f33958ef2335e247ec19c481426422c82e9fc26aa1d95de186f9", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 6\n(E) 5\n(F) 2", "task_type": "count", "pred_answer": "C", "answer": "F"}, "input": "In this video, how many instances are there of the 'cleaning toilet' action scene in total?\n(A) 3\n(B) 0\n(C) 1\n(D) 6\n(E) 5\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 457, "doc": {"video_name": "test_count_11.mp4", "duration": 597.9399999999999, "question": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 0\n(E) 5\n(F) 1", "candidates": ["3", "4", "6", "0", "5", "1"], "answer": "A", "task_type": "count", "question_id": "AC_15", "indices": ["360", "1020", "1170", "2040", "2700", "2970", "3660", "4290", "4710", "5400", "5940", "6660", "6690", "7680", "7980", "8640", "9330", "9600", "10200", "11100", "11340", "11760", "12480", "13350", "13440", "14040", "14640", "15300", "15840", "16740", "17250", "17760"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "610d693484e9e2e6c12dc1f87e27d32aad7e3722458d5c09dae4a1ba78415d02", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 0\n(E) 5\n(F) 1", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many instances are there of the 'water sliding' action scene in total?\n(A) 3\n(B) 4\n(C) 6\n(D) 0\n(E) 5\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 461, "doc": {"video_name": "test_count_46.mp4", "duration": 6620.6500000000015, "question": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 5\n(E) 2\n(F) 4", "candidates": ["6", "0", "3", "5", "2", "4"], "answer": "D", "task_type": "count", "question_id": "AC_19", "indices": ["3886", "11194", "17603", "22214", "28130", "34510", "41325", "43384", "51214", "56927", "63481", "70006", "75980", "86101", "89349", "97846", "102689", "105879", "116319", "122641", "127600", "135778", "138156", "148277", "150887", "155788", "165358", "167591", "175334", "183773", "188355", "196591"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 2"]], "filtered_resps": ["(E) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e95e2da7783b44668fb26789cc397feec6b9892500bd39bc52ecb980d9f78d33", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 5\n(E) 2\n(F) 4", "task_type": "count", "pred_answer": "E", "answer": "D"}, "input": "In this video, how many times does the scene of the 'shredding paper' action appear in total?\n(A) 6\n(B) 0\n(C) 3\n(D) 5\n(E) 2\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 465, "doc": {"video_name": "test_count_57.mp4", "duration": 500.03999999999996, "question": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 3\n(E) 6\n(F) 1", "candidates": ["4", "2", "5", "3", "6", "1"], "answer": "C", "task_type": "count", "question_id": "AC_23", "indices": ["150", "690", "1050", "1680", "2280", "2310", "3120", "3540", "3750", "4410", "5100", "5250", "5700", "6390", "6930", "7440", "7830", "8070", "8610", "9210", "9510", "9960", "10560", "10920", "11640", "11700", "12300", "13080", "13260", "13800", "14130", "14730"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 2"]], "filtered_resps": ["(B) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cd372cab0e0931faca90226963a5a798eb856ef3e1c22ca45aaa38b9829a12f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 3\n(E) 6\n(F) 1", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many times does the scene of the 'baking cookies' action appear in total?\n(A) 4\n(B) 2\n(C) 5\n(D) 3\n(E) 6\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 469, "doc": {"video_name": "test_count_7.mp4", "duration": 6606.18, "question": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 1", "candidates": ["2", "3", "6", "4", "0", "1"], "answer": "A", "task_type": "count", "question_id": "AC_27", "indices": ["2520", "9000", "16380", "22740", "26430", "33840", "42600", "45060", "54060", "57240", "65490", "69030", "76260", "83250", "92550", "93270", "99930", "110610", "117570", "117930", "129420", "130290", "141330", "146940", "152400", "157800", "163770", "169740", "173490", "181410", "188370", "192450"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff16d0739c7b97cc0cf5a7c7ae645b2a354ac4307248e20eb5046e783cdc6b1d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 1", "task_type": "count", "pred_answer": "F", "answer": "A"}, "input": "In this video, how many times does the scene of the 'clean and jerk' action appear in total?\n(A) 2\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 473, "doc": {"video_name": "test_count_33.mp4", "duration": 304.03, "question": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n(E) 0\n(F) 2", "candidates": ["1", "6", "3", "4", "0", "2"], "answer": "A", "task_type": "count", "question_id": "AC_31", "indices": ["0", "450", "720", "930", "1140", "1530", "1800", "1980", "2340", "2580", "3060", "3360", "3630", "3840", "4020", "4410", "4800", "4920", "5370", "5670", "5820", "6210", "6330", "6750", "6960", "7170", "7470", "7710", "8130", "8310", "8550", "8850"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "914a967172e120bcccb71bc015a2f7d92215585907f0590ab2775431331cbed8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n(E) 0\n(F) 2", "task_type": "count", "pred_answer": "A", "answer": "A"}, "input": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 4\n(E) 0\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 477, "doc": {"video_name": "test_count_16.mp4", "duration": 499.34, "question": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n(E) 5\n(F) 2", "candidates": ["4", "0", "1", "3", "5", "2"], "answer": "E", "task_type": "count", "question_id": "AC_35", "indices": ["60", "900", "960", "1530", "1980", "2730", "3210", "3270", "4140", "4230", "4830", "5280", "5970", "6360", "6780", "7440", "7800", "8310", "8730", "8880", "9330", "9870", "10260", "11130", "11250", "11790", "12570", "12900", "13350", "13980", "14070", "14850"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) 2"]], "filtered_resps": ["(F) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f131f5716ca04e06ebf0277178934e449eed10ea88efeb6820061a5f92069b6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n(E) 5\n(F) 2", "task_type": "count", "pred_answer": "F", "answer": "E"}, "input": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 4\n(B) 0\n(C) 1\n(D) 3\n(E) 5\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 481, "doc": {"video_name": "test_count_17.mp4", "duration": 452.78000000000003, "question": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 0\n(B) 5\n(C) 3\n(D) 6\n(E) 4\n(F) 2", "candidates": ["0", "5", "3", "6", "4", "2"], "answer": "B", "task_type": "count", "question_id": "AC_39", "indices": ["240", "1080", "1110", "1140", "1170", "1200", "1230", "1260", "1290", "1320", "1350", "2700", "2730", "2760", "2790", "2820", "2850", "2880", "2910", "2940", "2970", "6000", "6030", "11280", "11340", "11370", "11400", "11430", "11460", "11490", "11520", "11550"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 4"]], "filtered_resps": ["(E) 4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "25338f30551ea206d52ec9094c365d124b0d3571eb725b3cedb98b2fc5916b08", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 0\n(B) 5\n(C) 3\n(D) 6\n(E) 4\n(F) 2", "task_type": "count", "pred_answer": "E", "answer": "B"}, "input": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 0\n(B) 5\n(C) 3\n(D) 6\n(E) 4\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 485, "doc": {"video_name": "test_count_23.mp4", "duration": 489.04999999999995, "question": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n(E) 4\n(F) 1", "candidates": ["5", "3", "2", "0", "4", "1"], "answer": "A", "task_type": "count", "question_id": "AC_43", "indices": ["180", "630", "930", "1560", "2130", "2640", "3150", "3240", "3750", "4290", "4980", "5010", "5640", "6270", "6390", "7260", "7740", "7800", "8610", "8670", "9510", "10020", "10350", "10950", "11370", "11550", "11910", "12600", "12900", "13470", "13800", "14640"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ed99667311e06d00eb8ba8bd254cdde0ee5ecf4ca1f6bda8ece603475fa6edd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n(E) 4\n(F) 1", "task_type": "count", "pred_answer": "F", "answer": "A"}, "input": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 5\n(B) 3\n(C) 2\n(D) 0\n(E) 4\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 489, "doc": {"video_name": "test_count_41.mp4", "duration": 490.03, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'making jewelry' action\n(A) 2\n(B) 3\n(C) 1\n(D) 6\n(E) 5\n(F) 4", "candidates": ["2", "3", "1", "6", "5", "4"], "answer": "F", "task_type": "count", "question_id": "AC_47", "indices": ["232", "435", "1044", "1711", "2175", "2262", "2784", "3625", "3770", "4466", "4611", "5162", "5510", "6293", "6409", "7192", "7366", "7946", "8642", "9048", "9570", "9715", "10353", "10730", "11107", "11455", "12035", "12702", "12818", "13601", "13804", "14326"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc8f92f005731f16adfa70f3c103e5542ab06ee82803670ec0483c6040654e63", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'making jewelry' action\n(A) 2\n(B) 3\n(C) 1\n(D) 6\n(E) 5\n(F) 4", "task_type": "count", "pred_answer": "A", "answer": "F"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'making jewelry' action\n(A) 2\n(B) 3\n(C) 1\n(D) 6\n(E) 5\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 493, "doc": {"video_name": "test_count_8.mp4", "duration": 476.77000000000004, "question": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 6\n(C) 4\n(D) 0\n(E) 1\n(F) 5", "candidates": ["3", "6", "4", "0", "1", "5"], "answer": "A", "task_type": "count", "question_id": "AC_51", "indices": ["120", "810", "870", "1740", "1830", "2430", "3000", "3270", "3780", "4380", "4830", "4890", "5490", "6090", "6300", "7080", "7260", "7950", "8370", "8760", "9150", "9780", "9840", "10290", "10890", "11310", "11880", "12210", "12480", "13320", "13710", "14100"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 1"]], "filtered_resps": ["(E) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75386c0806a24e81c69b856243d8c1beb41a4eb25bda89d16efb6f57ff98d492", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 6\n(C) 4\n(D) 0\n(E) 1\n(F) 5", "task_type": "count", "pred_answer": "E", "answer": "A"}, "input": "In this video, how many instances are there of the 'clean and jerk' action scene in total?\n(A) 3\n(B) 6\n(C) 4\n(D) 0\n(E) 1\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 497, "doc": {"video_name": "test_count_40.mp4", "duration": 528.52, "question": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 1\n(D) 2\n(E) 6\n(F) 5", "candidates": ["0", "4", "1", "2", "6", "5"], "answer": "C", "task_type": "count", "question_id": "AC_55", "indices": ["210", "780", "1290", "1830", "2010", "2550", "3090", "3570", "4290", "4860", "5130", "5430", "6270", "6510", "7140", "7770", "8310", "8490", "9300", "9420", "10260", "10500", "11250", "11820", "11880", "12510", "12960", "13800", "14250", "14490", "14850", "15600"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A)"]], "filtered_resps": ["(A)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "04f106db69bbf47ae7a0d49faa3bd6dd8478c3641f2067e3b4a34261be102667", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 1\n(D) 2\n(E) 6\n(F) 5", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many instances are there of the 'making jewelry' action scene in total?\n(A) 0\n(B) 4\n(C) 1\n(D) 2\n(E) 6\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 501, "doc": {"video_name": "test_count_14.mp4", "duration": 426.15, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 3\n(C) 5\n(D) 2\n(E) 0\n(F) 1", "candidates": ["6", "3", "5", "2", "0", "1"], "answer": "C", "task_type": "count", "question_id": "AC_59", "indices": ["0", "600", "1110", "1260", "1710", "2010", "2700", "3150", "3180", "3630", "4260", "4380", "5070", "5220", "5940", "6300", "6540", "6960", "7230", "7680", "8340", "8640", "8760", "9270", "9780", "10020", "10500", "10770", "11460", "11790", "12240", "12660"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18b856bce0948cc45f1fac9e5eb0340c2f7bcbf4723896251c1d3c93ded66a60", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 3\n(C) 5\n(D) 2\n(E) 0\n(F) 1", "task_type": "count", "pred_answer": "D", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 6\n(B) 3\n(C) 5\n(D) 2\n(E) 0\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 2, "doc": {"video_name": "test_surveil_26.mp4", "duration": 269.87, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n(E) Arson\n(F) Assault", "candidates": ["Normal", "Fighting", "RoadAccidents", "Shooting", "Arson", "Assault"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_2", "indices": ["210", "240", "660", "960", "1020", "1260", "1710", "1890", "2220", "2400", "2520", "2820", "3030", "3480", "3630", "3750", "4170", "4470", "4590", "4770", "5130", "5460", "5670", "5790", "6150", "6390", "6750", "6840", "7140", "7440", "7710", "8040"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "32e769147e97b6d611e21b1e7f5499a79936b45c297f916827899b9419eefad2", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n(E) Arson\n(F) Assault", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "F"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Fighting\n(C) RoadAccidents\n(D) Shooting\n(E) Arson\n(F) Assault\nOnly give the best option.\nBest option: ("}
{"doc_id": 6, "doc": {"video_name": "test_surveil_35.mp4", "duration": 387.33, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Abuse\n(C) Arrest\n(D) Vandalism\n(E) Explosion\n(F) Robbery", "candidates": ["Shoplifting", "Abuse", "Arrest", "Vandalism", "Explosion", "Robbery"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_6", "indices": ["30", "660", "1020", "1080", "1470", "1800", "2460", "2610", "3210", "3240", "3870", "4260", "4350", "4680", "5310", "5490", "5940", "6390", "6660", "7200", "7290", "7860", "8100", "8550", "9000", "9240", "9450", "9810", "10260", "10530", "10950", "11250"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Arrest"]], "filtered_resps": ["(C) Arrest"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ad94195b11f8047ef8264f6cbc2523fc0b2bc68dc3ba1dcdfefd62e53224921", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Abuse\n(C) Arrest\n(D) Vandalism\n(E) Explosion\n(F) Robbery", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Shoplifting\n(B) Abuse\n(C) Arrest\n(D) Vandalism\n(E) Explosion\n(F) Robbery\nOnly give the best option.\nBest option: ("}
{"doc_id": 10, "doc": {"video_name": "test_surveil_14.mp4", "duration": 223.87, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Robbery\n(C) RoadAccidents\n(D) Assault\n(E) Shooting\n(F) Explosion", "candidates": ["Abuse", "Robbery", "RoadAccidents", "Assault", "Shooting", "Explosion"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_10", "indices": ["120", "180", "480", "720", "930", "1080", "1230", "1590", "1770", "1890", "2250", "2280", "2610", "2850", "3090", "3270", "3420", "3720", "3900", "4140", "4290", "4530", "4710", "4920", "5160", "5220", "5460", "5730", "5850", "6090", "6360", "6480"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Robbery"]], "filtered_resps": ["(B) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f864c2f14b223831650f27e780ee45980d5ce038fba4c8e9279c33a6e06606c0", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Robbery\n(C) RoadAccidents\n(D) Assault\n(E) Shooting\n(F) Explosion", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Abuse\n(B) Robbery\n(C) RoadAccidents\n(D) Assault\n(E) Shooting\n(F) Explosion\nOnly give the best option.\nBest option: ("}
{"doc_id": 14, "doc": {"video_name": "test_surveil_19.mp4", "duration": 220.13, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Burglary\n(C) Stealing\n(D) Arson\n(E) Vandalism\n(F) Shooting", "candidates": ["Shoplifting", "Burglary", "Stealing", "Arson", "Vandalism", "Shooting"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_14", "indices": ["120", "330", "390", "690", "990", "1170", "1380", "1500", "1710", "1980", "2040", "2430", "2490", "2700", "3060", "3090", "3300", "3570", "3810", "3900", "4290", "4320", "4680", "4860", "5010", "5220", "5490", "5640", "5760", "6030", "6180", "6480"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Burglary"]], "filtered_resps": ["(B) Burglary"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "841567bdbcbd6819543c3406149d4b19d78e37c75f53890f34beafe81b5f99bd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Burglary\n(C) Stealing\n(D) Arson\n(E) Vandalism\n(F) Shooting", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Shoplifting\n(B) Burglary\n(C) Stealing\n(D) Arson\n(E) Vandalism\n(F) Shooting\nOnly give the best option.\nBest option: ("}
{"doc_id": 18, "doc": {"video_name": "test_surveil_38.mp4", "duration": 435.79, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Fighting\n(D) Shooting\n(E) Abuse\n(F) Assault", "candidates": ["Stealing", "Vandalism", "Fighting", "Shooting", "Abuse", "Assault"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_18", "indices": ["210", "570", "990", "1230", "1680", "2250", "2610", "3030", "3390", "3780", "4290", "4590", "5100", "5610", "6000", "6390", "6510", "7110", "7620", "7980", "8190", "8820", "9060", "9570", "10140", "10200", "10770", "11190", "11760", "11880", "12330", "12840"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce3af689086c437f55a0c8bc092dd1833a0ea0ca5b9365ba0dcb018373c6bc74", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Fighting\n(D) Shooting\n(E) Abuse\n(F) Assault", "task_type": "anomaly_reco", "pred_answer": "A", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Stealing\n(B) Vandalism\n(C) Fighting\n(D) Shooting\n(E) Abuse\n(F) Assault\nOnly give the best option.\nBest option: ("}
{"doc_id": 22, "doc": {"video_name": "test_surveil_34.mp4", "duration": 360.0, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Abuse\n(D) Explosion\n(E) Shooting\n(F) Burglary", "candidates": ["Fighting", "Arrest", "Abuse", "Explosion", "Shooting", "Burglary"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_22", "indices": ["0", "600", "750", "1050", "1410", "1920", "2310", "2340", "3000", "3030", "3600", "3720", "4050", "4500", "4800", "5100", "5520", "5970", "6330", "6690", "6780", "7200", "7680", "7890", "8130", "8520", "9000", "9180", "9600", "10080", "10170", "10440"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Shooting"]], "filtered_resps": ["(E) Shooting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ed38d338e90843826273e0800770d15848a3d13730cdeede54489e16bcfd18bc", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Abuse\n(D) Explosion\n(E) Shooting\n(F) Burglary", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "B"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Fighting\n(B) Arrest\n(C) Abuse\n(D) Explosion\n(E) Shooting\n(F) Burglary\nOnly give the best option.\nBest option: ("}
{"doc_id": 26, "doc": {"video_name": "test_surveil_5.mp4", "duration": 341.07, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) Arrest\n(C) Robbery\n(D) RoadAccidents\n(E) Arson\n(F) Burglary", "candidates": ["Shoplifting", "Arrest", "Robbery", "RoadAccidents", "Arson", "Burglary"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_26", "indices": ["270", "300", "720", "1170", "1290", "1620", "2130", "2370", "2700", "2910", "3180", "3600", "4020", "4170", "4500", "5010", "5340", "5400", "5880", "6150", "6480", "6900", "6990", "7320", "7890", "8160", "8340", "8760", "9180", "9360", "9660", "10170"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Robbery"]], "filtered_resps": ["(C) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b62a6169a3ade7cfb8ff38530e6a42a2e7c3fe58d6195eb8e4c4fbfef675eb6a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) Arrest\n(C) Robbery\n(D) RoadAccidents\n(E) Arson\n(F) Burglary", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "E"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Shoplifting\n(B) Arrest\n(C) Robbery\n(D) RoadAccidents\n(E) Arson\n(F) Burglary\nOnly give the best option.\nBest option: ("}
{"doc_id": 30, "doc": {"video_name": "test_surveil_13.mp4", "duration": 274.1, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Robbery\n(C) Vandalism\n(D) Abuse\n(E) Shooting\n(F) Fighting", "candidates": ["Arson", "Robbery", "Vandalism", "Abuse", "Shooting", "Fighting"], "answer": "B", "task_type": "anomaly_reco", "question_id": "AR_30", "indices": ["180", "420", "540", "870", "1170", "1320", "1650", "1860", "2130", "2280", "2550", "3000", "3180", "3360", "3600", "3840", "4260", "4410", "4740", "4890", "5280", "5370", "5760", "6030", "6330", "6600", "6750", "7050", "7380", "7440", "7920", "7950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Robbery"]], "filtered_resps": ["(B) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df5637604f822486db53db2fde342890d0b555d135146ef5adb1d5a99e039f81", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Robbery\n(C) Vandalism\n(D) Abuse\n(E) Shooting\n(F) Fighting", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "B"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Arson\n(B) Robbery\n(C) Vandalism\n(D) Abuse\n(E) Shooting\n(F) Fighting\nOnly give the best option.\nBest option: ("}
{"doc_id": 34, "doc": {"video_name": "test_surveil_32.mp4", "duration": 187.37, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Shooting\n(C) Normal\n(D) Explosion\n(E) Arson\n(F) Assault", "candidates": ["RoadAccidents", "Shooting", "Normal", "Explosion", "Arson", "Assault"], "answer": "A", "task_type": "anomaly_reco", "question_id": "AR_34", "indices": ["0", "300", "450", "630", "780", "960", "1020", "1350", "1440", "1560", "1800", "1980", "2160", "2280", "2550", "2730", "2880", "2970", "3210", "3450", "3480", "3810", "3840", "4080", "4200", "4470", "4560", "4800", "4980", "5220", "5250", "5550"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Normal"]], "filtered_resps": ["(C) Normal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f27877248cd8c38e01413299196826d289344d393dc1faf8a1674fde0e28526", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Shooting\n(C) Normal\n(D) Explosion\n(E) Arson\n(F) Assault", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "A"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) RoadAccidents\n(B) Shooting\n(C) Normal\n(D) Explosion\n(E) Arson\n(F) Assault\nOnly give the best option.\nBest option: ("}
{"doc_id": 38, "doc": {"video_name": "test_surveil_0.mp4", "duration": 239.49, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Arrest\n(C) Burglary\n(D) Vandalism\n(E) Fighting\n(F) Shoplifting", "candidates": ["Robbery", "Arrest", "Burglary", "Vandalism", "Fighting", "Shoplifting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_38", "indices": ["0", "360", "600", "780", "870", "1080", "1440", "1530", "1830", "2010", "2400", "2430", "2850", "2880", "3210", "3390", "3750", "3780", "4110", "4290", "4620", "4680", "5040", "5160", "5490", "5730", "6000", "6150", "6300", "6570", "6900", "7050"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Vandalism"]], "filtered_resps": ["(D) Vandalism"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba95bd3dba9b7d842d1e053b01c5fffae436cf21655264e4ebbc547f15eee356", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Arrest\n(C) Burglary\n(D) Vandalism\n(E) Fighting\n(F) Shoplifting", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Robbery\n(B) Arrest\n(C) Burglary\n(D) Vandalism\n(E) Fighting\n(F) Shoplifting\nOnly give the best option.\nBest option: ("}
{"doc_id": 42, "doc": {"video_name": "test_sports_30.mp4", "duration": 5787.76, "question": "What is the event in the video?\n(A) Women's synchronized diving\n(B) Individual diving\n(C) Swimming\n(D) Men's synchronized diving\n(E) Swimming relay race\n(F) Synchronized swimming", "candidates": ["Women's synchronized diving", "Individual diving", "Swimming", "Men's synchronized diving", "Swimming relay race", "Synchronized swimming"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_3", "indices": ["3675", "7050", "10200", "17900", "22075", "27050", "29675", "32200", "36550", "41375", "49000", "49925", "55075", "62625", "67600", "69475", "75175", "79700", "83700", "88275", "92500", "95925", "101450", "106500", "109225", "115325", "118525", "125325", "127125", "134575", "138125", "144150"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28d49987af551bb5838a3a3f21fd3ebeb0a6b9893281a7537a386cc9da21949d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the event in the video?\n(A) Women's synchronized diving\n(B) Individual diving\n(C) Swimming\n(D) Men's synchronized diving\n(E) Swimming relay race\n(F) Synchronized swimming", "task_type": "sportsQA", "pred_answer": "D", "answer": "D"}, "input": "What is the event in the video?\n(A) Women's synchronized diving\n(B) Individual diving\n(C) Swimming\n(D) Men's synchronized diving\n(E) Swimming relay race\n(F) Synchronized swimming\nOnly give the best option.\nBest option: ("}
{"doc_id": 46, "doc": {"video_name": "test_sports_9.mp4", "duration": 303.11, "question": "Why didn't the player in white jersey number 13 score?\n(A) The football was blocked by the player in red team green jersey number 16\n(B) The player in white jersey number 13 fell down\n(C) The player in white jersey number 13 was offside\n(D) The player in red jersey number 15 blocked the offensive of the player in white jersey number 13\n(E) The player in white jersey number 13 committed a technical foul\n(F) The player in white jersey number 13 kicked the ball over", "candidates": ["The football was blocked by the player in red team green jersey number 16", "The player in white jersey number 13 fell down", "The player in white jersey number 13 was offside", "The player in red jersey number 15 blocked the offensive of the player in white jersey number 13", "The player in white jersey number 13 committed a technical foul", "The player in white jersey number 13 kicked the ball over"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_7", "indices": ["0", "522", "696", "841", "1276", "1508", "1856", "2117", "2262", "2552", "3074", "3103", "3567", "3654", "4089", "4379", "4756", "4901", "5220", "5481", "5771", "6119", "6322", "6728", "6960", "7163", "7395", "7685", "8062", "8236", "8584", "8961"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) The player in red jersey number 15 blocked the offensive of the"]], "filtered_resps": ["(D) The player in red jersey number 15 blocked the offensive of the"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd56f6f27e3a2f2b36416eb2ea66d9554bcd1b3ea8c15b97bda1c8df6671df95", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why didn't the player in white jersey number 13 score?\n(A) The football was blocked by the player in red team green jersey number 16\n(B) The player in white jersey number 13 fell down\n(C) The player in white jersey number 13 was offside\n(D) The player in red jersey number 15 blocked the offensive of the player in white jersey number 13\n(E) The player in white jersey number 13 committed a technical foul\n(F) The player in white jersey number 13 kicked the ball over", "task_type": "sportsQA", "pred_answer": "D", "answer": "A"}, "input": "Why didn't the player in white jersey number 13 score?\n(A) The football was blocked by the player in red team green jersey number 16\n(B) The player in white jersey number 13 fell down\n(C) The player in white jersey number 13 was offside\n(D) The player in red jersey number 15 blocked the offensive of the player in white jersey number 13\n(E) The player in white jersey number 13 committed a technical foul\n(F) The player in white jersey number 13 kicked the ball over\nOnly give the best option.\nBest option: ("}
{"doc_id": 50, "doc": {"video_name": "test_sports_3.mp4", "duration": 389.75, "question": "What action did the first player of the red team make when entering the field?\n(A) He accelerated his steps and then jumped\n(B) He clenched his fist or clapped\n(C) He showed his jersey number\n(D) He pointed to the ground\n(E) He performed a prayer or ritual\n(F) He waved to the crowd", "candidates": ["He accelerated his steps and then jumped", "He clenched his fist or clapped", "He showed his jersey number", "He pointed to the ground", "He performed a prayer or ritual", "He waved to the crowd"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_11", "indices": ["540", "720", "2100", "2220", "3120", "3900", "4320", "5460", "6120", "7200", "7560", "8580", "8880", "9780", "10680", "11400", "12300", "12960", "13440", "14280", "14520", "15660", "16080", "17400", "17880", "18180", "19140", "19680", "20760", "21600", "22380", "22980"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) He clenched his fist or clapped"]], "filtered_resps": ["(B) He clenched his fist or clapped"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33263894389d11713f9d9785c60a45b84791aa729bd3e1ee76b44039a49e90ba", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What action did the first player of the red team make when entering the field?\n(A) He accelerated his steps and then jumped\n(B) He clenched his fist or clapped\n(C) He showed his jersey number\n(D) He pointed to the ground\n(E) He performed a prayer or ritual\n(F) He waved to the crowd", "task_type": "sportsQA", "pred_answer": "B", "answer": "A"}, "input": "What action did the first player of the red team make when entering the field?\n(A) He accelerated his steps and then jumped\n(B) He clenched his fist or clapped\n(C) He showed his jersey number\n(D) He pointed to the ground\n(E) He performed a prayer or ritual\n(F) He waved to the crowd\nOnly give the best option.\nBest option: ("}
{"doc_id": 54, "doc": {"video_name": "test_sports_14.mp4", "duration": 597.91, "question": "How did player number 7 from the blue team score in the first quarter?\n(A) 360-degree dunk\n(B) Step-back three-pointer\n(C) Hook shot\n(D) Mid-range shot from the free-throw line\n(E) Step-back jump shot\n(F) Layup with a dunk under the legs", "candidates": ["360-degree dunk", "Step-back three-pointer", "Hook shot", "Mid-range shot from the free-throw line", "Step-back jump shot", "Layup with a dunk under the legs"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_15", "indices": ["885", "1416", "2419", "3953", "5074", "6608", "7670", "7847", "9440", "10738", "11977", "13334", "14042", "14986", "16461", "17523", "18113", "19175", "20886", "21771", "23305", "23777", "24957", "26255", "27612", "28969", "29500", "30916", "32214", "32627", "33571", "34869"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Step-back jump shot"]], "filtered_resps": ["(E) Step-back jump shot"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c00df1a3481644bd4935099949a00b672b969a117227f923865f6ce206795e3", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "How did player number 7 from the blue team score in the first quarter?\n(A) 360-degree dunk\n(B) Step-back three-pointer\n(C) Hook shot\n(D) Mid-range shot from the free-throw line\n(E) Step-back jump shot\n(F) Layup with a dunk under the legs", "task_type": "sportsQA", "pred_answer": "E", "answer": "B"}, "input": "How did player number 7 from the blue team score in the first quarter?\n(A) 360-degree dunk\n(B) Step-back three-pointer\n(C) Hook shot\n(D) Mid-range shot from the free-throw line\n(E) Step-back jump shot\n(F) Layup with a dunk under the legs\nOnly give the best option.\nBest option: ("}
{"doc_id": 58, "doc": {"video_name": "test_sports_39.mp4", "duration": 2822.29, "question": "How does the player in pink score the second point in the video?\n(A) The player in black committed a foul\n(B) The player in pink scored with a diagonal shot\n(C) The player in pink scored with a smash\n(D) The player in black hit the shuttlecock into the net\n(E) The player in black missed the shuttlecock\n(F) The player in pink scored with a flat high shot", "candidates": ["The player in black committed a foul", "The player in pink scored with a diagonal shot", "The player in pink scored with a smash", "The player in black hit the shuttlecock into the net", "The player in black missed the shuttlecock", "The player in pink scored with a flat high shot"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_19", "indices": ["1675", "3700", "5050", "6675", "9275", "12750", "14750", "15450", "19075", "21650", "22225", "25350", "26950", "30650", "32975", "33100", "36175", "39600", "41225", "43850", "46250", "46700", "48775", "52500", "54725", "55450", "59200", "60000", "63125", "64175", "66250", "70200"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) The player in pink scored with a diagonal shot"]], "filtered_resps": ["(B) The player in pink scored with a diagonal shot"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2f706d6d7a1fccb75e89bae00bc97fd0b0cda72862f77cb40d18b01eb111ecf7", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How does the player in pink score the second point in the video?\n(A) The player in black committed a foul\n(B) The player in pink scored with a diagonal shot\n(C) The player in pink scored with a smash\n(D) The player in black hit the shuttlecock into the net\n(E) The player in black missed the shuttlecock\n(F) The player in pink scored with a flat high shot", "task_type": "sportsQA", "pred_answer": "B", "answer": "D"}, "input": "How does the player in pink score the second point in the video?\n(A) The player in black committed a foul\n(B) The player in pink scored with a diagonal shot\n(C) The player in pink scored with a smash\n(D) The player in black hit the shuttlecock into the net\n(E) The player in black missed the shuttlecock\n(F) The player in pink scored with a flat high shot\nOnly give the best option.\nBest option: ("}
{"doc_id": 62, "doc": {"video_name": "test_sports_18.mp4", "duration": 5469.8, "question": "What is the player from the red team who is preparing for the jump ball doing?\n(A) Communicating with teammates\n(B) Checking equipment\n(C) Positioning himself\n(D) Communicating with the referee\n(E) Stretching\n(F) Observing the opponent", "candidates": ["Communicating with teammates", "Checking equipment", "Positioning himself", "Communicating with the referee", "Stretching", "Observing the opponent"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_23", "indices": ["400", "7100", "9875", "16975", "18400", "21350", "28525", "31275", "36175", "40275", "43950", "50600", "54800", "58925", "62200", "65950", "69850", "76300", "77900", "82300", "85425", "91675", "96650", "98325", "103975", "107250", "111550", "115725", "122650", "127275", "130375", "133450"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Positioning himself"]], "filtered_resps": ["(C) Positioning himself"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83a7689f8168cf898f1f075371f50e4b74760373da475e17d4240212c9a1753a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the player from the red team who is preparing for the jump ball doing?\n(A) Communicating with teammates\n(B) Checking equipment\n(C) Positioning himself\n(D) Communicating with the referee\n(E) Stretching\n(F) Observing the opponent", "task_type": "sportsQA", "pred_answer": "C", "answer": "E"}, "input": "What is the player from the red team who is preparing for the jump ball doing?\n(A) Communicating with teammates\n(B) Checking equipment\n(C) Positioning himself\n(D) Communicating with the referee\n(E) Stretching\n(F) Observing the opponent\nOnly give the best option.\nBest option: ("}
{"doc_id": 66, "doc": {"video_name": "test_sports_12.mp4", "duration": 633.74, "question": "How did the player number 23 from the yellow team score at 10 minutes and 33 seconds in the second quarter?\n(A) Hook shot\n(B) One-handed shot over the shoulder\n(C) Put-back\n(D) Layup with a swing\n(E) Faked left and right, then performed a fadeaway jump shot\n(F) Windmill dunk", "candidates": ["Hook shot", "One-handed shot over the shoulder", "Put-back", "Layup with a swing", "Faked left and right, then performed a fadeaway jump shot", "Windmill dunk"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_27", "indices": ["348", "696", "1450", "2001", "2610", "3132", "3538", "4263", "4756", "5365", "6351", "7076", "7395", "8178", "8700", "9425", "9512", "10063", "11136", "11803", "11861", "12702", "13427", "14152", "14384", "14819", "15921", "16327", "16936", "17429", "18038", "18618"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Faked left and right, then performed a fadeaway jump shot"]], "filtered_resps": ["(E) Faked left and right, then performed a fadeaway jump shot"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b0dde64f21fac600e7afee8db377a3de13ebb7b9bf90e48856726a3b2dd4231", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How did the player number 23 from the yellow team score at 10 minutes and 33 seconds in the second quarter?\n(A) Hook shot\n(B) One-handed shot over the shoulder\n(C) Put-back\n(D) Layup with a swing\n(E) Faked left and right, then performed a fadeaway jump shot\n(F) Windmill dunk", "task_type": "sportsQA", "pred_answer": "E", "answer": "E"}, "input": "How did the player number 23 from the yellow team score at 10 minutes and 33 seconds in the second quarter?\n(A) Hook shot\n(B) One-handed shot over the shoulder\n(C) Put-back\n(D) Layup with a swing\n(E) Faked left and right, then performed a fadeaway jump shot\n(F) Windmill dunk\nOnly give the best option.\nBest option: ("}
{"doc_id": 70, "doc": {"video_name": "test_sports_6.mp4", "duration": 477.8, "question": "What was the audience's reaction after the player in blue jersey scored the first goal?\n(A) Silence\n(B) Sighing\n(C) Cheering and drumming\n(D) Making praying gestures\n(E) Taking photos to share\n(F) High-fiving each other", "candidates": ["Silence", "Sighing", "Cheering and drumming", "Making praying gestures", "Taking photos to share", "High-fiving each other"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_31", "indices": ["450", "1100", "2150", "2300", "3350", "4250", "5000", "5400", "5950", "7200", "7600", "8200", "9600", "9750", "11100", "11200", "12150", "13300", "13700", "14300", "15550", "16300", "16350", "17650", "18000", "18950", "19800", "20250", "21550", "22050", "22400", "23450"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e65eea61f643b89d7068807cd585faa3bd241d1503135f042106c44614ea6c92", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What was the audience's reaction after the player in blue jersey scored the first goal?\n(A) Silence\n(B) Sighing\n(C) Cheering and drumming\n(D) Making praying gestures\n(E) Taking photos to share\n(F) High-fiving each other", "task_type": "sportsQA", "pred_answer": "C", "answer": "C"}, "input": "What was the audience's reaction after the player in blue jersey scored the first goal?\n(A) Silence\n(B) Sighing\n(C) Cheering and drumming\n(D) Making praying gestures\n(E) Taking photos to share\n(F) High-fiving each other\nOnly give the best option.\nBest option: ("}
{"doc_id": 74, "doc": {"video_name": "test_sports_7.mp4", "duration": 750.82, "question": "Where did the player in the red jersey number 7 hit the player in the blue and white stripe jersey number 5?\n(A) Arm\n(B) Back\n(C) Head\n(D) Thigh\n(E) Face\n(F) Chest", "candidates": ["Arm", "Back", "Head", "Thigh", "Face", "Chest"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_35", "indices": ["1180", "2301", "4130", "4248", "6431", "8201", "9027", "10797", "12095", "13275", "14514", "15989", "18113", "18703", "19765", "22243", "23246", "25016", "26550", "26786", "28733", "30385", "31388", "33512", "33807", "36167", "37111", "38704", "39707", "41890", "42598", "44191"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07385d91751ec75c20d561cbe1fd7b654d4232cf7627d7e0f4f528990294ec6e", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where did the player in the red jersey number 7 hit the player in the blue and white stripe jersey number 5?\n(A) Arm\n(B) Back\n(C) Head\n(D) Thigh\n(E) Face\n(F) Chest", "task_type": "sportsQA", "pred_answer": "E", "answer": "F"}, "input": "Where did the player in the red jersey number 7 hit the player in the blue and white stripe jersey number 5?\n(A) Arm\n(B) Back\n(C) Head\n(D) Thigh\n(E) Face\n(F) Chest\nOnly give the best option.\nBest option: ("}
{"doc_id": 78, "doc": {"video_name": "test_order_7.mp4", "duration": 489.32, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> jetskiing --> milking cow --> shredding paper\n(B) milking cow --> jetskiing --> carving pumpkin --> shredding paper\n(C) milking cow --> carving pumpkin --> shredding paper --> jetskiing\n(D) carving pumpkin --> milking cow --> shredding paper --> jetskiing\n(E) carving pumpkin --> jetskiing --> shredding paper --> milking cow\n(F) jetskiing --> milking cow --> shredding paper --> carving pumpkin", "candidates": ["carving pumpkin --> jetskiing --> milking cow --> shredding paper", "milking cow --> jetskiing --> carving pumpkin --> shredding paper", "milking cow --> carving pumpkin --> shredding paper --> jetskiing", "carving pumpkin --> milking cow --> shredding paper --> jetskiing", "carving pumpkin --> jetskiing --> shredding paper --> milking cow", "jetskiing --> milking cow --> shredding paper --> carving pumpkin"], "answer": "B", "task_type": "order", "question_id": "AO_3", "indices": ["180", "660", "1200", "1350", "2130", "2520", "3090", "3570", "3930", "4260", "4980", "5010", "5670", "5970", "6540", "7200", "7530", "7920", "8250", "8670", "9570", "9810", "10170", "10500", "11370", "11640", "12060", "12510", "13020", "13410", "14160", "14190"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7b053ee515529b9634b15688cdde3442812bffa4aa210b0c716080f9769dd64b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> jetskiing --> milking cow --> shredding paper\n(B) milking cow --> jetskiing --> carving pumpkin --> shredding paper\n(C) milking cow --> carving pumpkin --> shredding paper --> jetskiing\n(D) carving pumpkin --> milking cow --> shredding paper --> jetskiing\n(E) carving pumpkin --> jetskiing --> shredding paper --> milking cow\n(F) jetskiing --> milking cow --> shredding paper --> carving pumpkin", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) carving pumpkin --> jetskiing --> milking cow --> shredding paper\n(B) milking cow --> jetskiing --> carving pumpkin --> shredding paper\n(C) milking cow --> carving pumpkin --> shredding paper --> jetskiing\n(D) carving pumpkin --> milking cow --> shredding paper --> jetskiing\n(E) carving pumpkin --> jetskiing --> shredding paper --> milking cow\n(F) jetskiing --> milking cow --> shredding paper --> carving pumpkin\nOnly give the best option.\nBest option: ("}
{"doc_id": 82, "doc": {"video_name": "test_order_36.mp4", "duration": 393.01, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> stomping grapes --> zumba\n(B) stomping grapes --> carving pumpkin --> water sliding --> zumba\n(C) stomping grapes --> zumba --> carving pumpkin --> water sliding\n(D) carving pumpkin --> zumba --> stomping grapes --> water sliding\n(E) water sliding --> stomping grapes --> carving pumpkin --> zumba\n(F) water sliding --> zumba --> carving pumpkin --> stomping grapes", "candidates": ["carving pumpkin --> water sliding --> stomping grapes --> zumba", "stomping grapes --> carving pumpkin --> water sliding --> zumba", "stomping grapes --> zumba --> carving pumpkin --> water sliding", "carving pumpkin --> zumba --> stomping grapes --> water sliding", "water sliding --> stomping grapes --> carving pumpkin --> zumba", "water sliding --> zumba --> carving pumpkin --> stomping grapes"], "answer": "C", "task_type": "order", "question_id": "AO_7", "indices": ["0", "630", "960", "1080", "1530", "2070", "2370", "2670", "3000", "3630", "3990", "4290", "4500", "5100", "5190", "5790", "6150", "6540", "6720", "7170", "7410", "7890", "8400", "8790", "8940", "9510", "9540", "10110", "10560", "11010", "11040", "11580"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) stomping grapes --> zumba --> carving pumpkin --> water sliding"]], "filtered_resps": ["(C) stomping grapes --> zumba --> carving pumpkin --> water sliding"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "09ed050d5d63ddcaf62b505874cb586fbb689beea1a4ba67e581e150f19bd4e6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> stomping grapes --> zumba\n(B) stomping grapes --> carving pumpkin --> water sliding --> zumba\n(C) stomping grapes --> zumba --> carving pumpkin --> water sliding\n(D) carving pumpkin --> zumba --> stomping grapes --> water sliding\n(E) water sliding --> stomping grapes --> carving pumpkin --> zumba\n(F) water sliding --> zumba --> carving pumpkin --> stomping grapes", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> stomping grapes --> zumba\n(B) stomping grapes --> carving pumpkin --> water sliding --> zumba\n(C) stomping grapes --> zumba --> carving pumpkin --> water sliding\n(D) carving pumpkin --> zumba --> stomping grapes --> water sliding\n(E) water sliding --> stomping grapes --> carving pumpkin --> zumba\n(F) water sliding --> zumba --> carving pumpkin --> stomping grapes\nOnly give the best option.\nBest option: ("}
{"doc_id": 86, "doc": {"video_name": "test_order_1.mp4", "duration": 284.03, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> stomping grapes --> clean and jerk --> cleaning toilet\n(B) stomping grapes --> clean and jerk --> cleaning toilet --> water sliding\n(C) water sliding --> cleaning toilet --> stomping grapes --> clean and jerk\n(D) water sliding --> cleaning toilet --> clean and jerk --> stomping grapes\n(E) stomping grapes --> cleaning toilet --> clean and jerk --> water sliding\n(F) stomping grapes --> water sliding --> clean and jerk --> cleaning toilet", "candidates": ["water sliding --> stomping grapes --> clean and jerk --> cleaning toilet", "stomping grapes --> clean and jerk --> cleaning toilet --> water sliding", "water sliding --> cleaning toilet --> stomping grapes --> clean and jerk", "water sliding --> cleaning toilet --> clean and jerk --> stomping grapes", "stomping grapes --> cleaning toilet --> clean and jerk --> water sliding", "stomping grapes --> water sliding --> clean and jerk --> cleaning toilet"], "answer": "B", "task_type": "order", "question_id": "AO_11", "indices": ["180", "450", "660", "1020", "1230", "1380", "1800", "1950", "2280", "2400", "2820", "3090", "3180", "3690", "3780", "4230", "4260", "4650", "4800", "5070", "5490", "5670", "6090", "6150", "6480", "6630", "6960", "7230", "7470", "7800", "8160", "8400"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) water sliding --> cleaning toilet --> stomping grapes --> clean and jerk"]], "filtered_resps": ["(C) water sliding --> cleaning toilet --> stomping grapes --> clean and jerk"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "55152888eaa0615c7c76e510d4633f544d06c775cbcdabeb0c0d6440c1493477", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> stomping grapes --> clean and jerk --> cleaning toilet\n(B) stomping grapes --> clean and jerk --> cleaning toilet --> water sliding\n(C) water sliding --> cleaning toilet --> stomping grapes --> clean and jerk\n(D) water sliding --> cleaning toilet --> clean and jerk --> stomping grapes\n(E) stomping grapes --> cleaning toilet --> clean and jerk --> water sliding\n(F) stomping grapes --> water sliding --> clean and jerk --> cleaning toilet", "task_type": "order", "pred_answer": "C", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> stomping grapes --> clean and jerk --> cleaning toilet\n(B) stomping grapes --> clean and jerk --> cleaning toilet --> water sliding\n(C) water sliding --> cleaning toilet --> stomping grapes --> clean and jerk\n(D) water sliding --> cleaning toilet --> clean and jerk --> stomping grapes\n(E) stomping grapes --> cleaning toilet --> clean and jerk --> water sliding\n(F) stomping grapes --> water sliding --> clean and jerk --> cleaning toilet\nOnly give the best option.\nBest option: ("}
{"doc_id": 90, "doc": {"video_name": "test_order_26.mp4", "duration": 7551.700000000002, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> carving pumpkin --> water sliding --> paragliding\n(B) water sliding --> paragliding --> carving pumpkin --> abseiling\n(C) carving pumpkin --> water sliding --> abseiling --> paragliding\n(D) paragliding --> carving pumpkin --> water sliding --> abseiling\n(E) paragliding --> abseiling --> water sliding --> carving pumpkin\n(F) paragliding --> carving pumpkin --> abseiling --> water sliding", "candidates": ["abseiling --> carving pumpkin --> water sliding --> paragliding", "water sliding --> paragliding --> carving pumpkin --> abseiling", "carving pumpkin --> water sliding --> abseiling --> paragliding", "paragliding --> carving pumpkin --> water sliding --> abseiling", "paragliding --> abseiling --> water sliding --> carving pumpkin", "paragliding --> carving pumpkin --> abseiling --> water sliding"], "answer": "A", "task_type": "order", "question_id": "AO_15", "indices": ["6030", "12450", "17580", "28110", "28800", "41460", "46800", "51030", "60090", "67980", "76110", "82770", "90360", "93120", "105240", "106290", "115800", "124710", "134190", "139170", "143910", "149490", "160350", "167220", "174330", "178620", "188670", "194100", "203070", "207780", "219210", "225960"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e6d53e6d716b3536e1ddc8be00df3300647927373d41eaa1c7a7ef4de3ef0df", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> carving pumpkin --> water sliding --> paragliding\n(B) water sliding --> paragliding --> carving pumpkin --> abseiling\n(C) carving pumpkin --> water sliding --> abseiling --> paragliding\n(D) paragliding --> carving pumpkin --> water sliding --> abseiling\n(E) paragliding --> abseiling --> water sliding --> carving pumpkin\n(F) paragliding --> carving pumpkin --> abseiling --> water sliding", "task_type": "order", "pred_answer": "E", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) abseiling --> carving pumpkin --> water sliding --> paragliding\n(B) water sliding --> paragliding --> carving pumpkin --> abseiling\n(C) carving pumpkin --> water sliding --> abseiling --> paragliding\n(D) paragliding --> carving pumpkin --> water sliding --> abseiling\n(E) paragliding --> abseiling --> water sliding --> carving pumpkin\n(F) paragliding --> carving pumpkin --> abseiling --> water sliding\nOnly give the best option.\nBest option: ("}
{"doc_id": 94, "doc": {"video_name": "test_order_47.mp4", "duration": 596.05, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> riding mule --> cleaning toilet --> baking cookies\n(B) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(C) riding mule --> cleaning toilet --> javelin throw --> baking cookies\n(D) javelin throw --> baking cookies --> cleaning toilet --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) riding mule --> baking cookies --> cleaning toilet --> javelin throw", "candidates": ["javelin throw --> riding mule --> cleaning toilet --> baking cookies", "javelin throw --> cleaning toilet --> baking cookies --> riding mule", "riding mule --> cleaning toilet --> javelin throw --> baking cookies", "javelin throw --> baking cookies --> cleaning toilet --> riding mule", "baking cookies --> javelin throw --> riding mule --> cleaning toilet", "riding mule --> baking cookies --> cleaning toilet --> javelin throw"], "answer": "A", "task_type": "order", "question_id": "AO_19", "indices": ["210", "960", "1440", "1920", "2340", "3150", "3420", "3930", "4770", "5310", "5670", "6420", "7050", "7350", "8010", "8370", "9390", "10020", "10080", "10650", "11220", "12030", "12450", "13320", "13560", "14250", "14940", "15240", "15870", "16500", "16860", "17700"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["The correct answer is not provided in the options given, as the sequence of events"]], "filtered_resps": ["The correct answer is not provided in the options given, as the sequence of events"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b227fed22813660d96527244cd0d1566334a8b759cca9dfc9db98d42cdc7dc2a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> riding mule --> cleaning toilet --> baking cookies\n(B) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(C) riding mule --> cleaning toilet --> javelin throw --> baking cookies\n(D) javelin throw --> baking cookies --> cleaning toilet --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) riding mule --> baking cookies --> cleaning toilet --> javelin throw", "task_type": "order", "pred_answer": "The correct answer is not provided in the options given, as the sequence of events", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> riding mule --> cleaning toilet --> baking cookies\n(B) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(C) riding mule --> cleaning toilet --> javelin throw --> baking cookies\n(D) javelin throw --> baking cookies --> cleaning toilet --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) riding mule --> baking cookies --> cleaning toilet --> javelin throw\nOnly give the best option.\nBest option: ("}
{"doc_id": 98, "doc": {"video_name": "test_order_38.mp4", "duration": 447.16999999999996, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> javelin throw --> cooking sausages --> stomping grapes\n(B) cooking sausages --> javelin throw --> riding mule --> stomping grapes\n(C) javelin throw --> cooking sausages --> riding mule --> stomping grapes\n(D) javelin throw --> cooking sausages --> stomping grapes --> riding mule\n(E) javelin throw --> stomping grapes --> riding mule --> cooking sausages\n(F) stomping grapes --> javelin throw --> riding mule --> cooking sausages", "candidates": ["riding mule --> javelin throw --> cooking sausages --> stomping grapes", "cooking sausages --> javelin throw --> riding mule --> stomping grapes", "javelin throw --> cooking sausages --> riding mule --> stomping grapes", "javelin throw --> cooking sausages --> stomping grapes --> riding mule", "javelin throw --> stomping grapes --> riding mule --> cooking sausages", "stomping grapes --> javelin throw --> riding mule --> cooking sausages"], "answer": "C", "task_type": "order", "question_id": "AO_23", "indices": ["210", "540", "870", "1590", "1920", "2070", "2700", "2940", "3630", "4080", "4290", "4710", "5280", "5820", "6030", "6420", "6840", "7470", "7800", "8340", "8460", "8970", "9300", "9690", "10110", "10860", "10950", "11520", "11970", "12390", "12630", "13380"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of actions shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of actions shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3cabdf3b760981ab73784efe3cf9ef1793da932f7b711cadabd4a23f988eae8e", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> javelin throw --> cooking sausages --> stomping grapes\n(B) cooking sausages --> javelin throw --> riding mule --> stomping grapes\n(C) javelin throw --> cooking sausages --> riding mule --> stomping grapes\n(D) javelin throw --> cooking sausages --> stomping grapes --> riding mule\n(E) javelin throw --> stomping grapes --> riding mule --> cooking sausages\n(F) stomping grapes --> javelin throw --> riding mule --> cooking sausages", "task_type": "order", "pred_answer": "None of the options provided match the sequence of actions shown in the video. The", "answer": "C"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) riding mule --> javelin throw --> cooking sausages --> stomping grapes\n(B) cooking sausages --> javelin throw --> riding mule --> stomping grapes\n(C) javelin throw --> cooking sausages --> riding mule --> stomping grapes\n(D) javelin throw --> cooking sausages --> stomping grapes --> riding mule\n(E) javelin throw --> stomping grapes --> riding mule --> cooking sausages\n(F) stomping grapes --> javelin throw --> riding mule --> cooking sausages\nOnly give the best option.\nBest option: ("}
{"doc_id": 102, "doc": {"video_name": "test_order_14.mp4", "duration": 630.5699999999999, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> riding mule --> jetskiing --> cooking sausages\n(B) jetskiing --> riding mule --> water sliding --> cooking sausages\n(C) riding mule --> water sliding --> jetskiing --> cooking sausages\n(D) riding mule --> cooking sausages --> jetskiing --> water sliding\n(E) water sliding --> jetskiing --> riding mule --> cooking sausages\n(F) cooking sausages --> jetskiing --> riding mule --> water sliding", "candidates": ["water sliding --> riding mule --> jetskiing --> cooking sausages", "jetskiing --> riding mule --> water sliding --> cooking sausages", "riding mule --> water sliding --> jetskiing --> cooking sausages", "riding mule --> cooking sausages --> jetskiing --> water sliding", "water sliding --> jetskiing --> riding mule --> cooking sausages", "cooking sausages --> jetskiing --> riding mule --> water sliding"], "answer": "F", "task_type": "order", "question_id": "AO_27", "indices": ["290", "1015", "1276", "1914", "2349", "3219", "3770", "4292", "4756", "5278", "6409", "6989", "7279", "8207", "8468", "9106", "9570", "10556", "11049", "11629", "12325", "12702", "13456", "13630", "14703", "15225", "15341", "15892", "16936", "17081", "17719", "18473"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) water sliding --> jetskiing --> riding mule --> cooking saus"]], "filtered_resps": ["(E) water sliding --> jetskiing --> riding mule --> cooking saus"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af5278b5da0ec5389a15dc7bcbc7b75c415a5f1bc7aeb768ed0eaaf6d5a88fce", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> riding mule --> jetskiing --> cooking sausages\n(B) jetskiing --> riding mule --> water sliding --> cooking sausages\n(C) riding mule --> water sliding --> jetskiing --> cooking sausages\n(D) riding mule --> cooking sausages --> jetskiing --> water sliding\n(E) water sliding --> jetskiing --> riding mule --> cooking sausages\n(F) cooking sausages --> jetskiing --> riding mule --> water sliding", "task_type": "order", "pred_answer": "E", "answer": "F"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> riding mule --> jetskiing --> cooking sausages\n(B) jetskiing --> riding mule --> water sliding --> cooking sausages\n(C) riding mule --> water sliding --> jetskiing --> cooking sausages\n(D) riding mule --> cooking sausages --> jetskiing --> water sliding\n(E) water sliding --> jetskiing --> riding mule --> cooking sausages\n(F) cooking sausages --> jetskiing --> riding mule --> water sliding\nOnly give the best option.\nBest option: ("}
{"doc_id": 106, "doc": {"video_name": "test_order_45.mp4", "duration": 489.09999999999997, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> tossing coin --> clean and jerk\n(B) water sliding --> clean and jerk --> carving pumpkin --> tossing coin\n(C) tossing coin --> water sliding --> clean and jerk --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> tossing coin --> water sliding\n(E) carving pumpkin --> tossing coin --> water sliding --> clean and jerk\n(F) tossing coin --> clean and jerk --> water sliding --> carving pumpkin", "candidates": ["carving pumpkin --> water sliding --> tossing coin --> clean and jerk", "water sliding --> clean and jerk --> carving pumpkin --> tossing coin", "tossing coin --> water sliding --> clean and jerk --> carving pumpkin", "carving pumpkin --> clean and jerk --> tossing coin --> water sliding", "carving pumpkin --> tossing coin --> water sliding --> clean and jerk", "tossing coin --> clean and jerk --> water sliding --> carving pumpkin"], "answer": "E", "task_type": "order", "question_id": "AO_31", "indices": ["240", "600", "1320", "1440", "1860", "2700", "3120", "3240", "3840", "4230", "4680", "5070", "5490", "5940", "6780", "7110", "7380", "7770", "8580", "8790", "9420", "9720", "10440", "10500", "11040", "11730", "12240", "12420", "13230", "13440", "13830", "14580"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events described in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events described in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ef6c72f67c7b473a49462ba3febc033833bb61b28352a768d6e3ae5e94d3b4a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> tossing coin --> clean and jerk\n(B) water sliding --> clean and jerk --> carving pumpkin --> tossing coin\n(C) tossing coin --> water sliding --> clean and jerk --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> tossing coin --> water sliding\n(E) carving pumpkin --> tossing coin --> water sliding --> clean and jerk\n(F) tossing coin --> clean and jerk --> water sliding --> carving pumpkin", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events described in the video. The", "answer": "E"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> water sliding --> tossing coin --> clean and jerk\n(B) water sliding --> clean and jerk --> carving pumpkin --> tossing coin\n(C) tossing coin --> water sliding --> clean and jerk --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> tossing coin --> water sliding\n(E) carving pumpkin --> tossing coin --> water sliding --> clean and jerk\n(F) tossing coin --> clean and jerk --> water sliding --> carving pumpkin\nOnly give the best option.\nBest option: ("}
{"doc_id": 110, "doc": {"video_name": "test_order_2.mp4", "duration": 661.38, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> pole vault --> jetskiing --> cleaning toilet\n(B) jetskiing --> cleaning toilet --> javelin throw --> pole vault\n(C) pole vault --> javelin throw --> jetskiing --> cleaning toilet\n(D) javelin throw --> jetskiing --> cleaning toilet --> pole vault\n(E) javelin throw --> cleaning toilet --> jetskiing --> pole vault\n(F) cleaning toilet --> javelin throw --> jetskiing --> pole vault", "candidates": ["javelin throw --> pole vault --> jetskiing --> cleaning toilet", "jetskiing --> cleaning toilet --> javelin throw --> pole vault", "pole vault --> javelin throw --> jetskiing --> cleaning toilet", "javelin throw --> jetskiing --> cleaning toilet --> pole vault", "javelin throw --> cleaning toilet --> jetskiing --> pole vault", "cleaning toilet --> javelin throw --> jetskiing --> pole vault"], "answer": "B", "task_type": "order", "question_id": "AO_35", "indices": ["580", "928", "1653", "2117", "2668", "3161", "4060", "4814", "5452", "5916", "6264", "7337", "7569", "8410", "9135", "9686", "10440", "10527", "11455", "11745", "12441", "13050", "13949", "14645", "15167", "15979", "16124", "16820", "17690", "18212", "18879", "19169"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D)"]], "filtered_resps": ["(D)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6556771958fa779f69280fc18ea7bd6217b7249f4e76ce309fcf41b3d1301de2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> pole vault --> jetskiing --> cleaning toilet\n(B) jetskiing --> cleaning toilet --> javelin throw --> pole vault\n(C) pole vault --> javelin throw --> jetskiing --> cleaning toilet\n(D) javelin throw --> jetskiing --> cleaning toilet --> pole vault\n(E) javelin throw --> cleaning toilet --> jetskiing --> pole vault\n(F) cleaning toilet --> javelin throw --> jetskiing --> pole vault", "task_type": "order", "pred_answer": "D", "answer": "B"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> pole vault --> jetskiing --> cleaning toilet\n(B) jetskiing --> cleaning toilet --> javelin throw --> pole vault\n(C) pole vault --> javelin throw --> jetskiing --> cleaning toilet\n(D) javelin throw --> jetskiing --> cleaning toilet --> pole vault\n(E) javelin throw --> cleaning toilet --> jetskiing --> pole vault\n(F) cleaning toilet --> javelin throw --> jetskiing --> pole vault\nOnly give the best option.\nBest option: ("}
{"doc_id": 114, "doc": {"video_name": "test_order_30.mp4", "duration": 440.62, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> pole vault --> clean and jerk --> stomping grapes\n(B) stomping grapes --> pole vault --> carving pumpkin --> clean and jerk\n(C) pole vault --> clean and jerk --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> stomping grapes --> pole vault\n(E) clean and jerk --> stomping grapes --> carving pumpkin --> pole vault\n(F) stomping grapes --> carving pumpkin --> pole vault --> clean and jerk", "candidates": ["carving pumpkin --> pole vault --> clean and jerk --> stomping grapes", "stomping grapes --> pole vault --> carving pumpkin --> clean and jerk", "pole vault --> clean and jerk --> stomping grapes --> carving pumpkin", "carving pumpkin --> clean and jerk --> stomping grapes --> pole vault", "clean and jerk --> stomping grapes --> carving pumpkin --> pole vault", "stomping grapes --> carving pumpkin --> pole vault --> clean and jerk"], "answer": "D", "task_type": "order", "question_id": "AO_39", "indices": ["30", "750", "810", "1440", "1650", "2340", "2730", "3270", "3420", "3900", "4350", "4830", "5310", "5700", "5910", "6540", "6870", "7200", "7800", "7950", "8490", "8970", "9120", "9630", "9900", "10680", "11070", "11130", "11670", "12210", "12480", "13170"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) stomping grapes --> carving pumpkin --> pole vault --> clean and jerk"]], "filtered_resps": ["(F) stomping grapes --> carving pumpkin --> pole vault --> clean and jerk"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f55f638e801c56e9ae8dff5db63f0ff18297d651ab43fac5a1b7e85371861cc6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> pole vault --> clean and jerk --> stomping grapes\n(B) stomping grapes --> pole vault --> carving pumpkin --> clean and jerk\n(C) pole vault --> clean and jerk --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> stomping grapes --> pole vault\n(E) clean and jerk --> stomping grapes --> carving pumpkin --> pole vault\n(F) stomping grapes --> carving pumpkin --> pole vault --> clean and jerk", "task_type": "order", "pred_answer": "F", "answer": "D"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) carving pumpkin --> pole vault --> clean and jerk --> stomping grapes\n(B) stomping grapes --> pole vault --> carving pumpkin --> clean and jerk\n(C) pole vault --> clean and jerk --> stomping grapes --> carving pumpkin\n(D) carving pumpkin --> clean and jerk --> stomping grapes --> pole vault\n(E) clean and jerk --> stomping grapes --> carving pumpkin --> pole vault\n(F) stomping grapes --> carving pumpkin --> pole vault --> clean and jerk\nOnly give the best option.\nBest option: ("}
{"doc_id": 118, "doc": {"video_name": "test_order_20.mp4", "duration": 521.4000000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> milking cow --> javelin throw --> clean and jerk\n(B) javelin throw --> milking cow --> clean and jerk --> water sliding\n(C) clean and jerk --> javelin throw --> milking cow --> water sliding\n(D) milking cow --> water sliding --> clean and jerk --> javelin throw\n(E) milking cow --> clean and jerk --> water sliding --> javelin throw\n(F) water sliding --> milking cow --> clean and jerk --> javelin throw", "candidates": ["water sliding --> milking cow --> javelin throw --> clean and jerk", "javelin throw --> milking cow --> clean and jerk --> water sliding", "clean and jerk --> javelin throw --> milking cow --> water sliding", "milking cow --> water sliding --> clean and jerk --> javelin throw", "milking cow --> clean and jerk --> water sliding --> javelin throw", "water sliding --> milking cow --> clean and jerk --> javelin throw"], "answer": "C", "task_type": "order", "question_id": "AO_43", "indices": ["390", "780", "1350", "1830", "2370", "2670", "3210", "3690", "4350", "4440", "5310", "5820", "6090", "6330", "7260", "7560", "8010", "8430", "9150", "9540", "9990", "10680", "10890", "11670", "12030", "12450", "13110", "13350", "14040", "14490", "14700", "15270"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) clean and jerk --> javelin throw --> milking cow --> water sliding"]], "filtered_resps": ["(C) clean and jerk --> javelin throw --> milking cow --> water sliding"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "214fd171c30e3e0fa611d45a9d143c154045ef14297fbfabdfb5fa41069a68c0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> milking cow --> javelin throw --> clean and jerk\n(B) javelin throw --> milking cow --> clean and jerk --> water sliding\n(C) clean and jerk --> javelin throw --> milking cow --> water sliding\n(D) milking cow --> water sliding --> clean and jerk --> javelin throw\n(E) milking cow --> clean and jerk --> water sliding --> javelin throw\n(F) water sliding --> milking cow --> clean and jerk --> javelin throw", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> milking cow --> javelin throw --> clean and jerk\n(B) javelin throw --> milking cow --> clean and jerk --> water sliding\n(C) clean and jerk --> javelin throw --> milking cow --> water sliding\n(D) milking cow --> water sliding --> clean and jerk --> javelin throw\n(E) milking cow --> clean and jerk --> water sliding --> javelin throw\n(F) water sliding --> milking cow --> clean and jerk --> javelin throw\nOnly give the best option.\nBest option: ("}
{"doc_id": 122, "doc": {"video_name": "test_order_19.mp4", "duration": 463.58, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> cleaning toilet --> tossing coin --> javelin throw\n(C) javelin throw --> tossing coin --> cleaning toilet --> baking cookies\n(D) baking cookies --> javelin throw --> cleaning toilet --> tossing coin\n(E) tossing coin --> javelin throw --> baking cookies --> cleaning toilet\n(F) cleaning toilet --> baking cookies --> javelin throw --> tossing coin", "candidates": ["tossing coin --> javelin throw --> cleaning toilet --> baking cookies", "baking cookies --> cleaning toilet --> tossing coin --> javelin throw", "javelin throw --> tossing coin --> cleaning toilet --> baking cookies", "baking cookies --> javelin throw --> cleaning toilet --> tossing coin", "tossing coin --> javelin throw --> baking cookies --> cleaning toilet", "cleaning toilet --> baking cookies --> javelin throw --> tossing coin"], "answer": "C", "task_type": "order", "question_id": "AO_47", "indices": ["30", "600", "1110", "1260", "1830", "2220", "2670", "3420", "3450", "4260", "4350", "4830", "5430", "5610", "6270", "6870", "6960", "7770", "7800", "8460", "8730", "9330", "9690", "9990", "10500", "11040", "11610", "12030", "12450", "12750", "13230", "13650"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) baking cookies --> cleaning toilet --> tossing coin --> javelin throw"]], "filtered_resps": ["(B) baking cookies --> cleaning toilet --> tossing coin --> javelin throw"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1c01a5fd2e7e8eb7d183f1b6ab621058893f2a8ababf52a7d18873f93145a4c", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> cleaning toilet --> tossing coin --> javelin throw\n(C) javelin throw --> tossing coin --> cleaning toilet --> baking cookies\n(D) baking cookies --> javelin throw --> cleaning toilet --> tossing coin\n(E) tossing coin --> javelin throw --> baking cookies --> cleaning toilet\n(F) cleaning toilet --> baking cookies --> javelin throw --> tossing coin", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) tossing coin --> javelin throw --> cleaning toilet --> baking cookies\n(B) baking cookies --> cleaning toilet --> tossing coin --> javelin throw\n(C) javelin throw --> tossing coin --> cleaning toilet --> baking cookies\n(D) baking cookies --> javelin throw --> cleaning toilet --> tossing coin\n(E) tossing coin --> javelin throw --> baking cookies --> cleaning toilet\n(F) cleaning toilet --> baking cookies --> javelin throw --> tossing coin\nOnly give the best option.\nBest option: ("}
{"doc_id": 126, "doc": {"video_name": "test_order_48.mp4", "duration": 564.22, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> tossing coin --> paragliding --> abseiling\n(B) abseiling --> tossing coin --> making jewelry --> paragliding\n(C) abseiling --> paragliding --> tossing coin --> making jewelry\n(D) paragliding --> abseiling --> making jewelry --> tossing coin\n(E) making jewelry --> abseiling --> paragliding --> tossing coin\n(F) paragliding --> abseiling --> tossing coin --> making jewelry", "candidates": ["making jewelry --> tossing coin --> paragliding --> abseiling", "abseiling --> tossing coin --> making jewelry --> paragliding", "abseiling --> paragliding --> tossing coin --> making jewelry", "paragliding --> abseiling --> making jewelry --> tossing coin", "making jewelry --> abseiling --> paragliding --> tossing coin", "paragliding --> abseiling --> tossing coin --> making jewelry"], "answer": "F", "task_type": "order", "question_id": "AO_51", "indices": ["377", "841", "1479", "1798", "2320", "2958", "3509", "3828", "4640", "4843", "5742", "5829", "6438", "7076", "7859", "7917", "8874", "9251", "9744", "10266", "10788", "11281", "12093", "12325", "13050", "13369", "13891", "14442", "15283", "15805", "16298", "16501"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4f206a272f199707f3e9b7c9777a8ba5916eb7bd67590146384c55aafbf45e4d", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> tossing coin --> paragliding --> abseiling\n(B) abseiling --> tossing coin --> making jewelry --> paragliding\n(C) abseiling --> paragliding --> tossing coin --> making jewelry\n(D) paragliding --> abseiling --> making jewelry --> tossing coin\n(E) making jewelry --> abseiling --> paragliding --> tossing coin\n(F) paragliding --> abseiling --> tossing coin --> making jewelry", "task_type": "order", "pred_answer": "E", "answer": "F"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) making jewelry --> tossing coin --> paragliding --> abseiling\n(B) abseiling --> tossing coin --> making jewelry --> paragliding\n(C) abseiling --> paragliding --> tossing coin --> making jewelry\n(D) paragliding --> abseiling --> making jewelry --> tossing coin\n(E) making jewelry --> abseiling --> paragliding --> tossing coin\n(F) paragliding --> abseiling --> tossing coin --> making jewelry\nOnly give the best option.\nBest option: ("}
{"doc_id": 130, "doc": {"video_name": "test_order_4.mp4", "duration": 626.98, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> playing trombone --> playing harp --> carving pumpkin\n(B) carving pumpkin --> playing trombone --> playing harp --> javelin throw\n(C) carving pumpkin --> javelin throw --> playing harp --> playing trombone\n(D) carving pumpkin --> playing trombone --> javelin throw --> playing harp\n(E) playing trombone --> carving pumpkin --> javelin throw --> playing harp\n(F) playing harp --> javelin throw --> playing trombone --> carving pumpkin", "candidates": ["javelin throw --> playing trombone --> playing harp --> carving pumpkin", "carving pumpkin --> playing trombone --> playing harp --> javelin throw", "carving pumpkin --> javelin throw --> playing harp --> playing trombone", "carving pumpkin --> playing trombone --> javelin throw --> playing harp", "playing trombone --> carving pumpkin --> javelin throw --> playing harp", "playing harp --> javelin throw --> playing trombone --> carving pumpkin"], "answer": "C", "task_type": "order", "question_id": "AO_55", "indices": ["120", "750", "1500", "1890", "2460", "3480", "4050", "4650", "4680", "5820", "6150", "6930", "7320", "8130", "8730", "8970", "9840", "10170", "10650", "11310", "11910", "12720", "12930", "13950", "14130", "14850", "15690", "15840", "16890", "17250", "17760", "18780"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) carving pumpkin --> playing trombone --> playing harp --> javelin throw"]], "filtered_resps": ["(B) carving pumpkin --> playing trombone --> playing harp --> javelin throw"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad3c8a0414073b656033a09debcd5437d234d6d6f3e0119c0940f5610be998fa", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> playing trombone --> playing harp --> carving pumpkin\n(B) carving pumpkin --> playing trombone --> playing harp --> javelin throw\n(C) carving pumpkin --> javelin throw --> playing harp --> playing trombone\n(D) carving pumpkin --> playing trombone --> javelin throw --> playing harp\n(E) playing trombone --> carving pumpkin --> javelin throw --> playing harp\n(F) playing harp --> javelin throw --> playing trombone --> carving pumpkin", "task_type": "order", "pred_answer": "B", "answer": "C"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) javelin throw --> playing trombone --> playing harp --> carving pumpkin\n(B) carving pumpkin --> playing trombone --> playing harp --> javelin throw\n(C) carving pumpkin --> javelin throw --> playing harp --> playing trombone\n(D) carving pumpkin --> playing trombone --> javelin throw --> playing harp\n(E) playing trombone --> carving pumpkin --> javelin throw --> playing harp\n(F) playing harp --> javelin throw --> playing trombone --> carving pumpkin\nOnly give the best option.\nBest option: ("}
{"doc_id": 134, "doc": {"video_name": "test_order_8.mp4", "duration": 764.06, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> baking cookies --> riding mule --> cleaning toilet\n(B) riding mule --> baking cookies --> cleaning toilet --> javelin throw\n(C) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(D) cleaning toilet --> baking cookies --> javelin throw --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) baking cookies --> javelin throw --> cleaning toilet --> riding mule", "candidates": ["javelin throw --> baking cookies --> riding mule --> cleaning toilet", "riding mule --> baking cookies --> cleaning toilet --> javelin throw", "javelin throw --> cleaning toilet --> baking cookies --> riding mule", "cleaning toilet --> baking cookies --> javelin throw --> riding mule", "baking cookies --> javelin throw --> riding mule --> cleaning toilet", "baking cookies --> javelin throw --> cleaning toilet --> riding mule"], "answer": "F", "task_type": "order", "question_id": "AO_59", "indices": ["240", "1380", "1920", "2730", "3540", "4140", "4440", "5070", "6360", "6420", "7170", "7860", "9180", "9840", "10680", "11310", "11760", "12750", "13320", "13920", "14940", "15600", "16410", "17040", "17460", "18030", "18780", "19980", "20730", "21330", "21780", "22890"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b92ea3d5e7a0d558f5432b756370e17903c03141479ca417f98514810188bc4f", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> baking cookies --> riding mule --> cleaning toilet\n(B) riding mule --> baking cookies --> cleaning toilet --> javelin throw\n(C) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(D) cleaning toilet --> baking cookies --> javelin throw --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) baking cookies --> javelin throw --> cleaning toilet --> riding mule", "task_type": "order", "pred_answer": "C", "answer": "F"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) javelin throw --> baking cookies --> riding mule --> cleaning toilet\n(B) riding mule --> baking cookies --> cleaning toilet --> javelin throw\n(C) javelin throw --> cleaning toilet --> baking cookies --> riding mule\n(D) cleaning toilet --> baking cookies --> javelin throw --> riding mule\n(E) baking cookies --> javelin throw --> riding mule --> cleaning toilet\n(F) baking cookies --> javelin throw --> cleaning toilet --> riding mule\nOnly give the best option.\nBest option: ("}
{"doc_id": 138, "doc": {"video_name": "test_order_55.mp4", "duration": 886.54, "question": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->1->4->2\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->4->1->2\n(F) 2->3->4->1", "candidates": ["3->1->4->2", "2->1->4->3", "1->2->3->4", "4->3->2->1", "3->4->1->2", "2->3->4->1"], "answer": "B", "task_type": "order", "question_id": "AO_63", "indices": ["17139", "17197", "17255", "17313", "17342", "17603", "17690", "19401", "19430", "19459", "19488", "19517", "19546", "19778", "19807", "20213", "20300", "20329", "20358", "21228", "21431", "21460", "21866", "21895", "21924", "21953", "21982", "22040", "22069", "22243", "22272", "22301"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 2->1->4->3"]], "filtered_resps": ["(B) 2->1->4->3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f5d9124a33d94a3628701a10212fe616931b76b9fd3211116ef0d01a1b93b70", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->1->4->2\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->4->1->2\n(F) 2->3->4->1", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man explains wakeboarding concepts while his daughter wakeboards in a lake; (2)The video introduction about teaching a child to wakeboard is shown; (3)The girl wakeboards in the lake again while her father continues to explain the teaching techniques; (4)They practice wakeboarding in a pool while discussing techniques.\n(A) 3->1->4->2\n(B) 2->1->4->3\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 3->4->1->2\n(F) 2->3->4->1\nOnly give the best option.\nBest option: ("}
{"doc_id": 142, "doc": {"video_name": "test_order_17.mp4", "duration": 6796.250000000001, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> pole vault --> jetskiing --> playing harp\n(B) water sliding --> playing harp --> pole vault --> jetskiing\n(C) playing harp --> water sliding --> pole vault --> jetskiing\n(D) pole vault --> jetskiing --> playing harp --> water sliding\n(E) playing harp --> jetskiing --> pole vault --> water sliding\n(F) jetskiing --> pole vault --> water sliding --> playing harp", "candidates": ["water sliding --> pole vault --> jetskiing --> playing harp", "water sliding --> playing harp --> pole vault --> jetskiing", "playing harp --> water sliding --> pole vault --> jetskiing", "pole vault --> jetskiing --> playing harp --> water sliding", "playing harp --> jetskiing --> pole vault --> water sliding", "jetskiing --> pole vault --> water sliding --> playing harp"], "answer": "C", "task_type": "order", "question_id": "AO_67", "indices": ["1560", "11670", "14460", "19530", "30960", "37350", "41100", "49800", "53280", "61020", "66870", "70950", "78870", "83340", "92730", "100920", "106920", "111960", "118020", "124740", "128610", "139320", "146190", "150600", "156480", "164130", "170970", "175350", "179880", "187710", "194130", "202740"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the given options match the sequence of actions as they actually appear in the"]], "filtered_resps": ["None of the given options match the sequence of actions as they actually appear in the"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28156a5ace44efe81f4b135cba5d4d4af9279b9b6b673078d74e0111679223d8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> pole vault --> jetskiing --> playing harp\n(B) water sliding --> playing harp --> pole vault --> jetskiing\n(C) playing harp --> water sliding --> pole vault --> jetskiing\n(D) pole vault --> jetskiing --> playing harp --> water sliding\n(E) playing harp --> jetskiing --> pole vault --> water sliding\n(F) jetskiing --> pole vault --> water sliding --> playing harp", "task_type": "order", "pred_answer": "None of the given options match the sequence of actions as they actually appear in the", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) water sliding --> pole vault --> jetskiing --> playing harp\n(B) water sliding --> playing harp --> pole vault --> jetskiing\n(C) playing harp --> water sliding --> pole vault --> jetskiing\n(D) pole vault --> jetskiing --> playing harp --> water sliding\n(E) playing harp --> jetskiing --> pole vault --> water sliding\n(F) jetskiing --> pole vault --> water sliding --> playing harp\nOnly give the best option.\nBest option: ("}
{"doc_id": 146, "doc": {"video_name": "test_needle_33.mp4", "duration": 463.01, "question": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Drawing\n(B) Writing\n(C) Drinking coffee\n(D) Eating\n(E) Talking\n(F) Playing with a phone", "candidates": ["Drawing", "Writing", "Drinking coffee", "Eating", "Talking", "Playing with a phone"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_1", "indices": ["0", "609", "1102", "1595", "1711", "2320", "2900", "3393", "3741", "3973", "4698", "5162", "5307", "5742", "6322", "6699", "7047", "7424", "8062", "8526", "8932", "9483", "9512", "10237", "10730", "10875", "11571", "11745", "12296", "12905", "13398", "13659"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "32fb8ef997a497680236751cc11b19723b8e849b1e1713b460f297e182e6cf86", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Drawing\n(B) Writing\n(C) Drinking coffee\n(D) Eating\n(E) Talking\n(F) Playing with a phone", "task_type": "needleQA", "pred_answer": "E", "answer": "C"}, "input": "In the video, what are the man in a suit and the woman in a shirt doing while sitting outdoors in front of a table?\n(A) Drawing\n(B) Writing\n(C) Drinking coffee\n(D) Eating\n(E) Talking\n(F) Playing with a phone\nOnly give the best option.\nBest option: ("}
{"doc_id": 150, "doc": {"video_name": "test_needle_51.mp4", "duration": 472.14, "question": "In the collision event of objects in the video, if the rubber cylinder is removed, which of the following will not happen?\n(A) The rubber cylinder reappears\n(B) The cube and the brown cylinder collide\n(C) The cube changes direction abruptly\n(D) The cube moves in a straight line\n(E) The brown cylinder remains stationary\n(F) The brown cylinder changes color", "candidates": ["The rubber cylinder reappears", "The cube and the brown cylinder collide", "The cube changes direction abruptly", "The cube moves in a straight line", "The brown cylinder remains stationary", "The brown cylinder changes color"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_5", "indices": ["225", "550", "825", "1125", "1675", "1850", "2225", "2875", "2950", "3350", "3950", "4225", "4725", "4850", "5425", "5525", "5900", "6525", "6950", "7275", "7375", "7850", "8200", "8825", "8850", "9500", "9900", "10225", "10375", "10750", "11250", "11575"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) The brown cylinder remains stationary"]], "filtered_resps": ["(E) The brown cylinder remains stationary"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f456b4d91017e51ff67f7098cc976159f9247087582eaa40dfcec9056798fb2", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, if the rubber cylinder is removed, which of the following will not happen?\n(A) The rubber cylinder reappears\n(B) The cube and the brown cylinder collide\n(C) The cube changes direction abruptly\n(D) The cube moves in a straight line\n(E) The brown cylinder remains stationary\n(F) The brown cylinder changes color", "task_type": "needleQA", "pred_answer": "E", "answer": "B"}, "input": "In the collision event of objects in the video, if the rubber cylinder is removed, which of the following will not happen?\n(A) The rubber cylinder reappears\n(B) The cube and the brown cylinder collide\n(C) The cube changes direction abruptly\n(D) The cube moves in a straight line\n(E) The brown cylinder remains stationary\n(F) The brown cylinder changes color\nOnly give the best option.\nBest option: ("}
{"doc_id": 154, "doc": {"video_name": "test_needle_1.mp4", "duration": 502.79, "question": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n(E) 1\n(F) 6", "candidates": ["5", "3", "2", "4", "1", "6"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_9", "indices": ["232", "841", "1247", "1798", "1856", "2349", "2871", "3480", "3770", "4379", "4669", "5510", "5684", "6090", "6786", "7250", "7830", "8323", "8468", "9280", "9541", "10121", "10440", "10933", "11542", "12180", "12412", "12992", "13166", "13920", "14181", "14674"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "838ef81a515226d07f8e1c96967d0311868ee952174d77955c1908aba97f9268", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n(E) 1\n(F) 6", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 4\n(E) 1\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 158, "doc": {"video_name": "test_needle_7.mp4", "duration": 760.07, "question": "What are the two women wearing black vests and black pants doing in the video?\n(A) Dancing\n(B) Using a mobile phone\n(C) Playing on the computer\n(D) Singing\n(E) Playing football\n(F) Arguing", "candidates": ["Dancing", "Using a mobile phone", "Playing on the computer", "Singing", "Playing football", "Arguing"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_13", "indices": ["290", "1131", "1711", "2378", "3132", "3538", "4466", "5481", "5771", "6380", "7105", "8381", "8613", "9367", "10556", "10817", "11803", "12702", "12789", "13572", "14587", "15486", "16037", "16617", "17255", "17893", "18676", "19285", "20213", "21025", "21982", "22156"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0609f64db5f8c348ef47191cacb05d315f29206ce2b788b0e03697abef3a099", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What are the two women wearing black vests and black pants doing in the video?\n(A) Dancing\n(B) Using a mobile phone\n(C) Playing on the computer\n(D) Singing\n(E) Playing football\n(F) Arguing", "task_type": "needleQA", "pred_answer": "A", "answer": "A"}, "input": "What are the two women wearing black vests and black pants doing in the video?\n(A) Dancing\n(B) Using a mobile phone\n(C) Playing on the computer\n(D) Singing\n(E) Playing football\n(F) Arguing\nOnly give the best option.\nBest option: ("}
{"doc_id": 162, "doc": {"video_name": "test_needle_8.mp4", "duration": 457.02, "question": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) VEGETABLE\n(C) FRESH\n(D) SWEET\n(E) FOOD\n(F) RICE", "candidates": ["MEAT", "VEGETABLE", "FRESH", "SWEET", "FOOD", "RICE"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_17", "indices": ["150", "375", "800", "1325", "1750", "1825", "2375", "2675", "3075", "3450", "3725", "4250", "4275", "4925", "5300", "5350", "5700", "6200", "6400", "6775", "7375", "7525", "8050", "8525", "8675", "8975", "9525", "9950", "10175", "10675", "10850", "11125"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85b5e4b77262943ac850d85838f45fbca976e203cbe1edc823db689726f2abdf", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) VEGETABLE\n(C) FRESH\n(D) SWEET\n(E) FOOD\n(F) RICE", "task_type": "needleQA", "pred_answer": "B", "answer": "E"}, "input": "In the video, what word is spelled out with various vegetables on the flat pan?\n(A) MEAT\n(B) VEGETABLE\n(C) FRESH\n(D) SWEET\n(E) FOOD\n(F) RICE\nOnly give the best option.\nBest option: ("}
{"doc_id": 166, "doc": {"video_name": "test_needle_36.mp4", "duration": 632.6700000000001, "question": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Mobile phone\n(B) Tablet computer\n(C) Apple\n(D) Beverage\n(E) Paper materials\n(F) Laptop", "candidates": ["Mobile phone", "Tablet computer", "Apple", "Beverage", "Paper materials", "Laptop"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_21", "indices": ["29", "1102", "1566", "2030", "2407", "3132", "3625", "4495", "5104", "5858", "5887", "6670", "7627", "8178", "8381", "9106", "9454", "10324", "10730", "11600", "12093", "12963", "13021", "13601", "14268", "14906", "15921", "16327", "16878", "17632", "18299", "18676"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e66cddae8909e74ea2987801772de43c875d5739f0ae11111a079d72fe24bd26", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Mobile phone\n(B) Tablet computer\n(C) Apple\n(D) Beverage\n(E) Paper materials\n(F) Laptop", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "In the video, what is the woman wearing a suit and a black tie, sitting on the bench, holding in her hand?\n(A) Mobile phone\n(B) Tablet computer\n(C) Apple\n(D) Beverage\n(E) Paper materials\n(F) Laptop\nOnly give the best option.\nBest option: ("}
{"doc_id": 170, "doc": {"video_name": "test_needle_49.mp4", "duration": 6713.97, "question": "In the collision event of objects in the video, how many objects enter the scene?\n(A) 2\n(B) 3\n(C) 0\n(D) 4\n(E) 1\n(F) 5", "candidates": ["2", "3", "0", "4", "1", "5"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_25", "indices": ["1900", "7175", "11075", "18575", "25925", "31225", "35625", "41675", "46625", "47350", "57525", "60225", "65300", "72825", "77525", "83700", "85250", "90425", "94650", "104825", "109275", "110200", "120425", "121625", "127550", "132775", "141150", "145875", "150800", "157325", "157400", "166100"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 0"]], "filtered_resps": ["(C) 0"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82b925724aa66cc9a8a56c598139c0f871b8af360067c6e194727193e77ed7b5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, how many objects enter the scene?\n(A) 2\n(B) 3\n(C) 0\n(D) 4\n(E) 1\n(F) 5", "task_type": "needleQA", "pred_answer": "C", "answer": "A"}, "input": "In the collision event of objects in the video, how many objects enter the scene?\n(A) 2\n(B) 3\n(C) 0\n(D) 4\n(E) 1\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 174, "doc": {"video_name": "test_needle_43.mp4", "duration": 5345.59, "question": "In the collision event of objects in the video, what is the shape of the first object that enters the scene?\n(A) cube\n(B) cone\n(C) pyramid\n(D) cylinder\n(E) sphere\n(F) rectangular prism", "candidates": ["cube", "cone", "pyramid", "cylinder", "sphere", "rectangular prism"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_29", "indices": ["900", "5425", "8625", "12875", "17850", "21025", "27225", "32200", "35575", "41150", "42225", "48900", "50375", "56225", "61850", "66775", "67050", "74075", "78725", "83325", "84050", "91175", "93050", "99325", "102350", "106225", "110525", "116200", "118050", "121075", "125600", "131500"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "44a0454a28fa2f2ed418537b93bbb61ba7f28a83a7d944e5c4f695b1342af0a3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what is the shape of the first object that enters the scene?\n(A) cube\n(B) cone\n(C) pyramid\n(D) cylinder\n(E) sphere\n(F) rectangular prism", "task_type": "needleQA", "pred_answer": "B", "answer": "E"}, "input": "In the collision event of objects in the video, what is the shape of the first object that enters the scene?\n(A) cube\n(B) cone\n(C) pyramid\n(D) cylinder\n(E) sphere\n(F) rectangular prism\nOnly give the best option.\nBest option: ("}
{"doc_id": 178, "doc": {"video_name": "test_needle_11.mp4", "duration": 472.52, "question": "In the video, what falls into the transparent glass bowl?\n(A) Blueberries\n(B) Milk\n(C) Cereal\n(D) Egg\n(E) Banana\n(F) Apple", "candidates": ["Blueberries", "Milk", "Cereal", "Egg", "Banana", "Apple"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_33", "indices": ["290", "754", "1131", "1711", "2117", "2262", "2929", "3219", "3886", "4321", "4437", "5017", "5394", "6090", "6409", "7018", "7192", "7598", "8294", "8729", "9048", "9599", "10034", "10498", "10904", "11136", "11861", "12180", "12441", "12847", "13514", "13688"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "147456a63fa9cfa8ecdf2b92c486d785e1155099002ac2bdfd2986ee3048b2a2", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what falls into the transparent glass bowl?\n(A) Blueberries\n(B) Milk\n(C) Cereal\n(D) Egg\n(E) Banana\n(F) Apple", "task_type": "needleQA", "pred_answer": "A", "answer": "C"}, "input": "In the video, what falls into the transparent glass bowl?\n(A) Blueberries\n(B) Milk\n(C) Cereal\n(D) Egg\n(E) Banana\n(F) Apple\nOnly give the best option.\nBest option: ("}
{"doc_id": 182, "doc": {"video_name": "test_needle_0.mp4", "duration": 472.79, "question": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n(E) 4\n(F) 1", "candidates": ["5", "3", "2", "6", "4", "1"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_37", "indices": ["174", "696", "1044", "1450", "2088", "2465", "2639", "3132", "3538", "4060", "4524", "5133", "5307", "6061", "6554", "6612", "7482", "7627", "7946", "8700", "8903", "9280", "9976", "10179", "10991", "11455", "11484", "12354", "12644", "13166", "13630", "13978"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 4"]], "filtered_resps": ["(E) 4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d420cbe77d97cd98149acd626efbbc766ef33db6b34cfde283e31594f994f47", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n(E) 4\n(F) 1", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "How many bats hanging upside down on the tree branches in the video?\n(A) 5\n(B) 3\n(C) 2\n(D) 6\n(E) 4\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 186, "doc": {"video_name": "test_needle_39.mp4", "duration": 496.32, "question": "In the video, what is the white cup filled with black liquid placed on?\n(A) Ground\n(B) Leather\n(C) Bowl\n(D) Wooden board\n(E) Basin\n(F) Computer", "candidates": ["Ground", "Leather", "Bowl", "Wooden board", "Basin", "Computer"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_41", "indices": ["232", "870", "957", "1682", "2233", "2726", "3016", "3567", "3857", "4234", "4640", "5510", "5974", "6438", "6641", "7366", "7656", "7888", "8555", "9048", "9628", "10005", "10643", "10701", "11484", "11600", "12354", "12702", "13195", "13572", "14297", "14442"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b93e7069dea06cda3b82fff6a75842148b19d7f9d239f4ce1833bca0e74752df", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what is the white cup filled with black liquid placed on?\n(A) Ground\n(B) Leather\n(C) Bowl\n(D) Wooden board\n(E) Basin\n(F) Computer", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what is the white cup filled with black liquid placed on?\n(A) Ground\n(B) Leather\n(C) Bowl\n(D) Wooden board\n(E) Basin\n(F) Computer\nOnly give the best option.\nBest option: ("}
{"doc_id": 190, "doc": {"video_name": "test_needle_22.mp4", "duration": 6570.4800000000005, "question": "In the video, what fruit falls into the water from above?\n(A) Grape\n(B) Orange\n(C) Apple\n(D) Blueberry\n(E) Peach\n(F) Strawberry", "candidates": ["Grape", "Orange", "Apple", "Blueberry", "Peach", "Strawberry"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_45", "indices": ["8100", "8125", "8150", "8175", "8200", "8225", "8250", "8275", "8300", "8325", "8350", "8375", "8400", "8425", "8450", "8475", "8500", "8525", "8550", "8575", "8600", "8625", "8650", "8675", "8700", "8725", "8750", "8775", "8800", "8825", "8850", "8900"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54a64ad8ae6a12c287285207204e48a70eb7a14b70b6887b5b90d69154abe53c", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, what fruit falls into the water from above?\n(A) Grape\n(B) Orange\n(C) Apple\n(D) Blueberry\n(E) Peach\n(F) Strawberry", "task_type": "needleQA", "pred_answer": "F", "answer": "F"}, "input": "In the video, what fruit falls into the water from above?\n(A) Grape\n(B) Orange\n(C) Apple\n(D) Blueberry\n(E) Peach\n(F) Strawberry\nOnly give the best option.\nBest option: ("}
{"doc_id": 194, "doc": {"video_name": "test_needle_40.mp4", "duration": 6694.150000000001, "question": "In the collision event of objects in the video, how many rubber cylinders exit the scene after the cube enters the scene?\n(A) 5\n(B) 0\n(C) 3\n(D) 2\n(E) 4\n(F) 1", "candidates": ["5", "0", "3", "2", "4", "1"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_49", "indices": ["1350", "6775", "17575", "17600", "17625", "17650", "17675", "17700", "18450", "21825", "22025", "30125", "44325", "55875", "84625", "87875", "88075", "89250", "104475", "109500", "112150", "118325", "121900", "122975", "123000", "123025", "123225", "139975", "140900", "141975", "147450", "160100"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e7a68038a7b18ae8c9af001d26b8a60a2345af798c32607abd438433a9fe6c24", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, how many rubber cylinders exit the scene after the cube enters the scene?\n(A) 5\n(B) 0\n(C) 3\n(D) 2\n(E) 4\n(F) 1", "task_type": "needleQA", "pred_answer": "B", "answer": "B"}, "input": "In the collision event of objects in the video, how many rubber cylinders exit the scene after the cube enters the scene?\n(A) 5\n(B) 0\n(C) 3\n(D) 2\n(E) 4\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 198, "doc": {"video_name": "test_needle_20.mp4", "duration": 7655.280000000001, "question": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Piano\n(B) Guitar\n(C) Flute\n(D) Violin\n(E) Guzheng\n(F) Drum set", "candidates": ["Piano", "Guitar", "Flute", "Violin", "Guzheng", "Drum set"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_53", "indices": ["70225", "70350", "70375", "70400", "70450", "70475", "70500", "70525", "70600", "70650", "70725", "70750", "70900", "70925", "70950", "70975", "71050", "71100", "71225", "71300", "71325", "71375", "71400", "71425", "71450", "71500", "71525", "71550", "71575", "71600", "71625", "71650"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "55734a682f61b2cf73b89e95956b23bdacaa7dbe1ab0c0ec6deecaf00fbed60e", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Piano\n(B) Guitar\n(C) Flute\n(D) Violin\n(E) Guzheng\n(F) Drum set", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what instrument is the woman in the red dress playing by the water?\n(A) Piano\n(B) Guitar\n(C) Flute\n(D) Violin\n(E) Guzheng\n(F) Drum set\nOnly give the best option.\nBest option: ("}
{"doc_id": 202, "doc": {"video_name": "test_needle_59.mp4", "duration": 5879.96, "question": "In the collision event of objects in the video, without the cyan rubber object, what will happen?\n(A) The cylinder collides with the blue cube\n(B) The cylinder stops before reaching the red sphere\n(C) The cylinder changes direction and moves away\n(D) The cylinder passes through without colliding\n(E) The cylinder collides with the red sphere\n(F) The cylinder merges with the red sphere", "candidates": ["The cylinder collides with the blue cube", "The cylinder stops before reaching the red sphere", "The cylinder changes direction and moves away", "The cylinder passes through without colliding", "The cylinder collides with the red sphere", "The cylinder merges with the red sphere"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_57", "indices": ["2875", "5525", "12500", "15525", "19525", "25675", "29225", "35900", "36775", "42200", "50150", "50575", "59525", "61325", "67900", "70150", "74200", "80500", "84125", "89550", "93150", "98100", "101750", "107250", "114525", "115175", "122625", "127275", "132375", "136075", "138300", "146950"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "122b1ac19a82704c9e364330c46150f3caa77e7c92b01ecd882ad2517f27ff5b", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, without the cyan rubber object, what will happen?\n(A) The cylinder collides with the blue cube\n(B) The cylinder stops before reaching the red sphere\n(C) The cylinder changes direction and moves away\n(D) The cylinder passes through without colliding\n(E) The cylinder collides with the red sphere\n(F) The cylinder merges with the red sphere", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the collision event of objects in the video, without the cyan rubber object, what will happen?\n(A) The cylinder collides with the blue cube\n(B) The cylinder stops before reaching the red sphere\n(C) The cylinder changes direction and moves away\n(D) The cylinder passes through without colliding\n(E) The cylinder collides with the red sphere\n(F) The cylinder merges with the red sphere\nOnly give the best option.\nBest option: ("}
{"doc_id": 206, "doc": {"video_name": "test_cartoon_18.mp4", "duration": 355.39, "question": "After making peace in the video, where does the mouse stand on the cat to go out together?\n(A) Head\n(B) Finger\n(C) Shoulder\n(D) Arm\n(E) Belly\n(F) Palm", "candidates": ["Head", "Finger", "Shoulder", "Arm", "Belly", "Palm"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_1", "indices": ["232", "522", "667", "1189", "1537", "1653", "2204", "2581", "2842", "3132", "3393", "3886", "3973", "4437", "4814", "5075", "5539", "5887", "6090", "6380", "6815", "7134", "7395", "7714", "8062", "8410", "8816", "9193", "9599", "9686", "10179", "10295"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90ec3197a43969d4e8c811b0fd7a6cba7177f1dd4e07d6d499699dec2ad788e0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "After making peace in the video, where does the mouse stand on the cat to go out together?\n(A) Head\n(B) Finger\n(C) Shoulder\n(D) Arm\n(E) Belly\n(F) Palm", "task_type": "plotQA", "pred_answer": "E", "answer": "C"}, "input": "After making peace in the video, where does the mouse stand on the cat to go out together?\n(A) Head\n(B) Finger\n(C) Shoulder\n(D) Arm\n(E) Belly\n(F) Palm\nOnly give the best option.\nBest option: ("}
{"doc_id": 210, "doc": {"video_name": "test_entv_13.mp4", "duration": 678.6181831810065, "question": "At the end of the video, what were the woman and the man in the red plaid shirt fighting over?\n(A) Notebook\n(B) Cellphone\n(C) Ball\n(D) Phone\n(E) Pen\n(F) Burger", "candidates": ["Notebook", "Cellphone", "Ball", "Phone", "Pen", "Burger"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_5", "indices": ["175", "550", "1200", "2050", "2200", "2850", "3650", "4050", "4675", "5175", "5725", "5950", "6550", "7250", "7400", "7925", "8850", "9450", "9800", "10325", "10600", "11275", "11900", "12525", "12975", "13725", "13750", "14375", "15300", "15400", "16325", "16450"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b6756323a12c711cb546c059f0fec2d2c9af42415fd445b98835340e6959bb6f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "At the end of the video, what were the woman and the man in the red plaid shirt fighting over?\n(A) Notebook\n(B) Cellphone\n(C) Ball\n(D) Phone\n(E) Pen\n(F) Burger", "task_type": "plotQA", "pred_answer": "E", "answer": "D"}, "input": "At the end of the video, what were the woman and the man in the red plaid shirt fighting over?\n(A) Notebook\n(B) Cellphone\n(C) Ball\n(D) Phone\n(E) Pen\n(F) Burger\nOnly give the best option.\nBest option: ("}
{"doc_id": 214, "doc": {"video_name": "test_cartoon_3.mp4", "duration": 572.577543878504, "question": "What color is the hat worn by the cartoon sponge in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n(E) White\n(F) Orange", "candidates": ["Purple", "Black", "Yellow", "Red", "White", "Orange"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_9", "indices": ["0", "550", "1225", "1675", "2050", "2575", "2675", "3525", "3650", "4000", "4500", "5275", "5750", "5975", "6275", "7100", "7550", "7950", "8375", "8600", "9000", "9700", "9825", "10525", "10875", "11425", "12000", "12125", "12800", "13200", "13825", "13900"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d02dd6593583a9384f73a3c75ef0a3f91fd469c41c3c6fb1f993f2342a5a1718", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What color is the hat worn by the cartoon sponge in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n(E) White\n(F) Orange", "task_type": "plotQA", "pred_answer": "D", "answer": "E"}, "input": "What color is the hat worn by the cartoon sponge in the video?\n(A) Purple\n(B) Black\n(C) Yellow\n(D) Red\n(E) White\n(F) Orange\nOnly give the best option.\nBest option: ("}
{"doc_id": 218, "doc": {"video_name": "test_cartoon_17.mp4", "duration": 400.58, "question": "In the video, what did the grey mouse put into the king's mouth?\n(A) Stone\n(B) Lychee\n(C) Tomato\n(D) Grapes\n(E) Bread\n(F) Cheese", "candidates": ["Stone", "Lychee", "Tomato", "Grapes", "Bread", "Cheese"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_13", "indices": ["145", "348", "1073", "1131", "1711", "1972", "2407", "2900", "3248", "3683", "3886", "4321", "4814", "5017", "5249", "5626", "6264", "6496", "7047", "7163", "7830", "7917", "8381", "8787", "9077", "9657", "9889", "10411", "10527", "10933", "11310", "11716"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6be6fb2080e7f785693086b7dd004a74d96cae6c3366507f5e52ec9d275b55c6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what did the grey mouse put into the king's mouth?\n(A) Stone\n(B) Lychee\n(C) Tomato\n(D) Grapes\n(E) Bread\n(F) Cheese", "task_type": "plotQA", "pred_answer": "F", "answer": "D"}, "input": "In the video, what did the grey mouse put into the king's mouth?\n(A) Stone\n(B) Lychee\n(C) Tomato\n(D) Grapes\n(E) Bread\n(F) Cheese\nOnly give the best option.\nBest option: ("}
{"doc_id": 222, "doc": {"video_name": "test_entv_22.mp4", "duration": 407.116439508766, "question": "In the video, what do the two women do after tasting the food?\n(A) Spit it out\n(B) Share with each other\n(C) Enjoy\n(D) Throw it away\n(E) Praise\n(F) Continue eating", "candidates": ["Spit it out", "Share with each other", "Enjoy", "Throw it away", "Praise", "Continue eating"], "answer": "A", "task_type": "plotQA", "question_id": "PQA_17", "indices": ["230", "299", "805", "943", "1334", "1633", "1932", "2231", "2530", "3013", "3312", "3473", "3657", "4163", "4531", "4692", "5152", "5451", "5497", "5957", "6141", "6670", "6946", "7061", "7567", "7705", "8119", "8234", "8786", "8832", "9407", "9614"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbc15a44ede770457f7c032c7950071dd6aa8c06a88ab92f2279e902efb7698b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what do the two women do after tasting the food?\n(A) Spit it out\n(B) Share with each other\n(C) Enjoy\n(D) Throw it away\n(E) Praise\n(F) Continue eating", "task_type": "plotQA", "pred_answer": "E", "answer": "A"}, "input": "In the video, what do the two women do after tasting the food?\n(A) Spit it out\n(B) Share with each other\n(C) Enjoy\n(D) Throw it away\n(E) Praise\n(F) Continue eating\nOnly give the best option.\nBest option: ("}
{"doc_id": 226, "doc": {"video_name": "test_entv_21.mp4", "duration": 750.3097253436729, "question": "In the video, what letters are written on the shirt of the woman standing on the table?\n(A) USB\n(B) USA\n(C) CSB\n(D) VSA\n(E) UNA\n(F) CSA", "candidates": ["USB", "USA", "CSB", "VSA", "UNA", "CSA"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_21", "indices": ["230", "667", "1242", "2208", "2691", "2990", "3519", "4393", "4692", "5175", "6118", "6670", "6900", "7544", "8395", "8648", "9338", "10028", "10189", "10810", "11661", "12328", "12466", "13340", "13800", "14467", "14743", "15410", "16192", "16284", "17066", "17940"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) USA"]], "filtered_resps": ["(B) USA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1b167a330d2010eb683e39af600e68115c68b682585f5f657906fdcd8abc1099", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what letters are written on the shirt of the woman standing on the table?\n(A) USB\n(B) USA\n(C) CSB\n(D) VSA\n(E) UNA\n(F) CSA", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "In the video, what letters are written on the shirt of the woman standing on the table?\n(A) USB\n(B) USA\n(C) CSB\n(D) VSA\n(E) UNA\n(F) CSA\nOnly give the best option.\nBest option: ("}
{"doc_id": 230, "doc": {"video_name": "test_cartoon_13.mp4", "duration": 379.33, "question": "What did the cartoon mouse in the green dress conjure with the magic wand?\n(A) Remote control\n(B) Pink potion\n(C) Stick\n(D) Scissors\n(E) Crown\n(F) Car", "candidates": ["Remote control", "Pink potion", "Stick", "Scissors", "Crown", "Car"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_25", "indices": ["0", "580", "754", "1276", "1450", "2030", "2320", "2726", "2987", "3509", "3567", "4147", "4466", "4785", "5104", "5655", "5829", "6061", "6467", "6960", "7134", "7656", "7917", "8149", "8671", "9193", "9222", "9773", "10266", "10556", "10643", "11339"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b13524d2515ba6b39d3bb2266fe4c7368413a7fef13d7103925fcc4da14aecb", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the cartoon mouse in the green dress conjure with the magic wand?\n(A) Remote control\n(B) Pink potion\n(C) Stick\n(D) Scissors\n(E) Crown\n(F) Car", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What did the cartoon mouse in the green dress conjure with the magic wand?\n(A) Remote control\n(B) Pink potion\n(C) Stick\n(D) Scissors\n(E) Crown\n(F) Car\nOnly give the best option.\nBest option: ("}
{"doc_id": 234, "doc": {"video_name": "test_entv_18.mp4", "duration": 651.8112954069612, "question": "At the end of the video, what are the men around the round table doing?\n(A) Reading\n(B) Playing chess\n(C) Eating\n(D) Watching TV\n(E) Using computer\n(F) Using phone", "candidates": ["Reading", "Playing chess", "Eating", "Watching TV", "Using computer", "Using phone"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_29", "indices": ["15544", "15573", "15602", "15631", "15660", "15689", "15718", "15747", "15776", "15805", "15834", "15863", "15892", "15921", "16153", "16182", "17342", "17371", "17400", "17429", "17458", "17806", "17835", "18009", "18038", "18067", "19140", "19314", "19343", "19372", "19401", "19430"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68a215a478e2b95e700e98f7cab3f45f1f414cba50f55b9153caf6399276cc36", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "At the end of the video, what are the men around the round table doing?\n(A) Reading\n(B) Playing chess\n(C) Eating\n(D) Watching TV\n(E) Using computer\n(F) Using phone", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "At the end of the video, what are the men around the round table doing?\n(A) Reading\n(B) Playing chess\n(C) Eating\n(D) Watching TV\n(E) Using computer\n(F) Using phone\nOnly give the best option.\nBest option: ("}
{"doc_id": 238, "doc": {"video_name": "test_entv_2.mp4", "duration": 403.40632292212683, "question": "What expression does the short man have while speaking indoors in the video?\n(A) Excited\n(B) Crying\n(C) Guilty\n(D) Nervous\n(E) Fearful\n(F) Neutral", "candidates": ["Excited", "Crying", "Guilty", "Nervous", "Fearful", "Neutral"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_33", "indices": ["184", "483", "759", "966", "1288", "1587", "1978", "2369", "2484", "2921", "3243", "3427", "3887", "3979", "4462", "4508", "5037", "5221", "5704", "5934", "6118", "6371", "6831", "7153", "7429", "7797", "7889", "8303", "8648", "8786", "9315", "9545"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Crying"]], "filtered_resps": ["(B) Crying"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2661221e3d6696611d99df94a68f8c0c6650d14f71915b67bcddcffa47fd5b1a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What expression does the short man have while speaking indoors in the video?\n(A) Excited\n(B) Crying\n(C) Guilty\n(D) Nervous\n(E) Fearful\n(F) Neutral", "task_type": "plotQA", "pred_answer": "B", "answer": "C"}, "input": "What expression does the short man have while speaking indoors in the video?\n(A) Excited\n(B) Crying\n(C) Guilty\n(D) Nervous\n(E) Fearful\n(F) Neutral\nOnly give the best option.\nBest option: ("}
{"doc_id": 242, "doc": {"video_name": "test_cartoon_2.mp4", "duration": 733.4075768505323, "question": "What kind of clothes is the squirrel wearing in the video?\n(A) Dress\n(B) Jacket\n(C) Shirt\n(D) Spacesuit\n(E) T-shirt\n(F) Sweatshirt", "candidates": ["Dress", "Jacket", "Shirt", "Spacesuit", "T-shirt", "Sweatshirt"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_37", "indices": ["525", "1050", "1200", "2125", "2400", "3350", "3850", "4075", "4925", "5500", "5950", "6725", "6900", "7825", "8175", "8650", "9525", "9750", "10825", "10850", "11475", "12025", "12775", "13200", "13950", "14500", "15400", "15525", "16450", "17075", "17650", "18025"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "01cb8f4b3291ef68965737149b52c6273b4883fb4a1578f3a949e38784114eed", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What kind of clothes is the squirrel wearing in the video?\n(A) Dress\n(B) Jacket\n(C) Shirt\n(D) Spacesuit\n(E) T-shirt\n(F) Sweatshirt", "task_type": "plotQA", "pred_answer": "D", "answer": "D"}, "input": "What kind of clothes is the squirrel wearing in the video?\n(A) Dress\n(B) Jacket\n(C) Shirt\n(D) Spacesuit\n(E) T-shirt\n(F) Sweatshirt\nOnly give the best option.\nBest option: ("}
{"doc_id": 246, "doc": {"video_name": "test_cartoon_14.mp4", "duration": 453.5, "question": "Why did the big cat suddenly jump while teaching?\n(A) The kitten left\n(B) The kitten was talking\n(C) The kitten was playing\n(D) The kitten fell asleep\n(E) The kitten was eating\n(F) The kitten was drinking water", "candidates": ["The kitten left", "The kitten was talking", "The kitten was playing", "The kitten fell asleep", "The kitten was eating", "The kitten was drinking water"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_41", "indices": ["232", "783", "1131", "1334", "1740", "2175", "2871", "3364", "3422", "4089", "4466", "4959", "5307", "5829", "6293", "6351", "7163", "7540", "7801", "8410", "8816", "9251", "9483", "9831", "10382", "10701", "11397", "11658", "12064", "12412", "13050", "13282"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6d244159c6f259f62b9536b7f4a0f7dadf63aab5c83f2bd4cfe71f23d8c67ef1", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Why did the big cat suddenly jump while teaching?\n(A) The kitten left\n(B) The kitten was talking\n(C) The kitten was playing\n(D) The kitten fell asleep\n(E) The kitten was eating\n(F) The kitten was drinking water", "task_type": "plotQA", "pred_answer": "C", "answer": "D"}, "input": "Why did the big cat suddenly jump while teaching?\n(A) The kitten left\n(B) The kitten was talking\n(C) The kitten was playing\n(D) The kitten fell asleep\n(E) The kitten was eating\n(F) The kitten was drinking water\nOnly give the best option.\nBest option: ("}
{"doc_id": 250, "doc": {"video_name": "test_movie101_93.mp4", "duration": 641.0, "question": "What is the emotion of the woman in black inspecting the suitcase at the beginning of the video?\n(A) Excited\n(B) Angry\n(C) Terrified\n(D) Nervous\n(E) Neutral\n(F) Sad", "candidates": ["Excited", "Angry", "Terrified", "Nervous", "Neutral", "Sad"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_45", "indices": ["425", "525", "1475", "1800", "2050", "2850", "3150", "3550", "4475", "4975", "5475", "5775", "6350", "6550", "7375", "7800", "8150", "8950", "9350", "9975", "10475", "10525", "11125", "11550", "12475", "12950", "13050", "13950", "14325", "14925", "15325", "15600"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Neutral"]], "filtered_resps": ["(E) Neutral"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b7c649fa46e24b99274f0fc95c492146ebdca53e69c82d5ecdeadda97897e043", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the emotion of the woman in black inspecting the suitcase at the beginning of the video?\n(A) Excited\n(B) Angry\n(C) Terrified\n(D) Nervous\n(E) Neutral\n(F) Sad", "task_type": "plotQA", "pred_answer": "E", "answer": "B"}, "input": "What is the emotion of the woman in black inspecting the suitcase at the beginning of the video?\n(A) Excited\n(B) Angry\n(C) Terrified\n(D) Nervous\n(E) Neutral\n(F) Sad\nOnly give the best option.\nBest option: ("}
{"doc_id": 254, "doc": {"video_name": "test_entv_5.mp4", "duration": 700.678139986795, "question": "In the video, what color is the dress worn by the woman on the sailboat?\n(A) Red\n(B) Yellow\n(C) Blue\n(D) Purple\n(E) White\n(F) Green", "candidates": ["Red", "Yellow", "Blue", "Purple", "White", "Green"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_49", "indices": ["391", "828", "1173", "1840", "2185", "2990", "3542", "4117", "4186", "4853", "5612", "6026", "6578", "7314", "7337", "7958", "8579", "9177", "9890", "10327", "10856", "11017", "12006", "12443", "12673", "13409", "13961", "14145", "14973", "15272", "16192", "16307"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fcde016f133533179054b2e2adbd376fa169f0ede4f7fefecc5e34a6544f5ea0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what color is the dress worn by the woman on the sailboat?\n(A) Red\n(B) Yellow\n(C) Blue\n(D) Purple\n(E) White\n(F) Green", "task_type": "plotQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what color is the dress worn by the woman on the sailboat?\n(A) Red\n(B) Yellow\n(C) Blue\n(D) Purple\n(E) White\n(F) Green\nOnly give the best option.\nBest option: ("}
{"doc_id": 258, "doc": {"video_name": "test_UWA-6.mp4", "duration": 450.0, "question": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n(E) 2\n(F) 4", "candidates": ["5", "1", "3", "6", "2", "4"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_3", "indices": ["300", "425", "725", "1375", "1425", "1850", "2425", "2650", "2875", "3200", "3500", "4150", "4225", "4775", "5225", "5425", "5950", "5975", "6375", "6975", "7075", "7400", "7850", "8125", "8750", "9100", "9175", "9750", "9975", "10250", "10625", "10875"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "10a2cc2718b2c9a5447dc78da53b666915f1a0cd4d18b8df19a30d5e45dcc6e6", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n(E) 2\n(F) 4", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "F"}, "input": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 6\n(E) 2\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 262, "doc": {"video_name": "test_entv_2.mp4", "duration": 403.40632292212683, "question": "What is the setting of the entire video?\n(A) Castle\n(B) Desert\n(C) Library\n(D) Seaside\n(E) Campus\n(F) City", "candidates": ["Castle", "Desert", "Library", "Seaside", "Campus", "City"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_7", "indices": ["23", "483", "644", "1058", "1242", "1771", "1817", "2093", "2415", "2944", "3059", "3542", "3611", "4071", "4347", "4600", "4945", "5221", "5612", "5727", "6026", "6417", "6670", "7176", "7498", "7544", "8050", "8326", "8441", "8993", "9246", "9499"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "493e2c5293e1181211808023edca8246df8b7d79927fa17380584b452c00252d", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What is the setting of the entire video?\n(A) Castle\n(B) Desert\n(C) Library\n(D) Seaside\n(E) Campus\n(F) City", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What is the setting of the entire video?\n(A) Castle\n(B) Desert\n(C) Library\n(D) Seaside\n(E) Campus\n(F) City\nOnly give the best option.\nBest option: ("}
{"doc_id": 266, "doc": {"video_name": "test_entv_21.mp4", "duration": 750.3097253436729, "question": "What genre is this film?\n(A) Sci-Fi\n(B) Documentary\n(C) War\n(D) Horror\n(E) Comedy\n(F) Action", "candidates": ["Sci-Fi", "Documentary", "War", "Horror", "Comedy", "Action"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_11", "indices": ["414", "828", "1495", "1794", "2507", "2806", "3680", "4393", "4646", "5152", "6026", "6486", "6923", "7705", "8372", "8556", "9131", "9982", "10097", "10741", "11684", "12121", "12834", "13340", "13478", "14191", "15111", "15157", "15893", "16836", "17365", "17618"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "89d976fd31aa20627a3a87c59ea7f74e33a608a97902ba745ffeb592bee2ac70", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Sci-Fi\n(B) Documentary\n(C) War\n(D) Horror\n(E) Comedy\n(F) Action", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What genre is this film?\n(A) Sci-Fi\n(B) Documentary\n(C) War\n(D) Horror\n(E) Comedy\n(F) Action\nOnly give the best option.\nBest option: ("}
{"doc_id": 270, "doc": {"video_name": "test_game_12.mp4", "duration": 203.0, "question": "In what setting does the main action in the video occur?\n(A) Cave\n(B) Palace\n(C) Flatland\n(D) Cabin\n(E) Roof\n(F) Attic", "candidates": ["Cave", "Palace", "Flatland", "Cabin", "Roof", "Attic"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_15", "indices": ["0", "261", "464", "667", "812", "957", "1131", "1305", "1595", "1682", "1914", "2233", "2349", "2465", "2784", "3016", "3161", "3248", "3422", "3596", "3828", "4002", "4205", "4350", "4611", "4872", "5046", "5249", "5452", "5655", "5684", "5945"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "108a830bc1134dcbb2bf1e962103706c5ea5e46079ee3c0a8be38e6c5d9c0388", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the main action in the video occur?\n(A) Cave\n(B) Palace\n(C) Flatland\n(D) Cabin\n(E) Roof\n(F) Attic", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the main action in the video occur?\n(A) Cave\n(B) Palace\n(C) Flatland\n(D) Cabin\n(E) Roof\n(F) Attic\nOnly give the best option.\nBest option: ("}
{"doc_id": 274, "doc": {"video_name": "test_movie101_94.mp4", "duration": 439.0, "question": "In what setting does the video take place?\n(A) Urban\n(B) Forest\n(C) Ocean\n(D) Desert\n(E) Countryside\n(F) Campus", "candidates": ["Urban", "Forest", "Ocean", "Desert", "Countryside", "Campus"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_19", "indices": ["125", "575", "850", "1325", "1625", "1975", "2350", "2375", "2725", "3175", "3650", "3825", "4375", "4475", "5025", "5200", "5775", "5950", "6175", "6625", "6950", "7500", "7525", "8100", "8300", "8825", "8950", "9500", "9675", "10100", "10550", "10950"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Campus"]], "filtered_resps": ["(F) Campus"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "412d8ee9463d1760500974bc84adf71722649bedbc1e279b15d2a8d9ebbf471e", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Urban\n(B) Forest\n(C) Ocean\n(D) Desert\n(E) Countryside\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "A"}, "input": "In what setting does the video take place?\n(A) Urban\n(B) Forest\n(C) Ocean\n(D) Desert\n(E) Countryside\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 278, "doc": {"video_name": "test_entv_6.mp4", "duration": 573.0751841594767, "question": "In what setting does the video take place?\n(A) Campus\n(B) Countryside\n(C) Ocean\n(D) Desert\n(E) Forest\n(F) Castle", "candidates": ["Campus", "Countryside", "Ocean", "Desert", "Forest", "Castle"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_23", "indices": ["322", "644", "874", "1311", "1886", "2116", "2967", "2990", "3703", "4255", "4508", "4692", "5359", "5957", "5980", "6417", "7130", "7360", "7981", "8441", "8625", "9062", "9568", "10166", "10672", "10879", "11224", "11983", "12006", "12811", "12949", "13455"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Forest"]], "filtered_resps": ["(E) Forest"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1602f3510272a89c91f74dccd3157d6d2b209f3458bb7099fc1d6e427a956c6", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Campus\n(B) Countryside\n(C) Ocean\n(D) Desert\n(E) Forest\n(F) Castle", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "F"}, "input": "In what setting does the video take place?\n(A) Campus\n(B) Countryside\n(C) Ocean\n(D) Desert\n(E) Forest\n(F) Castle\nOnly give the best option.\nBest option: ("}
{"doc_id": 282, "doc": {"video_name": "test_game_10.mp4", "duration": 236.0, "question": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Car\n(C) Train\n(D) Truck\n(E) High-speed rail\n(F) Pickup", "candidates": ["Bus", "Car", "Train", "Truck", "High-speed rail", "Pickup"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_27", "indices": ["120", "210", "480", "780", "900", "1290", "1350", "1590", "1800", "2160", "2220", "2580", "2730", "3030", "3090", "3510", "3660", "3750", "3960", "4170", "4590", "4620", "4980", "5220", "5490", "5640", "5910", "5970", "6300", "6600", "6690", "7020"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Train"]], "filtered_resps": ["(C) Train"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3537ba2563c9680b7b349a538809b9612909efeee400fcd4bd008af8b3ef3752", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Car\n(C) Train\n(D) Truck\n(E) High-speed rail\n(F) Pickup", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of vehicle is being constructed in the video?\n(A) Bus\n(B) Car\n(C) Train\n(D) Truck\n(E) High-speed rail\n(F) Pickup\nOnly give the best option.\nBest option: ("}
{"doc_id": 286, "doc": {"video_name": "test_BWB-4.mp4", "duration": 450.08, "question": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 2\n(E) 4\n(F) 6", "candidates": ["5", "1", "3", "2", "4", "6"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_31", "indices": ["175", "425", "750", "1100", "1525", "2075", "2425", "2725", "2925", "3275", "3800", "3850", "4525", "4550", "4900", "5250", "5675", "6175", "6625", "6900", "7025", "7700", "7875", "8400", "8650", "8775", "9375", "9475", "10150", "10175", "10850", "10975"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A)"]], "filtered_resps": ["(A)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba1ac9ed3f76e2b00ecaf6cb7b9b931968fef5055a9d2b6a32f7de4e0bf583d5", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 2\n(E) 4\n(F) 6", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "F"}, "input": "How many types of animals appeared in the video?\n(A) 5\n(B) 1\n(C) 3\n(D) 2\n(E) 4\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 290, "doc": {"video_name": "test_game_11.mp4", "duration": 304.0, "question": "Which animal is primarily featured in the video?\n(A) Cow\n(B) Horse\n(C) Camel\n(D) Giraffe\n(E) Sheep\n(F) Moose", "candidates": ["Cow", "Horse", "Camel", "Giraffe", "Sheep", "Moose"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_35", "indices": ["0", "480", "750", "900", "1200", "1500", "1710", "2010", "2460", "2760", "2850", "3360", "3570", "3810", "4200", "4500", "4620", "4920", "5130", "5520", "5760", "6000", "6480", "6540", "6990", "7290", "7440", "7950", "8100", "8310", "8670", "8820"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a48b51f575d87c1ff6ca15e51ac6a54b4ea93a6a79089398ae342ec83105451", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which animal is primarily featured in the video?\n(A) Cow\n(B) Horse\n(C) Camel\n(D) Giraffe\n(E) Sheep\n(F) Moose", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "Which animal is primarily featured in the video?\n(A) Cow\n(B) Horse\n(C) Camel\n(D) Giraffe\n(E) Sheep\n(F) Moose\nOnly give the best option.\nBest option: ("}
{"doc_id": 294, "doc": {"video_name": "test_cartoon_10.mp4", "duration": 586.918084061671, "question": "In what environment does the video take place?\n(A) Underwater\n(B) Countryside\n(C) Desert\n(D) Forest\n(E) City\n(F) Campus", "candidates": ["Underwater", "Countryside", "Desert", "Forest", "City", "Campus"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_39", "indices": ["275", "525", "1100", "1800", "1975", "2550", "2875", "3450", "3975", "4400", "4550", "5325", "5750", "5925", "6425", "7025", "7550", "7950", "8600", "8700", "9525", "9600", "10400", "10600", "11400", "11725", "12025", "12550", "13225", "13500", "13925", "14575"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e14c6937b17cfc0467796aa788dd32ad0156b40a2eaad92ce91aecd29391091b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what environment does the video take place?\n(A) Underwater\n(B) Countryside\n(C) Desert\n(D) Forest\n(E) City\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what environment does the video take place?\n(A) Underwater\n(B) Countryside\n(C) Desert\n(D) Forest\n(E) City\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 298, "doc": {"video_name": "test_dailylife_1.mp4", "duration": 487.04, "question": "What action is primarily depicted in the video?\n(A) Playing music\n(B) Driving a car\n(C) Working in an office\n(D) Visiting an exhibition\n(E) Food preparation\n(F) Playing sports", "candidates": ["Playing music", "Driving a car", "Working in an office", "Visiting an exhibition", "Food preparation", "Playing sports"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_43", "indices": ["0", "450", "960", "1530", "2070", "2670", "3000", "3270", "3840", "4170", "4740", "5370", "5700", "6330", "6480", "7230", "7560", "7920", "8460", "8880", "9300", "9630", "10230", "10470", "11040", "11700", "12180", "12690", "13200", "13620", "13710", "14580"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Food preparation"]], "filtered_resps": ["(E) Food preparation"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b48cd5ec16a1d17d7f1d8da606cdf82d5470cb0eedd5f4d6459e55deb5ef98ef", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What action is primarily depicted in the video?\n(A) Playing music\n(B) Driving a car\n(C) Working in an office\n(D) Visiting an exhibition\n(E) Food preparation\n(F) Playing sports", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What action is primarily depicted in the video?\n(A) Playing music\n(B) Driving a car\n(C) Working in an office\n(D) Visiting an exhibition\n(E) Food preparation\n(F) Playing sports\nOnly give the best option.\nBest option: ("}
{"doc_id": 302, "doc": {"video_name": "test_dailylife_2.mp4", "duration": 532.81, "question": "What action is primarily shown in the video?\n(A) Mountain climbing\n(B) Running\n(C) Working\n(D) Gaming\n(E) Driving\n(F) Applying makeup", "candidates": ["Mountain climbing", "Running", "Working", "Gaming", "Driving", "Applying makeup"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_47", "indices": ["30", "570", "1230", "1830", "2190", "2850", "2970", "3480", "4080", "4860", "5280", "5730", "6030", "6720", "7200", "7800", "8310", "8880", "9360", "9480", "10410", "10650", "11100", "11880", "12060", "12570", "13110", "13530", "14160", "14790", "15030", "15930"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Applying makeup"]], "filtered_resps": ["(F) Applying makeup"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b132b5d1f3ec147f84bb83e74d4d87e0e45855ed09cc49eae0ffcb42b3dc0089", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What action is primarily shown in the video?\n(A) Mountain climbing\n(B) Running\n(C) Working\n(D) Gaming\n(E) Driving\n(F) Applying makeup", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What action is primarily shown in the video?\n(A) Mountain climbing\n(B) Running\n(C) Working\n(D) Gaming\n(E) Driving\n(F) Applying makeup\nOnly give the best option.\nBest option: ("}
{"doc_id": 306, "doc": {"video_name": "test_movie101_98.mp4", "duration": 394.0, "question": "What type of film is this?\n(A) Horror\n(B) Sci-fi\n(C) Documentary\n(D) Crime film\n(E) War\n(F) Romance", "candidates": ["Horror", "Sci-fi", "Documentary", "Crime film", "War", "Romance"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_51", "indices": ["250", "425", "725", "1050", "1300", "1525", "1850", "2125", "2525", "2800", "3275", "3450", "3700", "4175", "4350", "4875", "5150", "5500", "5700", "6050", "6300", "6475", "6900", "7325", "7600", "7725", "8000", "8525", "8775", "9200", "9300", "9575"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Sci-fi"]], "filtered_resps": ["(B) Sci-fi"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08bffdd21275e3ca1a356bf36d4a6f0b23625e28072e3c50e29477e8c623d4a2", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Horror\n(B) Sci-fi\n(C) Documentary\n(D) Crime film\n(E) War\n(F) Romance", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "D"}, "input": "What type of film is this?\n(A) Horror\n(B) Sci-fi\n(C) Documentary\n(D) Crime film\n(E) War\n(F) Romance\nOnly give the best option.\nBest option: ("}
{"doc_id": 310, "doc": {"video_name": "test_movie101_98.mp4", "duration": 394.0, "question": "In what setting does the video take place?\n(A) Urban\n(B) Countryside\n(C) Campus\n(D) Ocean\n(E) Forest\n(F) Desert", "candidates": ["Urban", "Countryside", "Campus", "Ocean", "Forest", "Desert"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_55", "indices": ["225", "450", "675", "1000", "1300", "1750", "1825", "2125", "2500", "2800", "3275", "3375", "3700", "4275", "4325", "4875", "4950", "5250", "5675", "6050", "6300", "6475", "6900", "7250", "7600", "7725", "8250", "8400", "8825", "9000", "9250", "9525"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) Urban"]], "filtered_resps": ["(A) Urban"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2bb44586a1c4d1e7d83a43dfc67c424e6b67d307422fb1819fad54ff30e765e1", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Urban\n(B) Countryside\n(C) Campus\n(D) Ocean\n(E) Forest\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "In what setting does the video take place?\n(A) Urban\n(B) Countryside\n(C) Campus\n(D) Ocean\n(E) Forest\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 314, "doc": {"video_name": "test_cartoon_13.mp4", "duration": 379.33, "question": "Who are the main characters in the video?\n(A) Cat and Dog\n(B) Dog and Rabbit\n(C) Mouse and Rabbit\n(D) Dog and Mouse\n(E) Cat and Mouse\n(F) Cat and Rabbit", "candidates": ["Cat and Dog", "Dog and Rabbit", "Mouse and Rabbit", "Dog and Mouse", "Cat and Mouse", "Cat and Rabbit"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_59", "indices": ["290", "406", "870", "1044", "1653", "2001", "2262", "2784", "2987", "3364", "3625", "3944", "4524", "4756", "5162", "5626", "5916", "6032", "6583", "6757", "7192", "7540", "7888", "8149", "8526", "8903", "9280", "9889", "10005", "10469", "10904", "11252"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9fbb19ccf6a177c3e8ae2da99a65ae1aedc6d6ae400b6264fa5bb80f71f4d93a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Who are the main characters in the video?\n(A) Cat and Dog\n(B) Dog and Rabbit\n(C) Mouse and Rabbit\n(D) Dog and Mouse\n(E) Cat and Mouse\n(F) Cat and Rabbit", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "Who are the main characters in the video?\n(A) Cat and Dog\n(B) Dog and Rabbit\n(C) Mouse and Rabbit\n(D) Dog and Mouse\n(E) Cat and Mouse\n(F) Cat and Rabbit\nOnly give the best option.\nBest option: ("}
{"doc_id": 318, "doc": {"video_name": "test_pet_2.mp4", "duration": 679.6, "question": "What is the main environment shown in the video?\n(A) Factory\n(B) Park\n(C) Zoo\n(D) Street\n(E) Home\n(F) Farm", "candidates": ["Factory", "Park", "Zoo", "Street", "Home", "Farm"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_63", "indices": ["551", "667", "1247", "2320", "2929", "3596", "3973", "4466", "5539", "6061", "6960", "7018", "7946", "8294", "9251", "10150", "10614", "11194", "11426", "12151", "13195", "13601", "14587", "14732", "15834", "15950", "16994", "17661", "17922", "18734", "19691", "19952"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "173c9a352b55d16d9fc0e9c0b01764887bcb955d861fce176ea9c617b089345c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the main environment shown in the video?\n(A) Factory\n(B) Park\n(C) Zoo\n(D) Street\n(E) Home\n(F) Farm", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What is the main environment shown in the video?\n(A) Factory\n(B) Park\n(C) Zoo\n(D) Street\n(E) Home\n(F) Farm\nOnly give the best option.\nBest option: ("}
{"doc_id": 322, "doc": {"video_name": "test_AWG-1.mp4", "duration": 450.04, "question": "In what environment does the scene in the video take place?\n(A) Snow Mountain\n(B) Ocean\n(C) Wilderness\n(D) Rainforest\n(E) City\n(F) Desert", "candidates": ["Snow Mountain", "Ocean", "Wilderness", "Rainforest", "City", "Desert"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_67", "indices": ["75", "500", "875", "1325", "1700", "1775", "2425", "2450", "2975", "3225", "3525", "4025", "4450", "4850", "4975", "5400", "5725", "6300", "6475", "7000", "7250", "7475", "7750", "8100", "8650", "8825", "9225", "9725", "10150", "10450", "10550", "10900"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a6eb1262cd2085e6df79a4a501e3f46cdef29a3fd6277e43e826439084aea9b9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Snow Mountain\n(B) Ocean\n(C) Wilderness\n(D) Rainforest\n(E) City\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what environment does the scene in the video take place?\n(A) Snow Mountain\n(B) Ocean\n(C) Wilderness\n(D) Rainforest\n(E) City\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 326, "doc": {"video_name": "test_entv_2.mp4", "duration": 403.40632292212683, "question": "What genre is this film?\n(A) Adventure\n(B) Mystery\n(C) Thriller\n(D) Action\n(E) Crime\n(F) Comedy", "candidates": ["Adventure", "Mystery", "Thriller", "Action", "Crime", "Comedy"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_71", "indices": ["23", "575", "759", "897", "1288", "1587", "1978", "2093", "2484", "2967", "3059", "3542", "3841", "4025", "4416", "4646", "5014", "5221", "5451", "5934", "6187", "6417", "6647", "7222", "7245", "7797", "8073", "8303", "8533", "8993", "9246", "9522"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Crime"]], "filtered_resps": ["(E) Crime"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3976acc8df3b17c07dbd469107163a3ccf6cdaeec3274a44ef05c95e7e2a00dc", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Adventure\n(B) Mystery\n(C) Thriller\n(D) Action\n(E) Crime\n(F) Comedy", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "A"}, "input": "What genre is this film?\n(A) Adventure\n(B) Mystery\n(C) Thriller\n(D) Action\n(E) Crime\n(F) Comedy\nOnly give the best option.\nBest option: ("}
{"doc_id": 330, "doc": {"video_name": "test_entv_13.mp4", "duration": 678.6181831810065, "question": "In what setting does the video take place?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Forest\n(F) Campus", "candidates": ["Countryside", "Desert", "Ocean", "Urban", "Forest", "Campus"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_75", "indices": ["475", "975", "1500", "1600", "2525", "3000", "3325", "4100", "4650", "5150", "5400", "6200", "6500", "7350", "7875", "8350", "8925", "9100", "9650", "10525", "10650", "11300", "12100", "12150", "13050", "13225", "13875", "14375", "14900", "15475", "16250", "16875"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33eab4c5265fe8ce2e4bfe2a438775b5529fbf53c55d2537a0fc460d501b5a66", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Forest\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what setting does the video take place?\n(A) Countryside\n(B) Desert\n(C) Ocean\n(D) Urban\n(E) Forest\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 334, "doc": {"video_name": "test_cartoon_15.mp4", "duration": 423.52, "question": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) City\n(C) Forest\n(D) Ocean\n(E) Snow Mountain\n(F) Desert", "candidates": ["Grassland", "City", "Forest", "Ocean", "Snow Mountain", "Desert"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_79", "indices": ["116", "551", "783", "1508", "1711", "2059", "2552", "3045", "3335", "3712", "4205", "4437", "4727", "5249", "5655", "6090", "6351", "7018", "7192", "7830", "8004", "8381", "8787", "9454", "9628", "10092", "10585", "10730", "11310", "11832", "11861", "12499"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b20df8b9166269b2f14359b3e9b15599637bf7b8998cd723d8b88855e21913f6", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) City\n(C) Forest\n(D) Ocean\n(E) Snow Mountain\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "B"}, "input": "In what environment does the scene in the video take place?\n(A) Grassland\n(B) City\n(C) Forest\n(D) Ocean\n(E) Snow Mountain\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 338, "doc": {"video_name": "test_cartoon_18.mp4", "duration": 355.39, "question": "What type of film is this?\n(A) Animated film\n(B) Documentary\n(C) Horror film\n(D) Romance film\n(E) War film\n(F) Action film", "candidates": ["Animated film", "Documentary", "Horror film", "Romance film", "War film", "Action film"], "answer": "A", "task_type": "topic_reasoning", "question_id": "TR_83", "indices": ["87", "319", "899", "986", "1566", "1769", "2001", "2494", "2929", "3277", "3364", "3683", "4176", "4466", "4785", "4959", "5336", "5713", "6206", "6612", "6873", "7250", "7511", "7627", "8033", "8613", "8642", "9251", "9425", "9860", "10034", "10614"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d57a7c9005af3e31df83e67a1d9b831d2a359ce75a223d9632598d6a47b087b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Animated film\n(B) Documentary\n(C) Horror film\n(D) Romance film\n(E) War film\n(F) Action film", "task_type": "topic_reasoning", "pred_answer": "A", "answer": "A"}, "input": "What type of film is this?\n(A) Animated film\n(B) Documentary\n(C) Horror film\n(D) Romance film\n(E) War film\n(F) Action film\nOnly give the best option.\nBest option: ("}
{"doc_id": 342, "doc": {"video_name": "test_cartoon_18.mp4", "duration": 355.39, "question": "Who are the main characters in the entire video?\n(A) Dog and Mouse\n(B) Tom and Jerry\n(C) Cat and Duck\n(D) Mouse and Duck\n(E) Dog and Duck\n(F) Cat and Dog", "candidates": ["Dog and Mouse", "Tom and Jerry", "Cat and Duck", "Mouse and Duck", "Dog and Duck", "Cat and Dog"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_87", "indices": ["290", "435", "754", "1189", "1450", "1856", "2088", "2610", "2784", "3045", "3393", "3770", "4002", "4553", "4814", "5104", "5510", "5713", "6177", "6380", "6641", "7221", "7511", "7627", "8207", "8497", "8816", "9135", "9483", "9889", "10266", "10324"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0019ed160e8e12ef7b69da18bb87ee5757aed383feae7801ef151d5cf1a6284b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who are the main characters in the entire video?\n(A) Dog and Mouse\n(B) Tom and Jerry\n(C) Cat and Duck\n(D) Mouse and Duck\n(E) Dog and Duck\n(F) Cat and Dog", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "Who are the main characters in the entire video?\n(A) Dog and Mouse\n(B) Tom and Jerry\n(C) Cat and Duck\n(D) Mouse and Duck\n(E) Dog and Duck\n(F) Cat and Dog\nOnly give the best option.\nBest option: ("}
{"doc_id": 346, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "Where was the phone charger\n(A) in the socket beside the dining\n(B) on the kitchen counter\n(C) in the living room\n(D) on the coffee table\n(E) under the couch\n(F) in the bedroom drawer", "candidates": ["in the socket beside the dining", "on the kitchen counter", "in the living room", "on the coffee table", "under the couch", "in the bedroom drawer"], "answer": "A", "task_type": "ego", "question_id": "ER_0", "indices": ["330", "750", "1320", "1560", "2190", "2310", "2760", "3390", "3750", "4320", "4650", "5130", "5610", "6090", "6660", "7110", "7590", "7650", "8400", "8970", "9000", "9780", "9960", "10590", "11160", "11460", "11760", "12480", "12930", "13440", "13890", "14010"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c61b129f3ee05f3220bd00f4fc6ebf1c89d28af286e04e5f41e22af85682dff", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was the phone charger\n(A) in the socket beside the dining\n(B) on the kitchen counter\n(C) in the living room\n(D) on the coffee table\n(E) under the couch\n(F) in the bedroom drawer", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Where was the phone charger\n(A) in the socket beside the dining\n(B) on the kitchen counter\n(C) in the living room\n(D) on the coffee table\n(E) under the couch\n(F) in the bedroom drawer\nOnly give the best option.\nBest option: ("}
{"doc_id": 350, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "How many bicycles were at the entrance lobby of my apartment\n(A) 3\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 1", "candidates": ["3", "4", "6", "5", "2", "1"], "answer": "E", "task_type": "ego", "question_id": "ER_4", "indices": ["360", "780", "1080", "1650", "2160", "2370", "3120", "3180", "3750", "4110", "4920", "5100", "5790", "5850", "6690", "6990", "7590", "7650", "8220", "8670", "9120", "9840", "9930", "10470", "11190", "11250", "12120", "12390", "12660", "13470", "13920", "14010"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 2"]], "filtered_resps": ["(E) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b83d94638e15f937c5ea26e2db7ba101deed0f8a93b2e77b551b809f8dae16e8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How many bicycles were at the entrance lobby of my apartment\n(A) 3\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 1", "task_type": "ego", "pred_answer": "E", "answer": "E"}, "input": "How many bicycles were at the entrance lobby of my apartment\n(A) 3\n(B) 4\n(C) 6\n(D) 5\n(E) 2\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 354, "doc": {"video_name": "test_ego_5.mp4", "duration": 480.0, "question": "Where was the brown box?\n(A) Behind the black door\n(B) In front of the black door\n(C) In front of the white door\n(D) On top of the black door\n(E) Under the black door\n(F) Next to the black door", "candidates": ["Behind the black door", "In front of the black door", "In front of the white door", "On top of the black door", "Under the black door", "Next to the black door"], "answer": "B", "task_type": "ego", "question_id": "ER_8", "indices": ["150", "540", "1170", "1410", "1830", "2430", "2970", "3360", "3660", "4200", "4920", "5130", "5670", "6090", "6480", "6750", "7530", "7650", "8280", "8700", "9330", "9630", "10080", "10350", "11130", "11580", "11730", "12540", "12630", "13170", "13530", "14250"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "97ec943ca44b7f0a790916f5b5dac926f2871d19761b31c64bfb9e7e202e5839", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the brown box?\n(A) Behind the black door\n(B) In front of the black door\n(C) In front of the white door\n(D) On top of the black door\n(E) Under the black door\n(F) Next to the black door", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the brown box?\n(A) Behind the black door\n(B) In front of the black door\n(C) In front of the white door\n(D) On top of the black door\n(E) Under the black door\n(F) Next to the black door\nOnly give the best option.\nBest option: ("}
{"doc_id": 358, "doc": {"video_name": "test_ego_3.mp4", "duration": 480.0, "question": "Where was the forks?\n(A) in the dishwasher\n(B) in the kitchen drawer.\n(C) in the bedroom\n(D) on the dining table\n(E) in the living room\n(F) in the pantry", "candidates": ["in the dishwasher", "in the kitchen drawer.", "in the bedroom", "on the dining table", "in the living room", "in the pantry"], "answer": "B", "task_type": "ego", "question_id": "ER_12", "indices": ["30", "600", "1050", "1350", "1890", "2250", "3120", "3390", "3960", "4380", "4800", "5160", "5730", "6240", "6330", "7080", "7620", "7950", "8520", "8730", "9000", "9630", "9930", "10590", "10950", "11250", "12060", "12240", "12690", "13170", "13860", "14190"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c01a2b912ff5edcef2eaf46794d0054fd264d8af674d7d75da57671473ae79ac", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Where was the forks?\n(A) in the dishwasher\n(B) in the kitchen drawer.\n(C) in the bedroom\n(D) on the dining table\n(E) in the living room\n(F) in the pantry", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Where was the forks?\n(A) in the dishwasher\n(B) in the kitchen drawer.\n(C) in the bedroom\n(D) on the dining table\n(E) in the living room\n(F) in the pantry\nOnly give the best option.\nBest option: ("}
{"doc_id": 362, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "What colour was the high chair in the dining\n(A) yellow\n(B) blue\n(C) red\n(D) white\n(E) green\n(F) black", "candidates": ["yellow", "blue", "red", "white", "green", "black"], "answer": "D", "task_type": "ego", "question_id": "ER_16", "indices": ["390", "450", "1110", "1380", "1980", "2580", "3120", "3480", "3960", "4110", "4680", "5130", "5820", "6030", "6660", "7170", "7320", "7680", "8460", "8610", "9030", "9870", "9930", "10620", "11070", "11550", "11760", "12150", "12930", "13290", "13800", "14220"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "035e838429b054962f76ec5be34531da651d5067678357c0683a50f84d0d9c02", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What colour was the high chair in the dining\n(A) yellow\n(B) blue\n(C) red\n(D) white\n(E) green\n(F) black", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "What colour was the high chair in the dining\n(A) yellow\n(B) blue\n(C) red\n(D) white\n(E) green\n(F) black\nOnly give the best option.\nBest option: ("}
{"doc_id": 366, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "Where was the scissor?\n(A) In the cupboard beside the fridge.\n(B) On the table next to the lamp.\n(C) Inside the drawer under the desk.\n(D) On the window behind the white rack.\n(E) On the shelf above the television.\n(F) Under the sofa near the window.", "candidates": ["In the cupboard beside the fridge.", "On the table next to the lamp.", "Inside the drawer under the desk.", "On the window behind the white rack.", "On the shelf above the television.", "Under the sofa near the window."], "answer": "D", "task_type": "ego", "question_id": "ER_20", "indices": ["240", "780", "990", "1410", "2130", "2430", "3030", "3360", "3810", "4260", "4860", "5220", "5730", "6270", "6690", "6870", "7200", "7800", "8130", "8670", "9420", "9450", "10140", "10680", "11220", "11610", "11850", "12510", "12660", "13350", "13920", "13950"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Under the sofa near the window."]], "filtered_resps": ["(F) Under the sofa near the window."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b730c19104ce7c4230d30544722e6f970df36c41c0bfd775e7c6089b393dd75f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Where was the scissor?\n(A) In the cupboard beside the fridge.\n(B) On the table next to the lamp.\n(C) Inside the drawer under the desk.\n(D) On the window behind the white rack.\n(E) On the shelf above the television.\n(F) Under the sofa near the window.", "task_type": "ego", "pred_answer": "F", "answer": "D"}, "input": "Where was the scissor?\n(A) In the cupboard beside the fridge.\n(B) On the table next to the lamp.\n(C) Inside the drawer under the desk.\n(D) On the window behind the white rack.\n(E) On the shelf above the television.\n(F) Under the sofa near the window.\nOnly give the best option.\nBest option: ("}
{"doc_id": 370, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "Where did I put the game board?\n(A) On the coffee table\n(B) On the bookshelf\n(C) In the game box.\n(D) In the drawer\n(E) Under the bed\n(F) In the closet", "candidates": ["On the coffee table", "On the bookshelf", "In the game box.", "In the drawer", "Under the bed", "In the closet"], "answer": "C", "task_type": "ego", "question_id": "ER_24", "indices": ["240", "690", "1020", "1440", "2190", "2430", "3030", "3150", "3990", "4320", "4710", "4980", "5400", "6240", "6540", "6960", "7200", "7710", "8490", "8970", "9090", "9510", "9990", "10530", "10800", "11340", "12090", "12510", "12900", "13350", "13680", "14190"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d0c6149fa2c17e619c7eafe81f97a1e7c6cc3ff95bd322a414c2d205cd04f5da", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where did I put the game board?\n(A) On the coffee table\n(B) On the bookshelf\n(C) In the game box.\n(D) In the drawer\n(E) Under the bed\n(F) In the closet", "task_type": "ego", "pred_answer": "A", "answer": "C"}, "input": "Where did I put the game board?\n(A) On the coffee table\n(B) On the bookshelf\n(C) In the game box.\n(D) In the drawer\n(E) Under the bed\n(F) In the closet\nOnly give the best option.\nBest option: ("}
{"doc_id": 374, "doc": {"video_name": "test_ego_8.mp4", "duration": 480.0, "question": "Where was the napkin before I picked it up\n(A) on the table\n(B) in the drawer\n(C) on the countertop\n(D) on the floor\n(E) in the sink\n(F) on the chair", "candidates": ["on the table", "in the drawer", "on the countertop", "on the floor", "in the sink", "on the chair"], "answer": "C", "task_type": "ego", "question_id": "ER_28", "indices": ["420", "660", "1260", "1530", "2190", "2460", "2820", "3210", "3930", "4470", "4710", "5130", "5520", "6000", "6510", "7080", "7440", "8040", "8220", "8850", "9360", "9780", "10080", "10440", "11190", "11670", "11970", "12210", "12600", "13470", "13890", "14250"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddc8b4968f36f2ca6c03db7d1513ff5d8ab2c4c115d8be99d3f4af810ec375d5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Where was the napkin before I picked it up\n(A) on the table\n(B) in the drawer\n(C) on the countertop\n(D) on the floor\n(E) in the sink\n(F) on the chair", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Where was the napkin before I picked it up\n(A) on the table\n(B) in the drawer\n(C) on the countertop\n(D) on the floor\n(E) in the sink\n(F) on the chair\nOnly give the best option.\nBest option: ("}
{"doc_id": 378, "doc": {"video_name": "test_ego_0.mp4", "duration": 480.0, "question": "What did I put in the dustbin?\n(A) Plastic bag\n(B) Food waste\n(C) Metal can\n(D) Paper cup\n(E) Cardboard box\n(F) Glass bottle", "candidates": ["Plastic bag", "Food waste", "Metal can", "Paper cup", "Cardboard box", "Glass bottle"], "answer": "A", "task_type": "ego", "question_id": "ER_32", "indices": ["0", "870", "1080", "1440", "1830", "2640", "2790", "3270", "3690", "4410", "4620", "5340", "5790", "6030", "6630", "7110", "7230", "7950", "8400", "8610", "9030", "9690", "9990", "10590", "10980", "11640", "11880", "12510", "13020", "13050", "13740", "14310"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) Plastic bag"]], "filtered_resps": ["(A) Plastic bag"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71968ffca1199a693e2e8335e7724252ecddfbf110c43dcf3335ac012a6b8dab", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What did I put in the dustbin?\n(A) Plastic bag\n(B) Food waste\n(C) Metal can\n(D) Paper cup\n(E) Cardboard box\n(F) Glass bottle", "task_type": "ego", "pred_answer": "A", "answer": "A"}, "input": "What did I put in the dustbin?\n(A) Plastic bag\n(B) Food waste\n(C) Metal can\n(D) Paper cup\n(E) Cardboard box\n(F) Glass bottle\nOnly give the best option.\nBest option: ("}
{"doc_id": 382, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "In what location did I first see the pack of wipes\n(A) in the living room drawer\n(B) in the hallway closet\n(C) on the kitchen counter\n(D) on the dining table\n(E) in the bathroom cabinet\n(F) on the bedside cupboard", "candidates": ["in the living room drawer", "in the hallway closet", "on the kitchen counter", "on the dining table", "in the bathroom cabinet", "on the bedside cupboard"], "answer": "F", "task_type": "ego", "question_id": "ER_36", "indices": ["390", "450", "900", "1710", "2100", "2460", "2820", "3270", "3630", "4230", "4800", "5040", "5640", "5880", "6450", "7080", "7350", "7920", "8130", "8580", "9030", "9630", "10050", "10770", "11160", "11250", "12060", "12210", "12630", "13140", "13650", "14100"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13760699094913ae7f1a3b3e16db2687c4c8919cc5bc5f81de7c280fb18a3ab4", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In what location did I first see the pack of wipes\n(A) in the living room drawer\n(B) in the hallway closet\n(C) on the kitchen counter\n(D) on the dining table\n(E) in the bathroom cabinet\n(F) on the bedside cupboard", "task_type": "ego", "pred_answer": "F", "answer": "F"}, "input": "In what location did I first see the pack of wipes\n(A) in the living room drawer\n(B) in the hallway closet\n(C) on the kitchen counter\n(D) on the dining table\n(E) in the bathroom cabinet\n(F) on the bedside cupboard\nOnly give the best option.\nBest option: ("}
{"doc_id": 386, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "Where did I put the white earpiece\n(A) on the dining room chair\n(B) on the office desk\n(C) in the bedroom drawer\n(D) in the car console\n(E) on the kitchen table\n(F) on the grey couch", "candidates": ["on the dining room chair", "on the office desk", "in the bedroom drawer", "in the car console", "on the kitchen table", "on the grey couch"], "answer": "F", "task_type": "ego", "question_id": "ER_40", "indices": ["360", "780", "1230", "1440", "2100", "2640", "2700", "3240", "3600", "4350", "4920", "5340", "5430", "5910", "6300", "6780", "7320", "8070", "8100", "8820", "9120", "9840", "10320", "10710", "11190", "11490", "12120", "12150", "12630", "13080", "13710", "14040"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3ca9769c1ac08d662c6aac94e33c73581e19433f20153effdd00f05d2696855a", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where did I put the white earpiece\n(A) on the dining room chair\n(B) on the office desk\n(C) in the bedroom drawer\n(D) in the car console\n(E) on the kitchen table\n(F) on the grey couch", "task_type": "ego", "pred_answer": "E", "answer": "F"}, "input": "Where did I put the white earpiece\n(A) on the dining room chair\n(B) on the office desk\n(C) in the bedroom drawer\n(D) in the car console\n(E) on the kitchen table\n(F) on the grey couch\nOnly give the best option.\nBest option: ("}
{"doc_id": 390, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "Who did I interact with when I fixed the television\n(A) the man in a blue shirt\n(B) the girl with a red hat\n(C) the guy with orange hair\n(D) the boy with a green jacket\n(E) the woman with glasses\n(F) the elderly gentleman with a cane", "candidates": ["the man in a blue shirt", "the girl with a red hat", "the guy with orange hair", "the boy with a green jacket", "the woman with glasses", "the elderly gentleman with a cane"], "answer": "C", "task_type": "ego", "question_id": "ER_44", "indices": ["0", "780", "990", "1560", "2220", "2460", "2880", "3540", "3600", "4290", "4800", "4950", "5670", "6210", "6330", "6840", "7410", "7740", "8490", "8970", "9060", "9600", "9900", "10740", "11190", "11430", "12120", "12270", "12600", "13170", "13770", "14130"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) the guy with orange hair"]], "filtered_resps": ["(C) the guy with orange hair"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96df58a30fa2a62bf722b1a024d8d9fddabe98d01ada8f4855009b0fef0c781a", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who did I interact with when I fixed the television\n(A) the man in a blue shirt\n(B) the girl with a red hat\n(C) the guy with orange hair\n(D) the boy with a green jacket\n(E) the woman with glasses\n(F) the elderly gentleman with a cane", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Who did I interact with when I fixed the television\n(A) the man in a blue shirt\n(B) the girl with a red hat\n(C) the guy with orange hair\n(D) the boy with a green jacket\n(E) the woman with glasses\n(F) the elderly gentleman with a cane\nOnly give the best option.\nBest option: ("}
{"doc_id": 394, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "Where did I put the blue toolbox\n(A) on the floor\n(B) in the closet\n(C) in the garage\n(D) behind the door\n(E) under the table\n(F) on the shelf", "candidates": ["on the floor", "in the closet", "in the garage", "behind the door", "under the table", "on the shelf"], "answer": "A", "task_type": "ego", "question_id": "ER_48", "indices": ["0", "450", "1110", "1650", "2160", "2280", "3120", "3150", "3960", "4410", "4800", "5040", "5520", "5910", "6330", "7170", "7230", "8010", "8160", "8880", "9420", "9720", "9930", "10470", "11100", "11250", "11850", "12240", "12690", "13170", "13800", "14100"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) in the closet"]], "filtered_resps": ["(B) in the closet"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7456a5127567b44be823f2858ef192571a42b30fcf2e18533684dc7374523dcd", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where did I put the blue toolbox\n(A) on the floor\n(B) in the closet\n(C) in the garage\n(D) behind the door\n(E) under the table\n(F) on the shelf", "task_type": "ego", "pred_answer": "B", "answer": "A"}, "input": "Where did I put the blue toolbox\n(A) on the floor\n(B) in the closet\n(C) in the garage\n(D) behind the door\n(E) under the table\n(F) on the shelf\nOnly give the best option.\nBest option: ("}
{"doc_id": 398, "doc": {"video_name": "test_ego_2.mp4", "duration": 388.76666662083335, "question": "How many pans did I see?\n(A) 7\n(B) 5\n(C) 6\n(D) 4\n(E) 3\n(F) 2", "candidates": ["7", "5", "6", "4", "3", "2"], "answer": "D", "task_type": "ego", "question_id": "ER_52", "indices": ["210", "540", "900", "1410", "1440", "1860", "2340", "2520", "3000", "3390", "3720", "3990", "4500", "5040", "5400", "5790", "6030", "6180", "6780", "6930", "7530", "7920", "8250", "8370", "9000", "9180", "9600", "9930", "10500", "10860", "10890", "11610"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 3"]], "filtered_resps": ["(E) 3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e0da647b4a968054bab7bda103d7fb9f25f2ac878d490c63004f859720119197", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many pans did I see?\n(A) 7\n(B) 5\n(C) 6\n(D) 4\n(E) 3\n(F) 2", "task_type": "ego", "pred_answer": "E", "answer": "D"}, "input": "How many pans did I see?\n(A) 7\n(B) 5\n(C) 6\n(D) 4\n(E) 3\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 402, "doc": {"video_name": "test_tutorial_7.mp4", "duration": 438.18, "question": "The sequence in which the woman grinds objects in the container\n(A) Grey granular object, green object, yellow round object\n(B) Yellow round object, green object, grey granular object\n(C) Green object, grey granular object, yellow round object\n(D) Green object, yellow round object, grey granular object\n(E) Grey granular object, yellow round object, green object\n(F) Yellow round object, grey granular object, green object", "candidates": ["Grey granular object, green object, yellow round object", "Yellow round object, green object, grey granular object", "Green object, grey granular object, yellow round object", "Green object, yellow round object, grey granular object", "Grey granular object, yellow round object, green object", "Yellow round object, grey granular object, green object"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_3", "indices": ["0", "480", "1170", "1320", "1710", "2040", "2670", "2910", "3600", "3720", "4470", "4620", "5280", "5550", "6000", "6360", "6660", "6960", "7530", "8100", "8280", "8940", "9030", "9810", "10200", "10230", "11040", "11220", "11610", "12060", "12360", "13110"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d8c89abc639b1a1db743f63a5482364e863d45dfd34c52008739f2fde5571c2f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "The sequence in which the woman grinds objects in the container\n(A) Grey granular object, green object, yellow round object\n(B) Yellow round object, green object, grey granular object\n(C) Green object, grey granular object, yellow round object\n(D) Green object, yellow round object, grey granular object\n(E) Grey granular object, yellow round object, green object\n(F) Yellow round object, grey granular object, green object", "task_type": "tutorialQA", "pred_answer": "B", "answer": "B"}, "input": "The sequence in which the woman grinds objects in the container\n(A) Grey granular object, green object, yellow round object\n(B) Yellow round object, green object, grey granular object\n(C) Green object, grey granular object, yellow round object\n(D) Green object, yellow round object, grey granular object\n(E) Grey granular object, yellow round object, green object\n(F) Yellow round object, grey granular object, green object\nOnly give the best option.\nBest option: ("}
{"doc_id": 406, "doc": {"video_name": "test_tutorial_25.mp4", "duration": 842.27, "question": "In the video, what operation was performed after defining x and y?\n(A) Model fitting\n(B) Parameter estimation\n(C) Data transformation\n(D) Regression analysis\n(E) Plotting\n(F) Hypothesis testing", "candidates": ["Model fitting", "Parameter estimation", "Data transformation", "Regression analysis", "Plotting", "Hypothesis testing"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_7", "indices": ["720", "1350", "1860", "2370", "3900", "4020", "5100", "6120", "6810", "7080", "8010", "8640", "9750", "11010", "11310", "12390", "12840", "13410", "14370", "15030", "16080", "17190", "18000", "18660", "18930", "20250", "20850", "21690", "22530", "22920", "23670", "24720"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27b087103ffbdfba12a4cb9dd398e57b1de174043bd42c0202af9da4246307e9", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what operation was performed after defining x and y?\n(A) Model fitting\n(B) Parameter estimation\n(C) Data transformation\n(D) Regression analysis\n(E) Plotting\n(F) Hypothesis testing", "task_type": "tutorialQA", "pred_answer": "E", "answer": "E"}, "input": "In the video, what operation was performed after defining x and y?\n(A) Model fitting\n(B) Parameter estimation\n(C) Data transformation\n(D) Regression analysis\n(E) Plotting\n(F) Hypothesis testing\nOnly give the best option.\nBest option: ("}
{"doc_id": 410, "doc": {"video_name": "test_tutorial_29.mp4", "duration": 775.45, "question": "What is the order of topics introduced in the video?\n(A) WINDOWS, THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(B) THE INTERFACE, MULTITASKING, WINDOWS, THE FINDER, UNIQUE FEATURES\n(C) THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES, WINDOWS\n(D) THE FINDER, THE INTERFACE, WINDOWS, MULTITASKING, UNIQUE FEATURES\n(E) THE INTERFACE, WINDOWS, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(F) THE INTERFACE, MULTITASKING, THE FINDER, WINDOWS, UNIQUE FEATURES", "candidates": ["WINDOWS, THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES", "THE INTERFACE, MULTITASKING, WINDOWS, THE FINDER, UNIQUE FEATURES", "THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES, WINDOWS", "THE FINDER, THE INTERFACE, WINDOWS, MULTITASKING, UNIQUE FEATURES", "THE INTERFACE, WINDOWS, MULTITASKING, THE FINDER, UNIQUE FEATURES", "THE INTERFACE, MULTITASKING, THE FINDER, WINDOWS, UNIQUE FEATURES"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_11", "indices": ["375", "650", "1750", "2050", "2450", "3325", "3650", "4800", "5225", "5450", "6500", "6675", "7525", "7975", "8525", "9475", "9800", "10275", "11400", "11525", "12275", "12950", "13625", "14400", "14750", "15650", "15725", "16350", "17375", "18100", "18725", "18750"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa0e21499a895c3b053491bc6873cb6173128f9aabf74adc93c69e73d83e44f4", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the order of topics introduced in the video?\n(A) WINDOWS, THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(B) THE INTERFACE, MULTITASKING, WINDOWS, THE FINDER, UNIQUE FEATURES\n(C) THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES, WINDOWS\n(D) THE FINDER, THE INTERFACE, WINDOWS, MULTITASKING, UNIQUE FEATURES\n(E) THE INTERFACE, WINDOWS, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(F) THE INTERFACE, MULTITASKING, THE FINDER, WINDOWS, UNIQUE FEATURES", "task_type": "tutorialQA", "pred_answer": "E", "answer": "E"}, "input": "What is the order of topics introduced in the video?\n(A) WINDOWS, THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(B) THE INTERFACE, MULTITASKING, WINDOWS, THE FINDER, UNIQUE FEATURES\n(C) THE INTERFACE, MULTITASKING, THE FINDER, UNIQUE FEATURES, WINDOWS\n(D) THE FINDER, THE INTERFACE, WINDOWS, MULTITASKING, UNIQUE FEATURES\n(E) THE INTERFACE, WINDOWS, MULTITASKING, THE FINDER, UNIQUE FEATURES\n(F) THE INTERFACE, MULTITASKING, THE FINDER, WINDOWS, UNIQUE FEATURES\nOnly give the best option.\nBest option: ("}
{"doc_id": 414, "doc": {"video_name": "test_tutorial_16.mp4", "duration": 500.02, "question": "The drawing sequence in the video\n(A) Face shape, eyes, nose, ears, mouth, hair\n(B) Face shape, eyes, nose, mouth, ears, hair\n(C) Face shape, ears, eyes, nose, mouth, hair\n(D) Face shape, mouth, ears, eyes, nose, hair\n(E) Face shape, eyes, nose, mouth, hair, ears\n(F) Face shape, eyes, ears, nose, mouth, hair", "candidates": ["Face shape, eyes, nose, ears, mouth, hair", "Face shape, eyes, nose, mouth, ears, hair", "Face shape, ears, eyes, nose, mouth, hair", "Face shape, mouth, ears, eyes, nose, hair", "Face shape, eyes, nose, mouth, hair, ears", "Face shape, eyes, ears, nose, mouth, hair"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_15", "indices": ["144", "360", "768", "1392", "1800", "1944", "2472", "2664", "3312", "3360", "3936", "4344", "4800", "4896", "5568", "5616", "6216", "6432", "6912", "7416", "7776", "7920", "8328", "8736", "9336", "9576", "9792", "10176", "10680", "11016", "11232", "11664"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "29992e529604322836b5dec920c88275e357c330c3b9f00db7b4e0b42f96cbb8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "The drawing sequence in the video\n(A) Face shape, eyes, nose, ears, mouth, hair\n(B) Face shape, eyes, nose, mouth, ears, hair\n(C) Face shape, ears, eyes, nose, mouth, hair\n(D) Face shape, mouth, ears, eyes, nose, hair\n(E) Face shape, eyes, nose, mouth, hair, ears\n(F) Face shape, eyes, ears, nose, mouth, hair", "task_type": "tutorialQA", "pred_answer": "B", "answer": "C"}, "input": "The drawing sequence in the video\n(A) Face shape, eyes, nose, ears, mouth, hair\n(B) Face shape, eyes, nose, mouth, ears, hair\n(C) Face shape, ears, eyes, nose, mouth, hair\n(D) Face shape, mouth, ears, eyes, nose, hair\n(E) Face shape, eyes, nose, mouth, hair, ears\n(F) Face shape, eyes, ears, nose, mouth, hair\nOnly give the best option.\nBest option: ("}
{"doc_id": 418, "doc": {"video_name": "test_tutorial_3.mp4", "duration": 1827.44, "question": "What is the sequence of the man processing the food?\n(A) Pineapple, pear, red round chili, avocado\n(B) Pineapple, avocado, pear, red round chili\n(C) Pineapple, red round chili, pear, avocado\n(D) Pear, red round chili, pineapple, avocado\n(E) Avocado, red round chili, pear, pineapple\n(F) Red round chili, pineapple, pear, avocado", "candidates": ["Pineapple, pear, red round chili, avocado", "Pineapple, avocado, pear, red round chili", "Pineapple, red round chili, pear, avocado", "Pear, red round chili, pineapple, avocado", "Avocado, red round chili, pear, pineapple", "Red round chili, pineapple, pear, avocado"], "answer": "C", "task_type": "tutorialQA", "question_id": "TQA_19", "indices": ["800", "1525", "3825", "5600", "6275", "7400", "8825", "11300", "11500", "14150", "15650", "16025", "18050", "18525", "20975", "22050", "23600", "24375", "26825", "28175", "29200", "30875", "32200", "33100", "35150", "36400", "38175", "38675", "40025", "41425", "44025", "44350"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eca556a66109feb11c3cd21206e0a4599acdfbfc3480de66714571f412689b8b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What is the sequence of the man processing the food?\n(A) Pineapple, pear, red round chili, avocado\n(B) Pineapple, avocado, pear, red round chili\n(C) Pineapple, red round chili, pear, avocado\n(D) Pear, red round chili, pineapple, avocado\n(E) Avocado, red round chili, pear, pineapple\n(F) Red round chili, pineapple, pear, avocado", "task_type": "tutorialQA", "pred_answer": "B", "answer": "C"}, "input": "What is the sequence of the man processing the food?\n(A) Pineapple, pear, red round chili, avocado\n(B) Pineapple, avocado, pear, red round chili\n(C) Pineapple, red round chili, pear, avocado\n(D) Pear, red round chili, pineapple, avocado\n(E) Avocado, red round chili, pear, pineapple\n(F) Red round chili, pineapple, pear, avocado\nOnly give the best option.\nBest option: ("}
{"doc_id": 422, "doc": {"video_name": "test_tutorial_22.mp4", "duration": 1182.94, "question": "What is the sequence of images processed in the video?\n(A) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person wearing a necklace, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(B) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person wearing a necklace, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(C) A photo of a person in a leather jacket, a photo of a person wearing a necklace, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(D) A photo of a person wearing a necklace, a photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(E) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person wearing a necklace, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(F) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person wearing a necklace, a photo of a person in red trousers", "candidates": ["A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person wearing a necklace, a photo of a person with slightly purple lips, a photo of a person in red trousers", "A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person wearing a necklace, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers", "A photo of a person in a leather jacket, a photo of a person wearing a necklace, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers", "A photo of a person wearing a necklace, a photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers", "A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person wearing a necklace, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers", "A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person wearing a necklace, a photo of a person in red trousers"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_23", "indices": ["840", "1740", "3180", "4020", "5220", "6210", "6750", "8100", "9510", "10230", "11370", "13200", "14010", "14880", "16080", "17250", "17760", "18840", "20940", "21360", "22140", "23580", "24390", "26400", "27180", "27690", "29010", "30390", "31770", "32130", "34170", "35310"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "099c3d8b3111a4214231375c60814db574fb11dbc47ee8dfc0384d2944a9d46b", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the sequence of images processed in the video?\n(A) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person wearing a necklace, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(B) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person wearing a necklace, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(C) A photo of a person in a leather jacket, a photo of a person wearing a necklace, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(D) A photo of a person wearing a necklace, a photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(E) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person wearing a necklace, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(F) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person wearing a necklace, a photo of a person in red trousers", "task_type": "tutorialQA", "pred_answer": "B", "answer": "D"}, "input": "What is the sequence of images processed in the video?\n(A) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person wearing a necklace, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(B) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person wearing a necklace, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(C) A photo of a person in a leather jacket, a photo of a person wearing a necklace, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(D) A photo of a person wearing a necklace, a photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(E) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person wearing a necklace, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person in red trousers\n(F) A photo of a person in a leather jacket, a photo of a person with a beard, a photo of a person in a denim jacket, a photo of a person in a white sleeveless top, a photo of a person with slightly purple lips, a photo of a person wearing a necklace, a photo of a person in red trousers\nOnly give the best option.\nBest option: ("}
{"doc_id": 426, "doc": {"video_name": "test_tutorial_30.mp4", "duration": 704.07, "question": "Which application is opened after 'App Store' in the video?\n(A) Health\n(B) Camera\n(C) Mail\n(D) Calendar\n(E) TV\n(F) Photos", "candidates": ["Health", "Camera", "Mail", "Calendar", "TV", "Photos"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_27", "indices": ["600", "1020", "1650", "2580", "2850", "3330", "4410", "5070", "5790", "6330", "6690", "7290", "8430", "8580", "9300", "10470", "10620", "11520", "12180", "12780", "13350", "14490", "14730", "15780", "16230", "16980", "17760", "17820", "18810", "19140", "20430", "20850"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c927b6ce06c65b2a7d8a5f2dc3c18a7448a716ffd8f29cf0a96ba5f0cbfee625", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which application is opened after 'App Store' in the video?\n(A) Health\n(B) Camera\n(C) Mail\n(D) Calendar\n(E) TV\n(F) Photos", "task_type": "tutorialQA", "pred_answer": "B", "answer": "A"}, "input": "Which application is opened after 'App Store' in the video?\n(A) Health\n(B) Camera\n(C) Mail\n(D) Calendar\n(E) TV\n(F) Photos\nOnly give the best option.\nBest option: ("}
{"doc_id": 430, "doc": {"video_name": "test_tutorial_18.mp4", "duration": 405.98, "question": "Sequence of tools used in the video\n(A) Brush, sponge, air pump, scissors, tape, glue\n(B) Brush, sponge, scissors, tape, air pump, glue\n(C) Brush, sponge, scissors, air pump, tape, glue\n(D) Brush, sponge, scissors, tape, glue, air pump\n(E) Brush, air pump, sponge, scissors, tape, glue\n(F) Air pump, brush, sponge, scissors, tape, glue", "candidates": ["Brush, sponge, air pump, scissors, tape, glue", "Brush, sponge, scissors, tape, air pump, glue", "Brush, sponge, scissors, air pump, tape, glue", "Brush, sponge, scissors, tape, glue, air pump", "Brush, air pump, sponge, scissors, tape, glue", "Air pump, brush, sponge, scissors, tape, glue"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_31", "indices": ["290", "725", "841", "1160", "1624", "1972", "2610", "2842", "3335", "3567", "3944", "4408", "4727", "5104", "5597", "5684", "6293", "6699", "6815", "7192", "7598", "8294", "8323", "8787", "9106", "9483", "9918", "10585", "10759", "10991", "11426", "11745"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "851c290f17ae4bee4e06bf0d8f07d877a2b905717ae57fef26d2a78836c9bde2", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Sequence of tools used in the video\n(A) Brush, sponge, air pump, scissors, tape, glue\n(B) Brush, sponge, scissors, tape, air pump, glue\n(C) Brush, sponge, scissors, air pump, tape, glue\n(D) Brush, sponge, scissors, tape, glue, air pump\n(E) Brush, air pump, sponge, scissors, tape, glue\n(F) Air pump, brush, sponge, scissors, tape, glue", "task_type": "tutorialQA", "pred_answer": "B", "answer": "F"}, "input": "Sequence of tools used in the video\n(A) Brush, sponge, air pump, scissors, tape, glue\n(B) Brush, sponge, scissors, tape, air pump, glue\n(C) Brush, sponge, scissors, air pump, tape, glue\n(D) Brush, sponge, scissors, tape, glue, air pump\n(E) Brush, air pump, sponge, scissors, tape, glue\n(F) Air pump, brush, sponge, scissors, tape, glue\nOnly give the best option.\nBest option: ("}
{"doc_id": 434, "doc": {"video_name": "test_tutorial_25.mp4", "duration": 842.27, "question": "What operations were introduced in the video?\n(A) Indexing operations, loop statements, concurrency and parallelism\n(B) Indexing operations, if-else statements, loop statements\n(C) Indexing operations, loop statements, string operations\n(D) Indexing operations, loop statements, file operations\n(E) Indexing operations, loop statements, exception handling\n(F) Indexing operations, loop statements, network programming", "candidates": ["Indexing operations, loop statements, concurrency and parallelism", "Indexing operations, if-else statements, loop statements", "Indexing operations, loop statements, string operations", "Indexing operations, loop statements, file operations", "Indexing operations, loop statements, exception handling", "Indexing operations, loop statements, network programming"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_35", "indices": ["720", "960", "1920", "2910", "3900", "4020", "5130", "5760", "6840", "7560", "7980", "8640", "9690", "10980", "11310", "12150", "12900", "14100", "14370", "15090", "16080", "16920", "18000", "18300", "19290", "19710", "20820", "21780", "22530", "22920", "24240", "24450"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0534dd358e38d55c7be91e97403b53222bf6e1df33d003e8d5396edd68bbdc35", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What operations were introduced in the video?\n(A) Indexing operations, loop statements, concurrency and parallelism\n(B) Indexing operations, if-else statements, loop statements\n(C) Indexing operations, loop statements, string operations\n(D) Indexing operations, loop statements, file operations\n(E) Indexing operations, loop statements, exception handling\n(F) Indexing operations, loop statements, network programming", "task_type": "tutorialQA", "pred_answer": "B", "answer": "B"}, "input": "What operations were introduced in the video?\n(A) Indexing operations, loop statements, concurrency and parallelism\n(B) Indexing operations, if-else statements, loop statements\n(C) Indexing operations, loop statements, string operations\n(D) Indexing operations, loop statements, file operations\n(E) Indexing operations, loop statements, exception handling\n(F) Indexing operations, loop statements, network programming\nOnly give the best option.\nBest option: ("}
{"doc_id": 438, "doc": {"video_name": "test_tutorial_12.mp4", "duration": 279.38, "question": "The sequence of actions in the origami process in the video\n(A) Adjusting the shape of the strip, cutting the paper, applying glue, fixing the paper, applying glue, fixing the strip\n(B) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the paper, fixing the strip\n(C) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the strip, fixing the paper\n(D) Cutting the paper, applying glue, fixing the paper, adjusting the shape of the strip, applying glue, fixing the strip\n(E) Cutting the paper, adjusting the shape of the strip, applying glue, fixing the paper, applying glue, fixing the strip\n(F) Adjusting the shape of the strip, applying glue, fixing the strip, cutting the paper, applying glue, fixing the paper", "candidates": ["Adjusting the shape of the strip, cutting the paper, applying glue, fixing the paper, applying glue, fixing the strip", "Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the paper, fixing the strip", "Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the strip, fixing the paper", "Cutting the paper, applying glue, fixing the paper, adjusting the shape of the strip, applying glue, fixing the strip", "Cutting the paper, adjusting the shape of the strip, applying glue, fixing the paper, applying glue, fixing the strip", "Adjusting the shape of the strip, applying glue, fixing the strip, cutting the paper, applying glue, fixing the paper"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_39", "indices": ["210", "240", "540", "750", "1140", "1500", "1740", "1950", "2070", "2550", "2670", "3000", "3330", "3510", "3810", "4110", "4200", "4410", "4740", "5010", "5250", "5580", "5940", "6180", "6450", "6510", "6840", "7140", "7320", "7800", "8070", "8250"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d60aa56b5d7f09292960b930d7ffe582b1064fcfa91a0f9657cfd2329ab548c8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "The sequence of actions in the origami process in the video\n(A) Adjusting the shape of the strip, cutting the paper, applying glue, fixing the paper, applying glue, fixing the strip\n(B) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the paper, fixing the strip\n(C) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the strip, fixing the paper\n(D) Cutting the paper, applying glue, fixing the paper, adjusting the shape of the strip, applying glue, fixing the strip\n(E) Cutting the paper, adjusting the shape of the strip, applying glue, fixing the paper, applying glue, fixing the strip\n(F) Adjusting the shape of the strip, applying glue, fixing the strip, cutting the paper, applying glue, fixing the paper", "task_type": "tutorialQA", "pred_answer": "E", "answer": "D"}, "input": "The sequence of actions in the origami process in the video\n(A) Adjusting the shape of the strip, cutting the paper, applying glue, fixing the paper, applying glue, fixing the strip\n(B) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the paper, fixing the strip\n(C) Cutting the paper, applying glue, adjusting the shape of the strip, applying glue, fixing the strip, fixing the paper\n(D) Cutting the paper, applying glue, fixing the paper, adjusting the shape of the strip, applying glue, fixing the strip\n(E) Cutting the paper, adjusting the shape of the strip, applying glue, fixing the paper, applying glue, fixing the strip\n(F) Adjusting the shape of the strip, applying glue, fixing the strip, cutting the paper, applying glue, fixing the paper\nOnly give the best option.\nBest option: ("}
{"doc_id": 442, "doc": {"video_name": "test_count_18.mp4", "duration": 673.0500000000001, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n(E) 0\n(F) 4", "candidates": ["1", "2", "5", "6", "0", "4"], "answer": "F", "task_type": "count", "question_id": "AC_0", "indices": ["145", "174", "203", "232", "261", "290", "319", "348", "377", "406", "3335", "3422", "3451", "3480", "3509", "3538", "5887", "5916", "5945", "5974", "6003", "6032", "6061", "6090", "6119", "6148", "6177", "16878", "16907", "16936", "17110", "17139"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b87c16d244340a7618987283cb8cc78a0697536d07cf51e02139b2f21ca826f2", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n(E) 0\n(F) 4", "task_type": "count", "pred_answer": "D", "answer": "F"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'pole vault' action\n(A) 1\n(B) 2\n(C) 5\n(D) 6\n(E) 0\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 446, "doc": {"video_name": "test_count_32.mp4", "duration": 810.39, "question": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n(E) 6\n(F) 2", "candidates": ["1", "0", "5", "3", "6", "2"], "answer": "E", "task_type": "count", "question_id": "AC_4", "indices": ["5610", "5640", "5670", "5700", "5730", "5760", "5790", "5820", "5850", "5880", "7800", "7830", "7860", "15300", "15330", "15390", "15420", "15450", "15480", "21480", "21540", "21720", "22260", "22290", "22320", "22350", "22380", "22410", "22440", "22470", "22500", "22530"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84b7216efa863fe3130fc528099d56f117027223c2d85ff56951a429391c305c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n(E) 6\n(F) 2", "task_type": "count", "pred_answer": "D", "answer": "E"}, "input": "In this video, how many times does the scene of the 'carving pumpkin' action appear in total?\n(A) 1\n(B) 0\n(C) 5\n(D) 3\n(E) 6\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 450, "doc": {"video_name": "test_count_43.mp4", "duration": 510.03, "question": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 5\n(C) 4\n(D) 3\n(E) 6\n(F) 0", "candidates": ["1", "5", "4", "3", "6", "0"], "answer": "D", "task_type": "count", "question_id": "AC_8", "indices": ["290", "522", "1160", "1537", "1914", "2349", "3190", "3741", "3857", "4437", "5104", "5336", "5887", "6293", "7018", "7482", "7627", "8555", "8642", "9164", "9976", "10034", "10585", "11310", "11571", "12093", "12499", "13079", "13630", "14123", "14500", "14935"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02961173afd5057845dab36406327e5588043cb453c26555a1b24ee9d49eaba5", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 5\n(C) 4\n(D) 3\n(E) 6\n(F) 0", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many times does the scene of the 'javelin throw' action appear in total?\n(A) 1\n(B) 5\n(C) 4\n(D) 3\n(E) 6\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 454, "doc": {"video_name": "test_count_26.mp4", "duration": 453.57, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 6\n(E) 5\n(F) 1", "candidates": ["2", "4", "3", "6", "5", "1"], "answer": "C", "task_type": "count", "question_id": "AC_12", "indices": ["29", "754", "870", "1566", "2030", "2088", "2610", "3132", "3741", "3973", "4321", "4988", "5423", "5539", "6264", "6757", "6960", "7598", "8004", "8149", "8845", "9048", "9425", "10150", "10266", "10846", "11049", "11484", "12006", "12615", "12702", "13485"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d86a7cc46f7e6020dee5fd0aa735fccdcdb354dc3d313ebf8052c2881d2924d8", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 6\n(E) 5\n(F) 1", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 2\n(B) 4\n(C) 3\n(D) 6\n(E) 5\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 458, "doc": {"video_name": "test_count_51.mp4", "duration": 643.0, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 1\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 0", "candidates": ["1", "2", "4", "3", "6", "0"], "answer": "E", "task_type": "count", "question_id": "AC_16", "indices": ["1320", "4950", "4980", "5010", "8610", "8640", "8670", "8700", "8730", "8760", "8790", "8820", "8850", "8880", "9300", "9330", "9420", "9450", "9480", "9510", "9540", "9570", "14010", "14040", "14070", "14100", "14130", "14160", "14190", "14220", "14250", "14280"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2544b0d4f0d1e52ef62d04a43b17fab89646e7363952c7b7ff2ff1d77bec2d4a", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 1\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 0", "task_type": "count", "pred_answer": "E", "answer": "E"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'zumba' action\n(A) 1\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 462, "doc": {"video_name": "test_count_53.mp4", "duration": 484.15, "question": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 5\n(B) 0\n(C) 4\n(D) 1\n(E) 2\n(F) 3", "candidates": ["5", "0", "4", "1", "2", "3"], "answer": "C", "task_type": "count", "question_id": "AC_20", "indices": ["29", "725", "1131", "1682", "1943", "2494", "3045", "3393", "3654", "4089", "4814", "5220", "5684", "5945", "6409", "7047", "7453", "7888", "8149", "8990", "9135", "9918", "10353", "10817", "11281", "11629", "12035", "12557", "12760", "13543", "13572", "14239"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2386d9c214e485e2e0b54ae57586b9444353bf0b16a90485bad6d8d2385562b9", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 5\n(B) 0\n(C) 4\n(D) 1\n(E) 2\n(F) 3", "task_type": "count", "pred_answer": "E", "answer": "C"}, "input": "In this video, how many times does the scene of the 'zumba' action appear in total?\n(A) 5\n(B) 0\n(C) 4\n(D) 1\n(E) 2\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 466, "doc": {"video_name": "test_count_59.mp4", "duration": 750.8299999999999, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "candidates": ["1", "6", "4", "2", "5", "0"], "answer": "B", "task_type": "count", "question_id": "AC_24", "indices": ["90", "1200", "2040", "2460", "3270", "3690", "4470", "5400", "6120", "6960", "7020", "8130", "9060", "9600", "9900", "11160", "11490", "12210", "12840", "13710", "14580", "15240", "16080", "16710", "17160", "17850", "18690", "19260", "19800", "20490", "21600", "21930"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c6d961d32f585c53398250b40b9f1712dc6bc4bb23e7d7951be2c9e1d7aa2da", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "task_type": "count", "pred_answer": "D", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'baking cookies' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 470, "doc": {"video_name": "test_count_27.mp4", "duration": 470.01, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 4\n(E) 2\n(F) 1", "candidates": ["3", "6", "0", "4", "2", "1"], "answer": "E", "task_type": "count", "question_id": "AC_28", "indices": ["330", "570", "1260", "1380", "1980", "2550", "2880", "3060", "3750", "3960", "4410", "5070", "5310", "5850", "6450", "6900", "7320", "7800", "8250", "8760", "8970", "9480", "9900", "10350", "10560", "11010", "11670", "11880", "12510", "12990", "13290", "14070"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "172e6d2b2d4db3c1edd206f54293fd9bae1d11901ada3d69bb291b3d7a8ddffa", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 4\n(E) 2\n(F) 1", "task_type": "count", "pred_answer": "F", "answer": "E"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 3\n(B) 6\n(C) 0\n(D) 4\n(E) 2\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 474, "doc": {"video_name": "test_count_35.mp4", "duration": 605.88, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'stomping grapes' action\n(A) 6\n(B) 2\n(C) 1\n(D) 0\n(E) 4\n(F) 3", "candidates": ["6", "2", "1", "0", "4", "3"], "answer": "C", "task_type": "count", "question_id": "AC_32", "indices": ["125", "625", "950", "1400", "1975", "2500", "2900", "3375", "3925", "4625", "5000", "5500", "5775", "6350", "6650", "7325", "7700", "8375", "8850", "9375", "9450", "9950", "10675", "11175", "11675", "12025", "12575", "13050", "13275", "14100", "14450", "14650"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03812c81c803ee9f4eeaa6f062d4f9c21c02dfa0ab9d48ebd956d55122205bae", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'stomping grapes' action\n(A) 6\n(B) 2\n(C) 1\n(D) 0\n(E) 4\n(F) 3", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'stomping grapes' action\n(A) 6\n(B) 2\n(C) 1\n(D) 0\n(E) 4\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 478, "doc": {"video_name": "test_count_9.mp4", "duration": 495.43, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n(E) 5\n(F) 6", "candidates": ["3", "4", "2", "1", "5", "6"], "answer": "E", "task_type": "count", "question_id": "AC_36", "indices": ["330", "720", "1020", "1500", "2220", "2490", "3120", "3420", "3750", "4500", "4680", "5250", "5610", "6300", "6570", "7350", "7830", "8310", "8730", "8820", "9420", "9990", "10530", "10980", "11400", "11910", "12180", "12840", "13170", "13680", "14310", "14490"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 2"]], "filtered_resps": ["(C) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "883c6eb8511704c9671c6ef95a98308fa0e81ca312a289263342af852d164ba7", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n(E) 5\n(F) 6", "task_type": "count", "pred_answer": "C", "answer": "E"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'water sliding' action\n(A) 3\n(B) 4\n(C) 2\n(D) 1\n(E) 5\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 482, "doc": {"video_name": "test_count_50.mp4", "duration": 760.34, "question": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 5\n(B) 1\n(C) 3\n(D) 4\n(E) 0\n(F) 6", "candidates": ["5", "1", "3", "4", "0", "6"], "answer": "B", "task_type": "count", "question_id": "AC_40", "indices": ["377", "928", "1769", "2784", "3364", "3741", "4727", "5481", "6003", "6380", "7714", "8497", "8526", "9628", "10266", "10672", "11890", "12615", "12876", "13659", "14442", "14906", "15863", "17023", "17516", "18299", "18966", "19720", "20387", "20590", "21779", "22707"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fd1888b7770ff26f0bc259bf0912a27ba95f5548349b778e9c60b86466529ac9", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 5\n(B) 1\n(C) 3\n(D) 4\n(E) 0\n(F) 6", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'cooking sausages' action appear in total?\n(A) 5\n(B) 1\n(C) 3\n(D) 4\n(E) 0\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 486, "doc": {"video_name": "test_count_31.mp4", "duration": 405.47999999999996, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 3\n(D) 0\n(E) 2\n(F) 5", "candidates": ["4", "6", "3", "0", "2", "5"], "answer": "F", "task_type": "count", "question_id": "AC_44", "indices": ["720", "750", "780", "810", "840", "870", "960", "990", "2430", "2460", "2490", "2520", "2550", "2580", "2610", "2640", "2670", "2700", "7290", "7320", "7350", "7380", "7410", "7440", "7500", "7560", "9960", "9990", "10020", "11790", "11820", "11940"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7fbe4fd8d331a681a5d92620d69325114d5d76beb8df2af96c1cc3b3690af8af", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 3\n(D) 0\n(E) 2\n(F) 5", "task_type": "count", "pred_answer": "A", "answer": "F"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'carving pumpkin' action\n(A) 4\n(B) 6\n(C) 3\n(D) 0\n(E) 2\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 490, "doc": {"video_name": "test_count_54.mp4", "duration": 659.74, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 3\n(B) 0\n(C) 6\n(D) 2\n(E) 1\n(F) 5", "candidates": ["3", "0", "6", "2", "1", "5"], "answer": "D", "task_type": "count", "question_id": "AC_48", "indices": ["580", "667", "1392", "2320", "2523", "3335", "4205", "4698", "5481", "5568", "6583", "6931", "7946", "8294", "8642", "9425", "10121", "10672", "11629", "11687", "12470", "13340", "14007", "14616", "14819", "15892", "16472", "17139", "17777", "18328", "18589", "19111"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3707f13649a00124c3ef8fd2a6d8c4391981e064aef612a26c1b4370a837aa49", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 3\n(B) 0\n(C) 6\n(D) 2\n(E) 1\n(F) 5", "task_type": "count", "pred_answer": "D", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'paragliding' action\n(A) 3\n(B) 0\n(C) 6\n(D) 2\n(E) 1\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 494, "doc": {"video_name": "test_count_36.mp4", "duration": 470.0, "question": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 5\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 1", "candidates": ["5", "2", "4", "3", "6", "1"], "answer": "B", "task_type": "count", "question_id": "AC_52", "indices": ["270", "570", "1080", "1590", "2010", "2490", "2610", "3450", "3900", "4020", "4440", "5130", "5280", "5700", "6570", "6900", "7410", "7500", "8040", "8460", "8820", "9600", "9960", "10470", "10950", "11070", "11820", "12270", "12690", "13140", "13200", "13950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 2"]], "filtered_resps": ["(B) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "83c394667082ad6ffd546d7655699d37c121c25623bc6c96cc7002b74e769bd8", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 5\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 1", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many times does the scene of the 'playing trombone' action appear in total?\n(A) 5\n(B) 2\n(C) 4\n(D) 3\n(E) 6\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 498, "doc": {"video_name": "test_count_10.mp4", "duration": 702.67, "question": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 1\n(B) 6\n(C) 5\n(D) 2\n(E) 4\n(F) 0", "candidates": ["1", "6", "5", "2", "4", "0"], "answer": "E", "task_type": "count", "question_id": "AC_56", "indices": ["510", "1200", "1380", "2040", "3000", "3780", "3960", "4770", "5610", "6510", "6570", "7500", "8430", "8940", "9330", "10140", "10680", "11790", "11910", "12660", "13320", "13830", "15090", "15750", "15900", "16440", "17580", "17820", "18600", "19170", "20070", "20520"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A", "A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ea7f4328c28417fd74254343171e36d8c026c052d95cfefc6e48c10ca3f3ba6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 1\n(B) 6\n(C) 5\n(D) 2\n(E) 4\n(F) 0", "task_type": "count", "pred_answer": "A", "answer": "E"}, "input": "In this video, how many times does the scene of the 'water sliding' action appear in total?\n(A) 1\n(B) 6\n(C) 5\n(D) 2\n(E) 4\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 3, "doc": {"video_name": "test_surveil_1.mp4", "duration": 419.73, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Vandalism\n(E) Shoplifting\n(F) Arson", "candidates": ["Fighting", "RoadAccidents", "Abuse", "Vandalism", "Shoplifting", "Arson"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_3", "indices": ["360", "420", "960", "1260", "1560", "2220", "2520", "2790", "3150", "3570", "4170", "4590", "4980", "5100", "5580", "5850", "6450", "6840", "7050", "7800", "8010", "8370", "8970", "9120", "9540", "9990", "10560", "10770", "11250", "11610", "11760", "12450"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Shoplifting"]], "filtered_resps": ["(E) Shoplifting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "32df23122a63869300746687aab5f8fd475550df9623197cdb850c8a04c54639", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Vandalism\n(E) Shoplifting\n(F) Arson", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "D"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Fighting\n(B) RoadAccidents\n(C) Abuse\n(D) Vandalism\n(E) Shoplifting\n(F) Arson\nOnly give the best option.\nBest option: ("}
{"doc_id": 7, "doc": {"video_name": "test_surveil_12.mp4", "duration": 220.07999999999998, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Robbery\n(D) Fighting\n(E) Shooting\n(F) RoadAccidents", "candidates": ["Burglary", "Shooting", "Robbery", "Fighting", "Shooting", "RoadAccidents"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_7", "indices": ["0", "360", "570", "780", "840", "1020", "1230", "1470", "1680", "1890", "2070", "2280", "2640", "2670", "3030", "3270", "3300", "3480", "3840", "3960", "4290", "4320", "4650", "4770", "5040", "5250", "5340", "5700", "5820", "6150", "6270", "6510"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "923755b63219b06ececbbb2f786aa479bfaf87c7d2f45c8720547621f2bba3d0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Robbery\n(D) Fighting\n(E) Shooting\n(F) RoadAccidents", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Burglary\n(B) Shooting\n(C) Robbery\n(D) Fighting\n(E) Shooting\n(F) RoadAccidents\nOnly give the best option.\nBest option: ("}
{"doc_id": 11, "doc": {"video_name": "test_surveil_6.mp4", "duration": 375.26, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Vandalism\n(B) RoadAccidents\n(C) Robbery\n(D) Stealing\n(E) Shooting\n(F) Burglary", "candidates": ["Vandalism", "RoadAccidents", "Robbery", "Stealing", "Shooting", "Burglary"], "answer": "F", "task_type": "anomaly_reco", "question_id": "AR_11", "indices": ["180", "420", "780", "1080", "1680", "1860", "2220", "2490", "2820", "3270", "3690", "3990", "4440", "4680", "5040", "5370", "5850", "5970", "6360", "6840", "7140", "7350", "8010", "8220", "8670", "9000", "9180", "9600", "10140", "10260", "10800", "11160"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Stealing"]], "filtered_resps": ["(D) Stealing"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9224d4751f6da7c6926612b21d771378abbcdde209f10556f98e17764348b04a", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Vandalism\n(B) RoadAccidents\n(C) Robbery\n(D) Stealing\n(E) Shooting\n(F) Burglary", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "F"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Vandalism\n(B) RoadAccidents\n(C) Robbery\n(D) Stealing\n(E) Shooting\n(F) Burglary\nOnly give the best option.\nBest option: ("}
{"doc_id": 15, "doc": {"video_name": "test_surveil_7.mp4", "duration": 196.27, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Robbery\n(C) Stealing\n(D) Burglary\n(E) Explosion\n(F) Shooting", "candidates": ["Fighting", "Robbery", "Stealing", "Burglary", "Explosion", "Shooting"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_15", "indices": ["150", "270", "510", "660", "840", "900", "1170", "1440", "1560", "1770", "1950", "2100", "2340", "2400", "2640", "2880", "3030", "3240", "3360", "3600", "3720", "3840", "4080", "4230", "4440", "4680", "4920", "5070", "5280", "5400", "5610", "5670"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Robbery"]], "filtered_resps": ["(B) Robbery"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ec82c5888898db3c1d066a6bac4757d5846429bddea38bfa5e042bc7106d6b4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Robbery\n(C) Stealing\n(D) Burglary\n(E) Explosion\n(F) Shooting", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "D"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Fighting\n(B) Robbery\n(C) Stealing\n(D) Burglary\n(E) Explosion\n(F) Shooting\nOnly give the best option.\nBest option: ("}
{"doc_id": 19, "doc": {"video_name": "test_surveil_24.mp4", "duration": 251.2, "question": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Normal\n(C) Shooting\n(D) Robbery\n(E) Assault\n(F) Fighting", "candidates": ["Explosion", "Normal", "Shooting", "Robbery", "Assault", "Fighting"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_19", "indices": ["0", "270", "600", "690", "990", "1320", "1410", "1830", "1860", "2190", "2520", "2640", "2790", "3210", "3300", "3630", "3930", "4020", "4260", "4620", "4890", "4920", "5220", "5490", "5670", "6000", "6210", "6480", "6690", "6900", "7170", "7440"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Assault"]], "filtered_resps": ["(E) Assault"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0314e8bb9eaa2a4e0ad374e795920dd3d99ed97c7da53e32f658e3a0b5a7b805", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Normal\n(C) Shooting\n(D) Robbery\n(E) Assault\n(F) Fighting", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "E"}, "input": "Are there any irregularities in this surveillance video? If there are, what sort are they?\n(A) Explosion\n(B) Normal\n(C) Shooting\n(D) Robbery\n(E) Assault\n(F) Fighting\nOnly give the best option.\nBest option: ("}
{"doc_id": 23, "doc": {"video_name": "test_surveil_3.mp4", "duration": 208.45, "question": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Arson\n(E) Shoplifting\n(F) Vandalism", "candidates": ["Normal", "Shooting", "Shooting", "Arson", "Shoplifting", "Vandalism"], "answer": "D", "task_type": "anomaly_reco", "question_id": "AR_23", "indices": ["0", "360", "480", "750", "930", "1050", "1320", "1530", "1560", "1770", "2010", "2280", "2370", "2670", "2850", "3000", "3150", "3360", "3570", "3690", "3900", "4260", "4290", "4470", "4680", "4920", "5100", "5250", "5580", "5790", "5970", "6030"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Arson"]], "filtered_resps": ["(D) Arson"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e086dc36274312bc463ddeaf438a490c2968eae7ddd0cda328303f6c22ae0ef8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Arson\n(E) Shoplifting\n(F) Vandalism", "task_type": "anomaly_reco", "pred_answer": "D", "answer": "D"}, "input": "Is there any abnormality in this surveillance video? If so, what type of abnormality is it?\n(A) Normal\n(B) Shooting\n(C) Shooting\n(D) Arson\n(E) Shoplifting\n(F) Vandalism\nOnly give the best option.\nBest option: ("}
{"doc_id": 27, "doc": {"video_name": "test_surveil_37.mp4", "duration": 276.23, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Robbery\n(C) Vandalism\n(D) Shoplifting\n(E) Shooting\n(F) Arson", "candidates": ["Arrest", "Robbery", "Vandalism", "Shoplifting", "Shooting", "Arson"], "answer": "E", "task_type": "anomaly_reco", "question_id": "AR_27", "indices": ["30", "480", "600", "750", "1140", "1470", "1710", "1860", "2220", "2310", "2730", "2910", "3180", "3540", "3840", "4110", "4290", "4410", "4710", "5100", "5280", "5640", "5880", "6150", "6390", "6630", "6930", "7200", "7440", "7560", "7860", "8160"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Shooting"]], "filtered_resps": ["(E) Shooting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8f19025f623f011f02e47bde59dcffcc2848cc7ebd6e2aa3291941f8da01a25e", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Robbery\n(C) Vandalism\n(D) Shoplifting\n(E) Shooting\n(F) Arson", "task_type": "anomaly_reco", "pred_answer": "E", "answer": "E"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) Arrest\n(B) Robbery\n(C) Vandalism\n(D) Shoplifting\n(E) Shooting\n(F) Arson\nOnly give the best option.\nBest option: ("}
{"doc_id": 31, "doc": {"video_name": "test_surveil_11.mp4", "duration": 255.0, "question": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Explosion\n(D) Arrest\n(E) Arson\n(F) Vandalism", "candidates": ["Robbery", "Shooting", "Explosion", "Arrest", "Arson", "Vandalism"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_31", "indices": ["120", "240", "600", "870", "930", "1170", "1470", "1650", "1980", "2160", "2550", "2730", "2970", "3270", "3480", "3660", "3900", "4200", "4350", "4590", "4860", "5100", "5430", "5520", "5760", "5940", "6210", "6420", "6810", "7110", "7290", "7470"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Shooting"]], "filtered_resps": ["(B) Shooting"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82067231e3f5ecb23330aaecdcb83967c12da865e08a1c3a8b8dbcdb4abb3cd", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Explosion\n(D) Arrest\n(E) Arson\n(F) Vandalism", "task_type": "anomaly_reco", "pred_answer": "B", "answer": "C"}, "input": "Is there anything unusual in this surveillance video? If there is, what type of unusual activity is it?\n(A) Robbery\n(B) Shooting\n(C) Explosion\n(D) Arrest\n(E) Arson\n(F) Vandalism\nOnly give the best option.\nBest option: ("}
{"doc_id": 35, "doc": {"video_name": "test_surveil_16.mp4", "duration": 280.29, "question": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Arson\n(C) Abuse\n(D) Arrest\n(E) Burglary\n(F) Normal", "candidates": ["RoadAccidents", "Arson", "Abuse", "Arrest", "Burglary", "Normal"], "answer": "C", "task_type": "anomaly_reco", "question_id": "AR_35", "indices": ["0", "330", "750", "930", "1080", "1530", "1800", "1830", "2250", "2490", "2790", "2940", "3180", "3450", "3750", "3990", "4380", "4440", "4860", "4980", "5250", "5670", "6000", "6210", "6360", "6720", "6870", "7110", "7530", "7710", "8100", "8130"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Abuse"]], "filtered_resps": ["(C) Abuse"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4efbdf6cfac616bc94e3bef86400e000996dc7f19b10e6ff620e3f81bad2145f", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Arson\n(C) Abuse\n(D) Arrest\n(E) Burglary\n(F) Normal", "task_type": "anomaly_reco", "pred_answer": "C", "answer": "C"}, "input": "Does this surveillance footage contain any anomalies? If yes, which kind of anomaly?\n(A) RoadAccidents\n(B) Arson\n(C) Abuse\n(D) Arrest\n(E) Burglary\n(F) Normal\nOnly give the best option.\nBest option: ("}
{"doc_id": 39, "doc": {"video_name": "test_sports_10.mp4", "duration": 1271.68, "question": "Why was the player in black jersey number 10 given a yellow card?\n(A) Technical foul\n(B) Tackling foul\n(C) Handball\n(D) Offside\n(E) Pulling a player\n(F) Malicious foul", "candidates": ["Technical foul", "Tackling foul", "Handball", "Offside", "Pulling a player", "Malicious foul"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_0", "indices": ["1700", "2950", "5250", "6500", "8900", "11500", "13500", "14550", "17450", "18100", "20900", "23250", "25150", "27600", "28150", "30650", "32550", "34650", "37350", "38300", "40800", "42400", "44400", "45650", "49000", "49700", "51750", "54800", "56250", "59400", "61200", "61550"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Tackling foul"]], "filtered_resps": ["(B) Tackling foul"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "557a740da618c3ebe6b88bf594dedb805134ab2e5b0f1062453a030abe21e803", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Why was the player in black jersey number 10 given a yellow card?\n(A) Technical foul\n(B) Tackling foul\n(C) Handball\n(D) Offside\n(E) Pulling a player\n(F) Malicious foul", "task_type": "sportsQA", "pred_answer": "B", "answer": "A"}, "input": "Why was the player in black jersey number 10 given a yellow card?\n(A) Technical foul\n(B) Tackling foul\n(C) Handball\n(D) Offside\n(E) Pulling a player\n(F) Malicious foul\nOnly give the best option.\nBest option: ("}
{"doc_id": 43, "doc": {"video_name": "test_sports_2.mp4", "duration": 7676.82, "question": "Which player in blue jersey kicked the ball onto the side netting of the red team's goal?\n(A) Player number 4 in blue jersey\n(B) Player number 47 in blue jersey\n(C) Player number 9 in blue jersey\n(D) Player number 17 in blue jersey\n(E) Player number 2 in blue jersey\n(F) Player number 28 in blue jersey", "candidates": ["Player number 4 in blue jersey", "Player number 47 in blue jersey", "Player number 9 in blue jersey", "Player number 17 in blue jersey", "Player number 2 in blue jersey", "Player number 28 in blue jersey"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_4", "indices": ["1650", "11875", "14075", "20875", "24675", "31075", "40200", "46400", "50475", "56950", "60000", "65975", "77475", "81075", "86975", "90025", "97125", "102575", "108125", "117125", "123250", "128550", "136650", "143450", "147350", "150900", "156225", "165925", "169200", "174100", "184400", "189150"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Player number 17 in blue jersey"]], "filtered_resps": ["(D) Player number 17 in blue jersey"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1efa001ef8bc85edd944fb13d0d09578cae6d70053960f44533601909b8d5eba", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which player in blue jersey kicked the ball onto the side netting of the red team's goal?\n(A) Player number 4 in blue jersey\n(B) Player number 47 in blue jersey\n(C) Player number 9 in blue jersey\n(D) Player number 17 in blue jersey\n(E) Player number 2 in blue jersey\n(F) Player number 28 in blue jersey", "task_type": "sportsQA", "pred_answer": "D", "answer": "B"}, "input": "Which player in blue jersey kicked the ball onto the side netting of the red team's goal?\n(A) Player number 4 in blue jersey\n(B) Player number 47 in blue jersey\n(C) Player number 9 in blue jersey\n(D) Player number 17 in blue jersey\n(E) Player number 2 in blue jersey\n(F) Player number 28 in blue jersey\nOnly give the best option.\nBest option: ("}
{"doc_id": 47, "doc": {"video_name": "test_sports_29.mp4", "duration": 1252.6, "question": "Which country's team member is in the 5th lane in the video?\n(A) Brazil\n(B) South Korea\n(C) United States\n(D) United Kingdom\n(E) Tunisia\n(F) Turkey", "candidates": ["Brazil", "South Korea", "United States", "United Kingdom", "Tunisia", "Turkey"], "answer": "E", "task_type": "sportsQA", "question_id": "SQA_8", "indices": ["125", "1700", "2100", "3025", "4625", "5650", "6625", "7700", "8650", "9450", "10200", "11025", "12375", "13350", "13875", "15350", "16025", "16850", "18250", "18925", "19700", "21425", "21925", "23400", "24250", "25125", "25725", "27225", "27775", "28600", "30175", "30325"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Tunisia"]], "filtered_resps": ["(E) Tunisia"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a630edf33fba76fb6370b6cba36c4b9f22dd90a62fc7a07dcc72d6bb0c724886", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which country's team member is in the 5th lane in the video?\n(A) Brazil\n(B) South Korea\n(C) United States\n(D) United Kingdom\n(E) Tunisia\n(F) Turkey", "task_type": "sportsQA", "pred_answer": "E", "answer": "E"}, "input": "Which country's team member is in the 5th lane in the video?\n(A) Brazil\n(B) South Korea\n(C) United States\n(D) United Kingdom\n(E) Tunisia\n(F) Turkey\nOnly give the best option.\nBest option: ("}
{"doc_id": 51, "doc": {"video_name": "test_sports_39.mp4", "duration": 2822.29, "question": "Which player scores the first point in the video?\n(A) The player in pink\n(B) The player in orange\n(C) The player in red\n(D) The player in white\n(E) The player in blue\n(F) The player in black", "candidates": ["The player in pink", "The player in orange", "The player in red", "The player in white", "The player in blue", "The player in black"], "answer": "A", "task_type": "sportsQA", "question_id": "SQA_12", "indices": ["1675", "2200", "5375", "6675", "9300", "12275", "14750", "17250", "19225", "21650", "22225", "25050", "26950", "28650", "32075", "33450", "37275", "39650", "41200", "43850", "46250", "46700", "49300", "52500", "54500", "56925", "58625", "60000", "63125", "64100", "66250", "68950"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e33b0e0b1f1a52b641c73fa16c6b11dcdff2298368bf4cb27759dbf383eafc03", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which player scores the first point in the video?\n(A) The player in pink\n(B) The player in orange\n(C) The player in red\n(D) The player in white\n(E) The player in blue\n(F) The player in black", "task_type": "sportsQA", "pred_answer": "A", "answer": "A"}, "input": "Which player scores the first point in the video?\n(A) The player in pink\n(B) The player in orange\n(C) The player in red\n(D) The player in white\n(E) The player in blue\n(F) The player in black\nOnly give the best option.\nBest option: ("}
{"doc_id": 55, "doc": {"video_name": "test_sports_3.mp4", "duration": 389.75, "question": "When the player in red shirt and green shorts took a free kick, how many players from the team in white were defending behind the white line?\n(A) Ten players were defending\n(B) No one was defending\n(C) Seven players were defending\n(D) Three players were lying down to defend\n(E) One player was defending\n(F) The goalkeeper was defending", "candidates": ["Ten players were defending", "No one was defending", "Seven players were defending", "Three players were lying down to defend", "One player was defending", "The goalkeeper was defending"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_16", "indices": ["540", "1140", "2040", "2220", "3540", "3960", "4320", "5580", "6120", "6960", "7620", "8640", "8820", "10080", "10740", "11040", "12300", "12840", "13740", "14400", "14760", "15540", "16080", "16920", "17640", "18840", "19380", "19740", "20520", "21420", "22020", "22740"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) No one was defending"]], "filtered_resps": ["(B) No one was defending"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e0cad1862d7b28ba7decf5cc2e32abe4b2f33a842f96e7eaa6a44d6da412dd0", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "When the player in red shirt and green shorts took a free kick, how many players from the team in white were defending behind the white line?\n(A) Ten players were defending\n(B) No one was defending\n(C) Seven players were defending\n(D) Three players were lying down to defend\n(E) One player was defending\n(F) The goalkeeper was defending", "task_type": "sportsQA", "pred_answer": "B", "answer": "C"}, "input": "When the player in red shirt and green shorts took a free kick, how many players from the team in white were defending behind the white line?\n(A) Ten players were defending\n(B) No one was defending\n(C) Seven players were defending\n(D) Three players were lying down to defend\n(E) One player was defending\n(F) The goalkeeper was defending\nOnly give the best option.\nBest option: ("}
{"doc_id": 59, "doc": {"video_name": "test_sports_1.mp4", "duration": 645.0, "question": "How did the player number 24 in white and blue jersey fall?\n(A) He was tripped by the soccer ball\n(B) He was tackled by a spectator\n(C) He was tackled by a person in black\n(D) He was tackled by player number 20 in red jersey\n(E) He was tackled by the goalkeeper\n(F) He fell on his own", "candidates": ["He was tripped by the soccer ball", "He was tackled by a spectator", "He was tackled by a person in black", "He was tackled by player number 20 in red jersey", "He was tackled by the goalkeeper", "He fell on his own"], "answer": "D", "task_type": "sportsQA", "question_id": "SQA_20", "indices": ["420", "810", "1260", "2190", "2970", "3570", "3630", "4800", "4830", "6000", "6600", "6660", "7350", "8040", "8640", "9420", "10230", "10440", "11070", "11790", "12420", "12990", "13800", "14070", "14730", "15390", "15870", "16410", "17160", "17550", "18150", "18810"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "045eca832be3d25e70281c6bda77e6f567598726ccd193e8a8e4c60b4778d9ca", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How did the player number 24 in white and blue jersey fall?\n(A) He was tripped by the soccer ball\n(B) He was tackled by a spectator\n(C) He was tackled by a person in black\n(D) He was tackled by player number 20 in red jersey\n(E) He was tackled by the goalkeeper\n(F) He fell on his own", "task_type": "sportsQA", "pred_answer": "D", "answer": "D"}, "input": "How did the player number 24 in white and blue jersey fall?\n(A) He was tripped by the soccer ball\n(B) He was tackled by a spectator\n(C) He was tackled by a person in black\n(D) He was tackled by player number 20 in red jersey\n(E) He was tackled by the goalkeeper\n(F) He fell on his own\nOnly give the best option.\nBest option: ("}
{"doc_id": 63, "doc": {"video_name": "test_sports_35.mp4", "duration": 1273.66, "question": "Why was the first race terminated and a rerun scheduled?\n(A) A team member was injured\n(B) A British team member started early\n(C) A Jamaican team member started early\n(D) An off-field person entered the field\n(E) An American team member started early\n(F) There was a disturbance on the scene", "candidates": ["A team member was injured", "A British team member started early", "A Jamaican team member started early", "An off-field person entered the field", "An American team member started early", "There was a disturbance on the scene"], "answer": "B", "task_type": "sportsQA", "question_id": "SQA_24", "indices": ["775", "1650", "2425", "3550", "4650", "5500", "6700", "7350", "8725", "9800", "10025", "11425", "12550", "13675", "14750", "14925", "16150", "16975", "18650", "19750", "19950", "20975", "21875", "22850", "24000", "24950", "26500", "27525", "28150", "29050", "30700", "31525"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b3a9eb9146eedd45d858b92d77d9414555f52643f62df04859a76e911eeb7728", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Why was the first race terminated and a rerun scheduled?\n(A) A team member was injured\n(B) A British team member started early\n(C) A Jamaican team member started early\n(D) An off-field person entered the field\n(E) An American team member started early\n(F) There was a disturbance on the scene", "task_type": "sportsQA", "pred_answer": "B", "answer": "B"}, "input": "Why was the first race terminated and a rerun scheduled?\n(A) A team member was injured\n(B) A British team member started early\n(C) A Jamaican team member started early\n(D) An off-field person entered the field\n(E) An American team member started early\n(F) There was a disturbance on the scene\nOnly give the best option.\nBest option: ("}
{"doc_id": 67, "doc": {"video_name": "test_sports_20.mp4", "duration": 848.16, "question": "What tactic does the trailing dark-colored team employ in the final moments of the game?\n(A) Long pass fast break\n(B) Time-out strategy\n(C) Full-court press\n(D) Three-point shot tactic\n(E) Giving up the game\n(F) Foul tactic", "candidates": ["Long pass fast break", "Time-out strategy", "Full-court press", "Three-point shot tactic", "Giving up the game", "Foul tactic"], "answer": "F", "task_type": "sportsQA", "question_id": "SQA_28", "indices": ["116", "1334", "1972", "2871", "3596", "4466", "5423", "6119", "6380", "7424", "8236", "9222", "10005", "10440", "11484", "12151", "12702", "13717", "14326", "15399", "16240", "16936", "17719", "18966", "19053", "20300", "21199", "21750", "22301", "23693", "24331", "24737"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Foul tactic"]], "filtered_resps": ["(F) Foul tactic"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45e4f9ea70f15d68508c78b9d057fdcbd8631090d033c7b534a501f9ced0b6a7", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What tactic does the trailing dark-colored team employ in the final moments of the game?\n(A) Long pass fast break\n(B) Time-out strategy\n(C) Full-court press\n(D) Three-point shot tactic\n(E) Giving up the game\n(F) Foul tactic", "task_type": "sportsQA", "pred_answer": "F", "answer": "F"}, "input": "What tactic does the trailing dark-colored team employ in the final moments of the game?\n(A) Long pass fast break\n(B) Time-out strategy\n(C) Full-court press\n(D) Three-point shot tactic\n(E) Giving up the game\n(F) Foul tactic\nOnly give the best option.\nBest option: ("}
{"doc_id": 71, "doc": {"video_name": "test_sports_8.mp4", "duration": 668.23, "question": "What were the player in yellow jersey number 10 and number 7 doing after the penalty kick goal?\n(A) Raising hands in cheer\n(B) High knee warm-up\n(C) Dancing\n(D) Crying in despair\n(E) Hugging\n(F) Walking together", "candidates": ["Raising hands in cheer", "High knee warm-up", "Dancing", "Crying in despair", "Hugging", "Walking together"], "answer": "C", "task_type": "sportsQA", "question_id": "SQA_32", "indices": ["1062", "1475", "2714", "3894", "5015", "6785", "7965", "9381", "10207", "12213", "12567", "14691", "15163", "16815", "18172", "19411", "20178", "22302", "23364", "24898", "24957", "26550", "28143", "29618", "30385", "31211", "33335", "33689", "35400", "37052", "38645", "38999"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6f3539fc7513020872112a7a8115127ad701c5b1100b5810d8b468470205b92", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What were the player in yellow jersey number 10 and number 7 doing after the penalty kick goal?\n(A) Raising hands in cheer\n(B) High knee warm-up\n(C) Dancing\n(D) Crying in despair\n(E) Hugging\n(F) Walking together", "task_type": "sportsQA", "pred_answer": "A", "answer": "C"}, "input": "What were the player in yellow jersey number 10 and number 7 doing after the penalty kick goal?\n(A) Raising hands in cheer\n(B) High knee warm-up\n(C) Dancing\n(D) Crying in despair\n(E) Hugging\n(F) Walking together\nOnly give the best option.\nBest option: ("}
{"doc_id": 75, "doc": {"video_name": "test_order_62.mp4", "duration": 950.77, "question": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 4->3->1->2\n(B) 1->2->4->3\n(C) 2->1->4->3\n(D) 2->1->3->4\n(E) 1->2->3->4\n(F) 3->4->2->1", "candidates": ["4->3->1->2", "1->2->4->3", "2->1->4->3", "2->1->3->4", "1->2->3->4", "3->4->2->1"], "answer": "C", "task_type": "order", "question_id": "AO_0", "indices": ["700", "725", "1675", "2725", "3025", "4300", "4675", "5350", "6325", "6725", "7450", "8500", "9400", "10225", "10500", "11750", "12325", "13225", "13900", "14150", "15125", "15675", "16400", "17525", "18500", "18850", "19300", "20350", "21325", "21775", "22425", "23625"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "62ad846d07d8866b9c53523eb7198bfc4b50aadae8b00957ec8c5e5ad39af713", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 4->3->1->2\n(B) 1->2->4->3\n(C) 2->1->4->3\n(D) 2->1->3->4\n(E) 1->2->3->4\n(F) 3->4->2->1", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The word BIKE is overlaid on a mountain scene; (2)Oregon daily emerald logo and title card pops up; (3)The instructions follow with a man in a white ensemble and purple hat; (4)REPAIR is then overlaid under BIKE, Becoming BIKE REPAIR.\n(A) 4->3->1->2\n(B) 1->2->4->3\n(C) 2->1->4->3\n(D) 2->1->3->4\n(E) 1->2->3->4\n(F) 3->4->2->1\nOnly give the best option.\nBest option: ("}
{"doc_id": 79, "doc": {"video_name": "test_order_51.mp4", "duration": 812.01, "question": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 2->1->4->3\n(F) 2->4->1->3", "candidates": ["2->1->3->4", "3->2->1->4", "1->2->3->4", "4->3->2->1", "2->1->4->3", "2->4->1->3"], "answer": "C", "task_type": "order", "question_id": "AO_4", "indices": ["116", "1160", "1972", "2755", "3016", "4263", "5162", "5510", "6148", "7018", "8091", "8497", "9338", "10382", "10788", "11745", "12325", "13108", "13746", "14471", "15486", "16356", "17342", "17458", "18473", "19401", "19894", "21083", "21344", "22040", "22939", "24128"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14ba780fa45ed4120360e5b7ab73b1ac13ecccbd88081a254ca027defdadf630", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 2->1->4->3\n(F) 2->4->1->3", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The guy measures the ingredient on the table; (2)The child and guy added the egg to the bowl; (3)The guy uses silverware to put dough on a baking pan; (4)The child, guy, and dog watch the baking process through the oven window.\n(A) 2->1->3->4\n(B) 3->2->1->4\n(C) 1->2->3->4\n(D) 4->3->2->1\n(E) 2->1->4->3\n(F) 2->4->1->3\nOnly give the best option.\nBest option: ("}
{"doc_id": 83, "doc": {"video_name": "test_order_3.mp4", "duration": 705.0799999999999, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> carving pumpkin --> making jewelry --> riding mule\n(B) carving pumpkin --> making jewelry --> riding mule --> stomping grapes\n(C) making jewelry --> riding mule --> carving pumpkin --> stomping grapes\n(D) carving pumpkin --> riding mule --> stomping grapes --> making jewelry\n(E) riding mule --> stomping grapes --> carving pumpkin --> making jewelry\n(F) riding mule --> carving pumpkin --> making jewelry --> stomping grapes", "candidates": ["stomping grapes --> carving pumpkin --> making jewelry --> riding mule", "carving pumpkin --> making jewelry --> riding mule --> stomping grapes", "making jewelry --> riding mule --> carving pumpkin --> stomping grapes", "carving pumpkin --> riding mule --> stomping grapes --> making jewelry", "riding mule --> stomping grapes --> carving pumpkin --> making jewelry", "riding mule --> carving pumpkin --> making jewelry --> stomping grapes"], "answer": "A", "task_type": "order", "question_id": "AO_8", "indices": ["510", "900", "1410", "2040", "2910", "3420", "4170", "4620", "5910", "5970", "6900", "7770", "8490", "9060", "9630", "9930", "11070", "11820", "11910", "12660", "13770", "14310", "14520", "15480", "16440", "16530", "17340", "18060", "18840", "19410", "20070", "20970"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of actions shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of actions shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4db7c7b72bf2a256348bd10173f3adb8aca79b451b377b1aff3105aa276245db", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> carving pumpkin --> making jewelry --> riding mule\n(B) carving pumpkin --> making jewelry --> riding mule --> stomping grapes\n(C) making jewelry --> riding mule --> carving pumpkin --> stomping grapes\n(D) carving pumpkin --> riding mule --> stomping grapes --> making jewelry\n(E) riding mule --> stomping grapes --> carving pumpkin --> making jewelry\n(F) riding mule --> carving pumpkin --> making jewelry --> stomping grapes", "task_type": "order", "pred_answer": "None of the options provided match the sequence of actions shown in the video. The", "answer": "A"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) stomping grapes --> carving pumpkin --> making jewelry --> riding mule\n(B) carving pumpkin --> making jewelry --> riding mule --> stomping grapes\n(C) making jewelry --> riding mule --> carving pumpkin --> stomping grapes\n(D) carving pumpkin --> riding mule --> stomping grapes --> making jewelry\n(E) riding mule --> stomping grapes --> carving pumpkin --> making jewelry\n(F) riding mule --> carving pumpkin --> making jewelry --> stomping grapes\nOnly give the best option.\nBest option: ("}
{"doc_id": 87, "doc": {"video_name": "test_order_34.mp4", "duration": 526.9399999999999, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> paragliding --> playing trombone --> riding mule\n(B) jetskiing --> paragliding --> riding mule --> playing trombone\n(C) paragliding --> playing trombone --> riding mule --> jetskiing\n(D) paragliding --> playing trombone --> jetskiing --> riding mule\n(E) playing trombone --> paragliding --> riding mule --> jetskiing\n(F) paragliding --> jetskiing --> riding mule --> playing trombone", "candidates": ["jetskiing --> paragliding --> playing trombone --> riding mule", "jetskiing --> paragliding --> riding mule --> playing trombone", "paragliding --> playing trombone --> riding mule --> jetskiing", "paragliding --> playing trombone --> jetskiing --> riding mule", "playing trombone --> paragliding --> riding mule --> jetskiing", "paragliding --> jetskiing --> riding mule --> playing trombone"], "answer": "A", "task_type": "order", "question_id": "AO_12", "indices": ["406", "957", "1015", "1914", "2088", "2668", "3219", "3915", "4002", "4582", "5017", "5626", "6119", "6873", "7279", "7569", "8294", "8410", "9164", "9570", "9860", "10527", "10933", "11542", "11861", "12789", "13253", "13775", "14036", "14616", "14819", "15428"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of events described in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of events described in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27244fcc1a65328a50654985a280fb3f51d805523fe000ab8fcc6a67bde261f8", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> paragliding --> playing trombone --> riding mule\n(B) jetskiing --> paragliding --> riding mule --> playing trombone\n(C) paragliding --> playing trombone --> riding mule --> jetskiing\n(D) paragliding --> playing trombone --> jetskiing --> riding mule\n(E) playing trombone --> paragliding --> riding mule --> jetskiing\n(F) paragliding --> jetskiing --> riding mule --> playing trombone", "task_type": "order", "pred_answer": "None of the options provided match the sequence of events described in the video. The", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> paragliding --> playing trombone --> riding mule\n(B) jetskiing --> paragliding --> riding mule --> playing trombone\n(C) paragliding --> playing trombone --> riding mule --> jetskiing\n(D) paragliding --> playing trombone --> jetskiing --> riding mule\n(E) playing trombone --> paragliding --> riding mule --> jetskiing\n(F) paragliding --> jetskiing --> riding mule --> playing trombone\nOnly give the best option.\nBest option: ("}
{"doc_id": 91, "doc": {"video_name": "test_order_59.mp4", "duration": 972.87, "question": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->3->4->2\n(D) 2->4->1->3\n(E) 2->1->4->3\n(F) 1->2->3->4", "candidates": ["4->3->2->1", "3->4->1->2", "1->3->4->2", "2->4->1->3", "2->1->4->3", "1->2->3->4"], "answer": "E", "task_type": "order", "question_id": "AO_16", "indices": ["841", "1131", "1914", "3538", "3683", "5423", "5481", "6351", "8091", "8526", "9396", "10817", "11629", "12093", "12818", "14355", "14703", "15602", "17226", "17371", "18473", "19546", "20010", "21257", "22678", "22736", "24041", "24621", "25578", "26477", "27521", "28710"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "27849127cc701e8a899263ca9207012ddd7d5874377f039a6f941613dc1f27b3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->3->4->2\n(D) 2->4->1->3\n(E) 2->1->4->3\n(F) 1->2->3->4", "task_type": "order", "pred_answer": "E", "answer": "E"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The trainer and class step in a circle and up on the platform; (2)The trainer leads an aerobic class with people in a gym; (3)The trainer and class walk over then in reverse over the platform; (4)The trainer and class step up sideways on the platform.\n(A) 4->3->2->1\n(B) 3->4->1->2\n(C) 1->3->4->2\n(D) 2->4->1->3\n(E) 2->1->4->3\n(F) 1->2->3->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 95, "doc": {"video_name": "test_order_21.mp4", "duration": 766.03, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> clean and jerk --> cooking sausages --> baking cookies\n(B) clean and jerk --> playing harp --> cooking sausages --> baking cookies\n(C) baking cookies --> playing harp --> clean and jerk --> cooking sausages\n(D) baking cookies --> playing harp --> cooking sausages --> clean and jerk\n(E) cooking sausages --> clean and jerk --> baking cookies --> playing harp\n(F) playing harp --> baking cookies --> clean and jerk --> cooking sausages", "candidates": ["playing harp --> clean and jerk --> cooking sausages --> baking cookies", "clean and jerk --> playing harp --> cooking sausages --> baking cookies", "baking cookies --> playing harp --> clean and jerk --> cooking sausages", "baking cookies --> playing harp --> cooking sausages --> clean and jerk", "cooking sausages --> clean and jerk --> baking cookies --> playing harp", "playing harp --> baking cookies --> clean and jerk --> cooking sausages"], "answer": "F", "task_type": "order", "question_id": "AO_20", "indices": ["360", "840", "1890", "2250", "3300", "3930", "4890", "5310", "6210", "7110", "7530", "7890", "9240", "9330", "10740", "10830", "11790", "12330", "12960", "14130", "14460", "15420", "16470", "17040", "17400", "18240", "18990", "19650", "20520", "20820", "21690", "22800"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of actions shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of actions shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d6846ee2cc3ba1cb094799437fd9c4683d4205bcadb85837e2d7fe92a8d688c6", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> clean and jerk --> cooking sausages --> baking cookies\n(B) clean and jerk --> playing harp --> cooking sausages --> baking cookies\n(C) baking cookies --> playing harp --> clean and jerk --> cooking sausages\n(D) baking cookies --> playing harp --> cooking sausages --> clean and jerk\n(E) cooking sausages --> clean and jerk --> baking cookies --> playing harp\n(F) playing harp --> baking cookies --> clean and jerk --> cooking sausages", "task_type": "order", "pred_answer": "None of the options provided match the sequence of actions shown in the video. The", "answer": "F"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) playing harp --> clean and jerk --> cooking sausages --> baking cookies\n(B) clean and jerk --> playing harp --> cooking sausages --> baking cookies\n(C) baking cookies --> playing harp --> clean and jerk --> cooking sausages\n(D) baking cookies --> playing harp --> cooking sausages --> clean and jerk\n(E) cooking sausages --> clean and jerk --> baking cookies --> playing harp\n(F) playing harp --> baking cookies --> clean and jerk --> cooking sausages\nOnly give the best option.\nBest option: ("}
{"doc_id": 99, "doc": {"video_name": "test_order_49.mp4", "duration": 6980.750000000001, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> playing trombone --> stomping grapes\n(B) playing trombone --> jetskiing --> abseiling --> stomping grapes\n(C) abseiling --> jetskiing --> stomping grapes --> playing trombone\n(D) playing trombone --> abseiling --> stomping grapes --> jetskiing\n(E) jetskiing --> playing trombone --> stomping grapes --> abseiling\n(F) abseiling --> playing trombone --> stomping grapes --> jetskiing", "candidates": ["jetskiing --> abseiling --> playing trombone --> stomping grapes", "playing trombone --> jetskiing --> abseiling --> stomping grapes", "abseiling --> jetskiing --> stomping grapes --> playing trombone", "playing trombone --> abseiling --> stomping grapes --> jetskiing", "jetskiing --> playing trombone --> stomping grapes --> abseiling", "abseiling --> playing trombone --> stomping grapes --> jetskiing"], "answer": "A", "task_type": "order", "question_id": "AO_24", "indices": ["1080", "9300", "16020", "24090", "32160", "35820", "44880", "50820", "53670", "61860", "70860", "72330", "84510", "88140", "95940", "103860", "108210", "115920", "119880", "130440", "132000", "141990", "149100", "156270", "158160", "168600", "174750", "179760", "187680", "191340", "201990", "203820"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C)"]], "filtered_resps": ["(C)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "33d0cb35d36278d6d7e60346b42c1bd097f98b75cb67a3d52cefd9c161eb6d09", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> playing trombone --> stomping grapes\n(B) playing trombone --> jetskiing --> abseiling --> stomping grapes\n(C) abseiling --> jetskiing --> stomping grapes --> playing trombone\n(D) playing trombone --> abseiling --> stomping grapes --> jetskiing\n(E) jetskiing --> playing trombone --> stomping grapes --> abseiling\n(F) abseiling --> playing trombone --> stomping grapes --> jetskiing", "task_type": "order", "pred_answer": "C", "answer": "A"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) jetskiing --> abseiling --> playing trombone --> stomping grapes\n(B) playing trombone --> jetskiing --> abseiling --> stomping grapes\n(C) abseiling --> jetskiing --> stomping grapes --> playing trombone\n(D) playing trombone --> abseiling --> stomping grapes --> jetskiing\n(E) jetskiing --> playing trombone --> stomping grapes --> abseiling\n(F) abseiling --> playing trombone --> stomping grapes --> jetskiing\nOnly give the best option.\nBest option: ("}
{"doc_id": 103, "doc": {"video_name": "test_order_16.mp4", "duration": 649.4499999999999, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> making jewelry --> zumba --> tossing coin\n(B) cooking sausages --> zumba --> making jewelry --> tossing coin\n(C) making jewelry --> tossing coin --> zumba --> cooking sausages\n(D) cooking sausages --> making jewelry --> tossing coin --> zumba\n(E) tossing coin --> zumba --> making jewelry --> cooking sausages\n(F) tossing coin --> making jewelry --> cooking sausages --> zumba", "candidates": ["cooking sausages --> making jewelry --> zumba --> tossing coin", "cooking sausages --> zumba --> making jewelry --> tossing coin", "making jewelry --> tossing coin --> zumba --> cooking sausages", "cooking sausages --> making jewelry --> tossing coin --> zumba", "tossing coin --> zumba --> making jewelry --> cooking sausages", "tossing coin --> making jewelry --> cooking sausages --> zumba"], "answer": "B", "task_type": "order", "question_id": "AO_28", "indices": ["330", "930", "1620", "2040", "2850", "3480", "3810", "4320", "5130", "5730", "6420", "6870", "7440", "8040", "8820", "9540", "9990", "10410", "11280", "11640", "12660", "13320", "13680", "13980", "15120", "15510", "16170", "16920", "17130", "18210", "18810", "19440"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) cooking sausages --> zumba --> making jewelry --> tossing coin"]], "filtered_resps": ["(B) cooking sausages --> zumba --> making jewelry --> tossing coin"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "daea0025b6dc982a316cc325ff70754783436fd87207f516b48cb65191dad01e", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> making jewelry --> zumba --> tossing coin\n(B) cooking sausages --> zumba --> making jewelry --> tossing coin\n(C) making jewelry --> tossing coin --> zumba --> cooking sausages\n(D) cooking sausages --> making jewelry --> tossing coin --> zumba\n(E) tossing coin --> zumba --> making jewelry --> cooking sausages\n(F) tossing coin --> making jewelry --> cooking sausages --> zumba", "task_type": "order", "pred_answer": "B", "answer": "B"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cooking sausages --> making jewelry --> zumba --> tossing coin\n(B) cooking sausages --> zumba --> making jewelry --> tossing coin\n(C) making jewelry --> tossing coin --> zumba --> cooking sausages\n(D) cooking sausages --> making jewelry --> tossing coin --> zumba\n(E) tossing coin --> zumba --> making jewelry --> cooking sausages\n(F) tossing coin --> making jewelry --> cooking sausages --> zumba\nOnly give the best option.\nBest option: ("}
{"doc_id": 107, "doc": {"video_name": "test_order_12.mp4", "duration": 587.6, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> zumba --> riding mule --> shredding paper\n(B) riding mule --> zumba --> playing trombone --> shredding paper\n(C) shredding paper --> zumba --> playing trombone --> riding mule\n(D) riding mule --> shredding paper --> zumba --> playing trombone\n(E) riding mule --> playing trombone --> zumba --> shredding paper\n(F) shredding paper --> playing trombone --> zumba --> riding mule", "candidates": ["playing trombone --> zumba --> riding mule --> shredding paper", "riding mule --> zumba --> playing trombone --> shredding paper", "shredding paper --> zumba --> playing trombone --> riding mule", "riding mule --> shredding paper --> zumba --> playing trombone", "riding mule --> playing trombone --> zumba --> shredding paper", "shredding paper --> playing trombone --> zumba --> riding mule"], "answer": "C", "task_type": "order", "question_id": "AO_32", "indices": ["0", "720", "1590", "2070", "2580", "3240", "3450", "4110", "4380", "5010", "5910", "6300", "6690", "7500", "7890", "8400", "9210", "9480", "10050", "10500", "11400", "11760", "12240", "13020", "13500", "14250", "14280", "15240", "15570", "16230", "16920", "17490"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0cac04f5fdc87cd65538cb20105cd4195dc67b8867647759ae50ba814c383c90", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> zumba --> riding mule --> shredding paper\n(B) riding mule --> zumba --> playing trombone --> shredding paper\n(C) shredding paper --> zumba --> playing trombone --> riding mule\n(D) riding mule --> shredding paper --> zumba --> playing trombone\n(E) riding mule --> playing trombone --> zumba --> shredding paper\n(F) shredding paper --> playing trombone --> zumba --> riding mule", "task_type": "order", "pred_answer": "D", "answer": "C"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) playing trombone --> zumba --> riding mule --> shredding paper\n(B) riding mule --> zumba --> playing trombone --> shredding paper\n(C) shredding paper --> zumba --> playing trombone --> riding mule\n(D) riding mule --> shredding paper --> zumba --> playing trombone\n(E) riding mule --> playing trombone --> zumba --> shredding paper\n(F) shredding paper --> playing trombone --> zumba --> riding mule\nOnly give the best option.\nBest option: ("}
{"doc_id": 111, "doc": {"video_name": "test_order_23.mp4", "duration": 479.05999999999995, "question": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> paragliding --> playing harp\n(B) making jewelry --> paragliding --> water sliding --> playing harp\n(C) playing harp --> making jewelry --> water sliding --> paragliding\n(D) water sliding --> playing harp --> making jewelry --> paragliding\n(E) paragliding --> water sliding --> playing harp --> making jewelry\n(F) water sliding --> making jewelry --> playing harp --> paragliding", "candidates": ["water sliding --> making jewelry --> paragliding --> playing harp", "making jewelry --> paragliding --> water sliding --> playing harp", "playing harp --> making jewelry --> water sliding --> paragliding", "water sliding --> playing harp --> making jewelry --> paragliding", "paragliding --> water sliding --> playing harp --> making jewelry", "water sliding --> making jewelry --> playing harp --> paragliding"], "answer": "B", "task_type": "order", "question_id": "AO_36", "indices": ["203", "551", "870", "1682", "2175", "2262", "2958", "3451", "3857", "4408", "4640", "5191", "5481", "5974", "6583", "6786", "7337", "8033", "8149", "8845", "9164", "9657", "10092", "10643", "10846", "11310", "11832", "12528", "12644", "13079", "13717", "14036"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F)"]], "filtered_resps": ["(F)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20f4921f8ff9e2ecb79b7cf6f666391d34bcb02fd5baaea47fded2d9eaee627d", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> paragliding --> playing harp\n(B) making jewelry --> paragliding --> water sliding --> playing harp\n(C) playing harp --> making jewelry --> water sliding --> paragliding\n(D) water sliding --> playing harp --> making jewelry --> paragliding\n(E) paragliding --> water sliding --> playing harp --> making jewelry\n(F) water sliding --> making jewelry --> playing harp --> paragliding", "task_type": "order", "pred_answer": "F", "answer": "B"}, "input": "Please identify the option that corresponds to the order of events as they occur in the video.\n(A) water sliding --> making jewelry --> paragliding --> playing harp\n(B) making jewelry --> paragliding --> water sliding --> playing harp\n(C) playing harp --> making jewelry --> water sliding --> paragliding\n(D) water sliding --> playing harp --> making jewelry --> paragliding\n(E) paragliding --> water sliding --> playing harp --> making jewelry\n(F) water sliding --> making jewelry --> playing harp --> paragliding\nOnly give the best option.\nBest option: ("}
{"doc_id": 115, "doc": {"video_name": "test_order_53.mp4", "duration": 703.98, "question": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n(E) 4->1->2->3\n(F) 3->2->1->4", "candidates": ["3->4->1->2", "4->3->2->1", "2->1->3->4", "1->2->3->4", "4->1->2->3", "3->2->1->4"], "answer": "C", "task_type": "order", "question_id": "AO_40", "indices": ["600", "1850", "2200", "3250", "4950", "5650", "7350", "8450", "9000", "10250", "11400", "12300", "13250", "15200", "15350", "16450", "17700", "19400", "20750", "21500", "22700", "23550", "25000", "25750", "26350", "27950", "29350", "30400", "31050", "32300", "33000", "34150"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8cec8ca922068aac7f9eb2458d6853be4a820ff41c14bdc355cfdf1cdf52fd88", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n(E) 4->1->2->3\n(F) 3->2->1->4", "task_type": "order", "pred_answer": "C", "answer": "C"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The man shows how to work the monkey bars; (2)A male fitness trainer from Iron Edge is about to demonstrate various workouts using bars; (3)The trainer shows how to maneuver straight bar, pulleys, and medicine ball; (4)A workout regimen is displayed as part of the conclusion of the demonstration.\n(A) 3->4->1->2\n(B) 4->3->2->1\n(C) 2->1->3->4\n(D) 1->2->3->4\n(E) 4->1->2->3\n(F) 3->2->1->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 119, "doc": {"video_name": "test_order_58.mp4", "duration": 6259.48, "question": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->4->3->1\n(C) 1->2->3->4\n(D) 4->1->3->2\n(E) 4->3->2->1\n(F) 2->3->1->4", "candidates": ["3->1->2->4", "2->4->3->1", "1->2->3->4", "4->1->3->2", "4->3->2->1", "2->3->1->4"], "answer": "A", "task_type": "order", "question_id": "AO_44", "indices": ["89813", "89987", "90045", "91640", "91669", "91698", "91756", "91785", "91814", "91872", "91959", "92046", "92075", "92104", "92191", "92220", "93815", "93844", "93902", "93931", "93960", "94018", "94076", "94105", "94134", "94192", "94308", "94453", "94511", "95352", "95729", "95961"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7ed8c5f58d6806521cda607b7e5bcbb5017cec24f3f15966681f3bf29b02b982", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->4->3->1\n(C) 1->2->3->4\n(D) 4->1->3->2\n(E) 4->3->2->1\n(F) 2->3->1->4", "task_type": "order", "pred_answer": "E", "answer": "A"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The group begins to dance in unison; (2)Some of the group are on their feet; (3)A group gathers to the center of a gym floor; (4)Some are in wheel chairs.\n(A) 3->1->2->4\n(B) 2->4->3->1\n(C) 1->2->3->4\n(D) 4->1->3->2\n(E) 4->3->2->1\n(F) 2->3->1->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 123, "doc": {"video_name": "test_order_56.mp4", "duration": 665.06, "question": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->3->1->4\n(B) 1->3->2->4\n(C) 3->1->2->4\n(D) 1->2->3->4\n(E) 2->1->3->4\n(F) 4->2->1->3", "candidates": ["2->3->1->4", "1->3->2->4", "3->1->2->4", "1->2->3->4", "2->1->3->4", "4->2->1->3"], "answer": "E", "task_type": "order", "question_id": "AO_48", "indices": ["84", "1008", "1232", "2240", "2324", "3276", "3556", "4620", "4844", "5824", "5908", "6440", "7224", "8148", "8736", "9352", "9380", "10360", "11088", "11704", "11732", "12460", "13188", "13720", "14644", "15008", "15372", "16212", "16968", "17304", "18172", "18228"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "364458768ef75c373f0c30aea5c99d973267f86500f36974e20c09c650f32f2c", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->3->1->4\n(B) 1->3->2->4\n(C) 3->1->2->4\n(D) 1->2->3->4\n(E) 2->1->3->4\n(F) 4->2->1->3", "task_type": "order", "pred_answer": "D", "answer": "E"}, "input": "Arrange the following events from the video in the correct chronological order: (1)A man on the street with a poster sign tries to get customers; (2)A university swim team is doing a fund raiser washing cars; (3)The students thank people in the video and to come support them; (4)A black screen appears with a website address.\n(A) 2->3->1->4\n(B) 1->3->2->4\n(C) 3->1->2->4\n(D) 1->2->3->4\n(E) 2->1->3->4\n(F) 4->2->1->3\nOnly give the best option.\nBest option: ("}
{"doc_id": 127, "doc": {"video_name": "test_order_39.mp4", "duration": 539.33, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cleaning toilet --> baking cookies --> paragliding --> cooking sausages\n(B) baking cookies --> cooking sausages --> paragliding --> cleaning toilet\n(C) cleaning toilet --> cooking sausages --> paragliding --> baking cookies\n(D) paragliding --> cooking sausages --> baking cookies --> cleaning toilet\n(E) baking cookies --> cleaning toilet --> cooking sausages --> paragliding\n(F) paragliding --> cooking sausages --> cleaning toilet --> baking cookies", "candidates": ["cleaning toilet --> baking cookies --> paragliding --> cooking sausages", "baking cookies --> cooking sausages --> paragliding --> cleaning toilet", "cleaning toilet --> cooking sausages --> paragliding --> baking cookies", "paragliding --> cooking sausages --> baking cookies --> cleaning toilet", "baking cookies --> cleaning toilet --> cooking sausages --> paragliding", "paragliding --> cooking sausages --> cleaning toilet --> baking cookies"], "answer": "A", "task_type": "order", "question_id": "AO_52", "indices": ["150", "600", "1080", "1830", "2010", "2640", "3300", "3600", "4110", "4500", "5130", "5700", "6150", "6840", "7050", "7800", "8190", "8610", "9120", "9570", "10350", "10770", "11130", "11640", "12360", "13020", "13500", "13980", "14220", "15090", "15630", "15720"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) baking cookies --> cleaning toilet --> cooking sausages --> paragl"]], "filtered_resps": ["(E) baking cookies --> cleaning toilet --> cooking sausages --> paragl"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59594f72f1ef539671edfa81dc8da539c51c8fd2a2ce987ddc701ad054e4e1b4", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cleaning toilet --> baking cookies --> paragliding --> cooking sausages\n(B) baking cookies --> cooking sausages --> paragliding --> cleaning toilet\n(C) cleaning toilet --> cooking sausages --> paragliding --> baking cookies\n(D) paragliding --> cooking sausages --> baking cookies --> cleaning toilet\n(E) baking cookies --> cleaning toilet --> cooking sausages --> paragliding\n(F) paragliding --> cooking sausages --> cleaning toilet --> baking cookies", "task_type": "order", "pred_answer": "E", "answer": "A"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) cleaning toilet --> baking cookies --> paragliding --> cooking sausages\n(B) baking cookies --> cooking sausages --> paragliding --> cleaning toilet\n(C) cleaning toilet --> cooking sausages --> paragliding --> baking cookies\n(D) paragliding --> cooking sausages --> baking cookies --> cleaning toilet\n(E) baking cookies --> cleaning toilet --> cooking sausages --> paragliding\n(F) paragliding --> cooking sausages --> cleaning toilet --> baking cookies\nOnly give the best option.\nBest option: ("}
{"doc_id": 131, "doc": {"video_name": "test_order_46.mp4", "duration": 490.03, "question": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> jetskiing --> shredding paper --> making jewelry\n(B) jetskiing --> tossing coin --> shredding paper --> making jewelry\n(C) making jewelry --> shredding paper --> jetskiing --> tossing coin\n(D) making jewelry --> tossing coin --> jetskiing --> shredding paper\n(E) shredding paper --> making jewelry --> tossing coin --> jetskiing\n(F) shredding paper --> jetskiing --> making jewelry --> tossing coin", "candidates": ["tossing coin --> jetskiing --> shredding paper --> making jewelry", "jetskiing --> tossing coin --> shredding paper --> making jewelry", "making jewelry --> shredding paper --> jetskiing --> tossing coin", "making jewelry --> tossing coin --> jetskiing --> shredding paper", "shredding paper --> making jewelry --> tossing coin --> jetskiing", "shredding paper --> jetskiing --> making jewelry --> tossing coin"], "answer": "D", "task_type": "order", "question_id": "AO_56", "indices": ["150", "540", "1110", "1800", "2220", "2280", "3000", "3600", "4080", "4110", "4770", "5460", "5580", "6120", "6720", "7080", "7380", "7800", "8670", "9000", "9510", "10050", "10110", "10860", "11310", "11640", "12180", "12750", "13260", "13320", "13980", "14670"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["None of the options provided match the sequence of actions shown in the video. The"]], "filtered_resps": ["None of the options provided match the sequence of actions shown in the video. The"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5770ed32c58e6b55980b8a9a3ed47bcb11c91a25a90fe75866b8d20b462aec58", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> jetskiing --> shredding paper --> making jewelry\n(B) jetskiing --> tossing coin --> shredding paper --> making jewelry\n(C) making jewelry --> shredding paper --> jetskiing --> tossing coin\n(D) making jewelry --> tossing coin --> jetskiing --> shredding paper\n(E) shredding paper --> making jewelry --> tossing coin --> jetskiing\n(F) shredding paper --> jetskiing --> making jewelry --> tossing coin", "task_type": "order", "pred_answer": "None of the options provided match the sequence of actions shown in the video. The", "answer": "D"}, "input": "Can you tell me which option represents the actual order of actions shown in the video?\n(A) tossing coin --> jetskiing --> shredding paper --> making jewelry\n(B) jetskiing --> tossing coin --> shredding paper --> making jewelry\n(C) making jewelry --> shredding paper --> jetskiing --> tossing coin\n(D) making jewelry --> tossing coin --> jetskiing --> shredding paper\n(E) shredding paper --> making jewelry --> tossing coin --> jetskiing\n(F) shredding paper --> jetskiing --> making jewelry --> tossing coin\nOnly give the best option.\nBest option: ("}
{"doc_id": 135, "doc": {"video_name": "test_order_22.mp4", "duration": 634.73, "question": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) carving pumpkin --> abseiling --> jetskiing --> playing harp\n(B) abseiling --> jetskiing --> playing harp --> carving pumpkin\n(C) carving pumpkin --> jetskiing --> abseiling --> playing harp\n(D) abseiling --> playing harp --> carving pumpkin --> jetskiing\n(E) abseiling --> playing harp --> jetskiing --> carving pumpkin\n(F) carving pumpkin --> abseiling --> playing harp --> jetskiing", "candidates": ["carving pumpkin --> abseiling --> jetskiing --> playing harp", "abseiling --> jetskiing --> playing harp --> carving pumpkin", "carving pumpkin --> jetskiing --> abseiling --> playing harp", "abseiling --> playing harp --> carving pumpkin --> jetskiing", "abseiling --> playing harp --> jetskiing --> carving pumpkin", "carving pumpkin --> abseiling --> playing harp --> jetskiing"], "answer": "E", "task_type": "order", "question_id": "AO_60", "indices": ["0", "780", "1650", "2220", "2580", "3510", "3720", "4440", "4890", "5730", "6240", "6810", "7620", "7980", "8460", "9180", "9540", "10560", "10920", "11850", "11910", "12810", "13320", "13650", "14520", "15180", "15420", "16050", "17190", "17670", "17940", "18720"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B)"]], "filtered_resps": ["(B)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a431dfd76629a1a0c6b8ff4e1dafc5068ed3726fed271d0ab63c484ac555ff18", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) carving pumpkin --> abseiling --> jetskiing --> playing harp\n(B) abseiling --> jetskiing --> playing harp --> carving pumpkin\n(C) carving pumpkin --> jetskiing --> abseiling --> playing harp\n(D) abseiling --> playing harp --> carving pumpkin --> jetskiing\n(E) abseiling --> playing harp --> jetskiing --> carving pumpkin\n(F) carving pumpkin --> abseiling --> playing harp --> jetskiing", "task_type": "order", "pred_answer": "B", "answer": "E"}, "input": "Which of the following options correctly matches the sequence of actions as they actually appear in the video?\n(A) carving pumpkin --> abseiling --> jetskiing --> playing harp\n(B) abseiling --> jetskiing --> playing harp --> carving pumpkin\n(C) carving pumpkin --> jetskiing --> abseiling --> playing harp\n(D) abseiling --> playing harp --> carving pumpkin --> jetskiing\n(E) abseiling --> playing harp --> jetskiing --> carving pumpkin\n(F) carving pumpkin --> abseiling --> playing harp --> jetskiing\nOnly give the best option.\nBest option: ("}
{"doc_id": 139, "doc": {"video_name": "test_order_61.mp4", "duration": 812.08, "question": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 1->2->3->4\n(D) 3->2->4->1\n(E) 4->3->2->1\n(F) 3->1->4->2", "candidates": ["2->3->1->4", "4->1->2->3", "1->2->3->4", "3->2->4->1", "4->3->2->1", "3->1->4->2"], "answer": "D", "task_type": "order", "question_id": "AO_64", "indices": ["116", "1015", "1508", "2465", "3741", "4350", "4553", "5887", "6090", "7163", "7801", "8816", "9280", "9860", "11107", "11687", "12267", "13630", "13891", "14413", "15399", "16414", "17081", "17719", "18444", "19169", "19865", "20706", "21692", "22533", "23316", "24157"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c7aa7710aa6187bedd2e6eab5f5c7d3a820e44c3bda06e2505a590fd198ce99", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 1->2->3->4\n(D) 3->2->4->1\n(E) 4->3->2->1\n(F) 3->1->4->2", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1) The slack line athletes finish competing and shake hands at the finish; (2) A bald headed man performs tricks on a yellow trick line; (3) Two men meet to compete in a slack line competition; (4) The camera pans back to the bald man performing trick on the slack line and back to the man wearing a baseball cap performing tricks on the traditional line.\n(A) 2->3->1->4\n(B) 4->1->2->3\n(C) 1->2->3->4\n(D) 3->2->4->1\n(E) 4->3->2->1\n(F) 3->1->4->2\nOnly give the best option.\nBest option: ("}
{"doc_id": 143, "doc": {"video_name": "test_order_57.mp4", "duration": 891.23, "question": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 2->1->4->3\n(B) 1->3->4->2\n(C) 3->4->2->1\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 1->2->3->4", "candidates": ["2->1->4->3", "1->3->4->2", "3->4->2->1", "2->4->1->3", "4->2->1->3", "1->2->3->4"], "answer": "D", "task_type": "order", "question_id": "AO_68", "indices": ["12120", "12180", "12210", "12240", "12270", "12450", "12600", "12810", "12840", "12870", "12900", "13020", "13050", "13170", "13980", "14100", "14130", "14520", "14550", "14760", "14790", "14820", "14940", "14970", "15600", "15900", "15930", "15990", "16980", "17370", "17520", "17610"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78677459da2d8210a5a0f5ad7c7f7a9fdbcec45b7e6a1583910830d3decdbda8", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 2->1->4->3\n(B) 1->3->4->2\n(C) 3->4->2->1\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 1->2->3->4", "task_type": "order", "pred_answer": "D", "answer": "D"}, "input": "Arrange the following events from the video in the correct chronological order: (1)The blue and gray guy are on a seesaw and the blue guy jumps at the gray guy; (2)A man's image as he talks is imposed over trees and the man make gestures towards his mouth; (3)Both of the characters fall off the map; (4)We see a game of rock em sock em style robots on a computer screen cut with images of the man narrating in the lower left corner.\n(A) 2->1->4->3\n(B) 1->3->4->2\n(C) 3->4->2->1\n(D) 2->4->1->3\n(E) 4->2->1->3\n(F) 1->2->3->4\nOnly give the best option.\nBest option: ("}
{"doc_id": 147, "doc": {"video_name": "test_needle_55.mp4", "duration": 455.12, "question": "In the collision event of objects in the video, how many moving objects are there when the video ends?\n(A) 5\n(B) 4\n(C) 6\n(D) 7\n(E) 3\n(F) 2", "candidates": ["5", "4", "6", "7", "3", "2"], "answer": "B", "task_type": "needleQA", "question_id": "NQA_2", "indices": ["75", "475", "925", "1150", "1525", "1875", "2375", "2550", "2975", "3200", "3775", "3950", "4500", "4825", "5150", "5600", "5775", "6125", "6500", "7025", "7325", "7775", "7925", "8350", "8825", "9025", "9225", "9700", "10175", "10600", "10700", "11250"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E)"]], "filtered_resps": ["(E)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5dbab6932a4788884370781db71305095ef6f5f5cff1a77f8b4e63c02654cc98", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, how many moving objects are there when the video ends?\n(A) 5\n(B) 4\n(C) 6\n(D) 7\n(E) 3\n(F) 2", "task_type": "needleQA", "pred_answer": "E", "answer": "B"}, "input": "In the collision event of objects in the video, how many moving objects are there when the video ends?\n(A) 5\n(B) 4\n(C) 6\n(D) 7\n(E) 3\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 151, "doc": {"video_name": "test_needle_38.mp4", "duration": 496.32, "question": "In the video, what is the white cup filled with black liquid placed on?\n(A) Leather\n(B) Computer\n(C) Basin\n(D) Wooden board\n(E) Ground\n(F) Bowl", "candidates": ["Leather", "Computer", "Basin", "Wooden board", "Ground", "Bowl"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_6", "indices": ["261", "551", "1189", "1421", "2030", "2407", "2842", "3567", "3828", "4263", "5075", "5278", "5597", "6235", "6612", "7018", "7743", "7888", "8729", "9135", "9483", "10150", "10237", "10759", "11513", "11832", "12499", "12528", "13137", "13659", "14123", "14703"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be9e35bbf5bf254ec093e9ca3a9919e6455f73ed0781cc80de20554b09808bd6", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what is the white cup filled with black liquid placed on?\n(A) Leather\n(B) Computer\n(C) Basin\n(D) Wooden board\n(E) Ground\n(F) Bowl", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what is the white cup filled with black liquid placed on?\n(A) Leather\n(B) Computer\n(C) Basin\n(D) Wooden board\n(E) Ground\n(F) Bowl\nOnly give the best option.\nBest option: ("}
{"doc_id": 155, "doc": {"video_name": "test_needle_13.mp4", "duration": 458.64, "question": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Fighting\n(C) Hugging\n(D) Playing football\n(E) Arguing\n(F) Dancing", "candidates": ["Singing", "Fighting", "Hugging", "Playing football", "Arguing", "Dancing"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_10", "indices": ["0", "425", "875", "1125", "1575", "1925", "2275", "2775", "3175", "3225", "3575", "3900", "4425", "4650", "5150", "5475", "5975", "6225", "6675", "6975", "7425", "7500", "8050", "8450", "8875", "9025", "9450", "9750", "10250", "10550", "10800", "11350"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Hugging"]], "filtered_resps": ["(C) Hugging"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "959eb616d5a8e27416abc4ad871ac5ad137b0b7cbf27a27aef500f0c94a27a17", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Fighting\n(C) Hugging\n(D) Playing football\n(E) Arguing\n(F) Dancing", "task_type": "needleQA", "pred_answer": "C", "answer": "C"}, "input": "In the video, what are the man and woman doing under the tree full of flowers?\n(A) Singing\n(B) Fighting\n(C) Hugging\n(D) Playing football\n(E) Arguing\n(F) Dancing\nOnly give the best option.\nBest option: ("}
{"doc_id": 159, "doc": {"video_name": "test_needle_27.mp4", "duration": 473.22, "question": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 1\n(B) 2\n(C) 5\n(D) 3\n(E) 4\n(F) 6", "candidates": ["1", "2", "5", "3", "4", "6"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_14", "indices": ["348", "580", "1160", "1363", "2146", "2204", "2726", "3277", "3567", "4263", "4553", "5075", "5394", "5887", "6409", "6641", "7221", "7888", "8033", "8555", "9048", "9570", "9744", "10295", "10614", "11310", "11890", "11919", "12586", "12992", "13340", "14123"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c85f1b677bc892dae28055044841572229a166a559dee60e8e018807b469e075", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 1\n(B) 2\n(C) 5\n(D) 3\n(E) 4\n(F) 6", "task_type": "needleQA", "pred_answer": "B", "answer": "F"}, "input": "In the video, under the blue sky and white clouds, how many trees are there in total on both sides of the door of the white building with a flag floating on the roof?\n(A) 1\n(B) 2\n(C) 5\n(D) 3\n(E) 4\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 163, "doc": {"video_name": "test_needle_16.mp4", "duration": 532.92, "question": "In the video, what did the woman take out from the oven after opening it?\n(A) Pizza\n(B) Lamb chops\n(C) Chicken leg\n(D) Chicken wings\n(E) Bread\n(F) Sweet potato", "candidates": ["Pizza", "Lamb chops", "Chicken leg", "Chicken wings", "Bread", "Sweet potato"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_18", "indices": ["225", "450", "1175", "1225", "1950", "2300", "2750", "3300", "3650", "4000", "4275", "4850", "5275", "5775", "5850", "6575", "7000", "7050", "7800", "8075", "8450", "9100", "9125", "9650", "10200", "10600", "10950", "11525", "11625", "12300", "12700", "13000"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c84d5caa2fccbb892bd12f8e39ae45c676a1d677a830be01d2189d6acf795b7", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what did the woman take out from the oven after opening it?\n(A) Pizza\n(B) Lamb chops\n(C) Chicken leg\n(D) Chicken wings\n(E) Bread\n(F) Sweet potato", "task_type": "needleQA", "pred_answer": "B", "answer": "A"}, "input": "In the video, what did the woman take out from the oven after opening it?\n(A) Pizza\n(B) Lamb chops\n(C) Chicken leg\n(D) Chicken wings\n(E) Bread\n(F) Sweet potato\nOnly give the best option.\nBest option: ("}
{"doc_id": 167, "doc": {"video_name": "test_needle_4.mp4", "duration": 518.88, "question": "In the video, how many white swans are there on the water surface?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n(E) 6\n(F) 2", "candidates": ["5", "1", "4", "3", "6", "2"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_22", "indices": ["325", "500", "800", "1550", "1800", "2200", "2600", "3125", "3550", "3925", "4025", "4625", "5225", "5425", "5825", "6350", "6725", "7125", "7325", "8025", "8275", "8750", "9025", "9475", "9875", "10200", "10625", "11250", "11550", "11750", "12150", "12650"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8ea741b49c426a2dace02dceb960c4a729b3173b8d9d159a37317e94a25c049", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, how many white swans are there on the water surface?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n(E) 6\n(F) 2", "task_type": "needleQA", "pred_answer": "B", "answer": "D"}, "input": "In the video, how many white swans are there on the water surface?\n(A) 5\n(B) 1\n(C) 4\n(D) 3\n(E) 6\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 171, "doc": {"video_name": "test_needle_44.mp4", "duration": 455.12, "question": "In the collision event of objects in the video, what is the material of the object to collide with the cyan cylinder?\n(A) plastic\n(B) metal\n(C) rubber\n(D) glass\n(E) wood\n(F) ceramic", "candidates": ["plastic", "metal", "rubber", "glass", "wood", "ceramic"], "answer": "C", "task_type": "needleQA", "question_id": "NQA_26", "indices": ["100", "650", "825", "1075", "1500", "1950", "2225", "2700", "2875", "3375", "3750", "3975", "4425", "4925", "4975", "5425", "5675", "6350", "6450", "6925", "7150", "7750", "8000", "8325", "8700", "8925", "9550", "9700", "10150", "10550", "10975", "11100"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "78fdf65f82bc2ff0acdc6b7fd55950d3c383963a5b8860a23ef15ae0fb6e5605", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what is the material of the object to collide with the cyan cylinder?\n(A) plastic\n(B) metal\n(C) rubber\n(D) glass\n(E) wood\n(F) ceramic", "task_type": "needleQA", "pred_answer": "B", "answer": "C"}, "input": "In the collision event of objects in the video, what is the material of the object to collide with the cyan cylinder?\n(A) plastic\n(B) metal\n(C) rubber\n(D) glass\n(E) wood\n(F) ceramic\nOnly give the best option.\nBest option: ("}
{"doc_id": 175, "doc": {"video_name": "test_needle_42.mp4", "duration": 645.14, "question": "In the collision event of objects in the video, which of the following is not responsible for the collision between the rubber sphere and the cylinder?\n(A) the presence of the metal cube\n(B) the surface texture of the cylinder\n(C) the mass of the rubber sphere\n(D) the cylinder's position\n(E) the gravitational pull on the objects\n(F) the rubber sphere's velocity", "candidates": ["the presence of the metal cube", "the surface texture of the cylinder", "the mass of the rubber sphere", "the cylinder's position", "the gravitational pull on the objects", "the rubber sphere's velocity"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_30", "indices": ["275", "625", "1400", "1575", "2175", "2500", "3125", "3575", "4375", "4675", "5475", "5750", "6150", "6875", "7175", "7975", "8475", "9025", "9175", "9900", "10075", "10775", "11150", "11800", "12175", "13025", "13425", "13725", "14250", "14725", "15525", "15750"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A)"]], "filtered_resps": ["(A)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "028e9693ffbb1f9576382b67936c95f2eb3530b7a53fa709ec14e3eadda1de60", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, which of the following is not responsible for the collision between the rubber sphere and the cylinder?\n(A) the presence of the metal cube\n(B) the surface texture of the cylinder\n(C) the mass of the rubber sphere\n(D) the cylinder's position\n(E) the gravitational pull on the objects\n(F) the rubber sphere's velocity", "task_type": "needleQA", "pred_answer": "A", "answer": "A"}, "input": "In the collision event of objects in the video, which of the following is not responsible for the collision between the rubber sphere and the cylinder?\n(A) the presence of the metal cube\n(B) the surface texture of the cylinder\n(C) the mass of the rubber sphere\n(D) the cylinder's position\n(E) the gravitational pull on the objects\n(F) the rubber sphere's velocity\nOnly give the best option.\nBest option: ("}
{"doc_id": 179, "doc": {"video_name": "test_needle_45.mp4", "duration": 771.14, "question": "In the collision event of objects in the video, which of the following will not happen without the rubber sphere?\n(A) The rubber sphere will collide with the gray cylinder\n(B) The rubber sphere will bounce off the cube\n(C) The cube will remain stationary\n(D) The cube and the gray cylinder collide\n(E) The gray cylinder will roll away\n(F) The cube and the rubber sphere will stick together", "candidates": ["The rubber sphere will collide with the gray cylinder", "The rubber sphere will bounce off the cube", "The cube will remain stationary", "The cube and the gray cylinder collide", "The gray cylinder will roll away", "The cube and the rubber sphere will stick together"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_34", "indices": ["375", "625", "1550", "1925", "2575", "3425", "3825", "4750", "5350", "5425", "6400", "6775", "7375", "7875", "8425", "9225", "10150", "10225", "11250", "11850", "12275", "12625", "13450", "14175", "14550", "15200", "15725", "16700", "17175", "17800", "18400", "19200"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) The cube and the gray cylinder collide"]], "filtered_resps": ["(D) The cube and the gray cylinder collide"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a913cf2410836b78c4859f07c7c35c5919827366d18d3dd00ed595baedf57043", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, which of the following will not happen without the rubber sphere?\n(A) The rubber sphere will collide with the gray cylinder\n(B) The rubber sphere will bounce off the cube\n(C) The cube will remain stationary\n(D) The cube and the gray cylinder collide\n(E) The gray cylinder will roll away\n(F) The cube and the rubber sphere will stick together", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the collision event of objects in the video, which of the following will not happen without the rubber sphere?\n(A) The rubber sphere will collide with the gray cylinder\n(B) The rubber sphere will bounce off the cube\n(C) The cube will remain stationary\n(D) The cube and the gray cylinder collide\n(E) The gray cylinder will roll away\n(F) The cube and the rubber sphere will stick together\nOnly give the best option.\nBest option: ("}
{"doc_id": 183, "doc": {"video_name": "test_needle_23.mp4", "duration": 483.56, "question": "In the video, what fruit falls into the water from above?\n(A) Peach\n(B) Blueberry\n(C) Grape\n(D) Strawberry\n(E) Orange\n(F) Apple", "candidates": ["Peach", "Blueberry", "Grape", "Strawberry", "Orange", "Apple"], "answer": "D", "task_type": "needleQA", "question_id": "NQA_38", "indices": ["25", "725", "1100", "1475", "1725", "2075", "2250", "2625", "3000", "3675", "3925", "4125", "4650", "5200", "5250", "5825", "6025", "6450", "6775", "7200", "7875", "8075", "8350", "8975", "9375", "9450", "9900", "10300", "10775", "11000", "11600", "11925"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc45349e1ff8dc1b8a64badbd62fb560147827c7e6118eebb02ffde139b86d3f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what fruit falls into the water from above?\n(A) Peach\n(B) Blueberry\n(C) Grape\n(D) Strawberry\n(E) Orange\n(F) Apple", "task_type": "needleQA", "pred_answer": "D", "answer": "D"}, "input": "In the video, what fruit falls into the water from above?\n(A) Peach\n(B) Blueberry\n(C) Grape\n(D) Strawberry\n(E) Orange\n(F) Apple\nOnly give the best option.\nBest option: ("}
{"doc_id": 187, "doc": {"video_name": "test_needle_5.mp4", "duration": 482.12, "question": "In the video, how many white swans are there on the water surface?\n(A) 4\n(B) 1\n(C) 2\n(D) 6\n(E) 5\n(F) 3", "candidates": ["4", "1", "2", "6", "5", "3"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_42", "indices": ["350", "425", "825", "1125", "1725", "2150", "2250", "2950", "3050", "3425", "4075", "4175", "4625", "5075", "5475", "5625", "6075", "6650", "6925", "7400", "7775", "8225", "8350", "8800", "9275", "9675", "9775", "10475", "10625", "10950", "11400", "11650"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 1"]], "filtered_resps": ["(B) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b52bc1bf4f51398e7668e4d72bf1f55d623fbadd82129b8407138151dc13eab4", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, how many white swans are there on the water surface?\n(A) 4\n(B) 1\n(C) 2\n(D) 6\n(E) 5\n(F) 3", "task_type": "needleQA", "pred_answer": "B", "answer": "F"}, "input": "In the video, how many white swans are there on the water surface?\n(A) 4\n(B) 1\n(C) 2\n(D) 6\n(E) 5\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 191, "doc": {"video_name": "test_needle_19.mp4", "duration": 464.18, "question": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Square\n(B) Heart shape\n(C) Check mark\n(D) Triangle\n(E) Circle\n(F) Cross", "candidates": ["Square", "Heart shape", "Check mark", "Triangle", "Circle", "Cross"], "answer": "F", "task_type": "needleQA", "question_id": "NQA_46", "indices": ["29", "522", "899", "1276", "1827", "2175", "2987", "3045", "3683", "3944", "4669", "4756", "5510", "6032", "6061", "6728", "7018", "7656", "8091", "8265", "8729", "9135", "9802", "10063", "10788", "11136", "11281", "12122", "12354", "12760", "13398", "13514"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0beb2a201781bb79ef89a18c43c5f5b23308178b57f9efab5a152a992cbf8ff8", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Square\n(B) Heart shape\n(C) Check mark\n(D) Triangle\n(E) Circle\n(F) Cross", "task_type": "needleQA", "pred_answer": "B", "answer": "F"}, "input": "In the video, what pattern did the woman in the blue nurse uniform draw in the air with a pen?\n(A) Square\n(B) Heart shape\n(C) Check mark\n(D) Triangle\n(E) Circle\n(F) Cross\nOnly give the best option.\nBest option: ("}
{"doc_id": 195, "doc": {"video_name": "test_needle_10.mp4", "duration": 472.64, "question": "In the video, what falls into the transparent glass bowl?\n(A) Cereal\n(B) Milk\n(C) Apple\n(D) Egg\n(E) Banana\n(F) Blueberries", "candidates": ["Cereal", "Milk", "Apple", "Egg", "Banana", "Blueberries"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_50", "indices": ["203", "725", "986", "1624", "2001", "2204", "2639", "3306", "3857", "4118", "4698", "4988", "5568", "6061", "6206", "6844", "7482", "7598", "8294", "8758", "8845", "9280", "9918", "10498", "10759", "11339", "11861", "12064", "12412", "12963", "13398", "13949"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "298e02c6e13851b5aefc0ea4dfbd0e8899e6cd2a51f74e4541320fe7a4d45eb6", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the video, what falls into the transparent glass bowl?\n(A) Cereal\n(B) Milk\n(C) Apple\n(D) Egg\n(E) Banana\n(F) Blueberries", "task_type": "needleQA", "pred_answer": "F", "answer": "A"}, "input": "In the video, what falls into the transparent glass bowl?\n(A) Cereal\n(B) Milk\n(C) Apple\n(D) Egg\n(E) Banana\n(F) Blueberries\nOnly give the best option.\nBest option: ("}
{"doc_id": 199, "doc": {"video_name": "test_needle_47.mp4", "duration": 482.14, "question": "In the collision event of objects in the video, what material is the last object that enters the scene?\n(A) rubber\n(B) ceramic\n(C) wood\n(D) metal\n(E) glass\n(F) plastic", "candidates": ["rubber", "ceramic", "wood", "metal", "glass", "plastic"], "answer": "A", "task_type": "needleQA", "question_id": "NQA_54", "indices": ["325", "725", "750", "1450", "1800", "2200", "2500", "2650", "3025", "3475", "3825", "4350", "4850", "4950", "5450", "5925", "6025", "6700", "7075", "7175", "7775", "8075", "8500", "8675", "9200", "9475", "9950", "10375", "10825", "11000", "11600", "11800"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c5722f8946b873b73a3585e3c6927ddbbcea7d3c175ed28555071d88a92564a", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In the collision event of objects in the video, what material is the last object that enters the scene?\n(A) rubber\n(B) ceramic\n(C) wood\n(D) metal\n(E) glass\n(F) plastic", "task_type": "needleQA", "pred_answer": "E", "answer": "A"}, "input": "In the collision event of objects in the video, what material is the last object that enters the scene?\n(A) rubber\n(B) ceramic\n(C) wood\n(D) metal\n(E) glass\n(F) plastic\nOnly give the best option.\nBest option: ("}
{"doc_id": 203, "doc": {"video_name": "test_needle_14.mp4", "duration": 698.42, "question": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) White\n(B) Purple\n(C) Blue\n(D) Gray\n(E) Pink\n(F) Green", "candidates": ["White", "Purple", "Blue", "Gray", "Pink", "Green"], "answer": "E", "task_type": "needleQA", "question_id": "NQA_58", "indices": ["708", "2124", "3540", "4071", "6254", "6608", "8437", "10207", "11682", "11859", "14219", "14514", "16461", "17759", "18821", "20473", "21299", "22361", "24662", "25488", "27199", "27494", "29205", "30680", "32332", "33866", "35046", "35282", "37642", "38822", "39294", "41359"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac353ed4b79469c5e75ba2d4773dd1d3686b40ba5fce7527adf6b65ca4d93681", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) White\n(B) Purple\n(C) Blue\n(D) Gray\n(E) Pink\n(F) Green", "task_type": "needleQA", "pred_answer": "E", "answer": "E"}, "input": "In the video, what color clothes is the woman with beige hair wearing while carefully listening to the doctor's explanation?\n(A) White\n(B) Purple\n(C) Blue\n(D) Gray\n(E) Pink\n(F) Green\nOnly give the best option.\nBest option: ("}
{"doc_id": 207, "doc": {"video_name": "test_cartoon_18.mp4", "duration": 355.39, "question": "What did the mouse find in the box after running into the mouse hole?\n(A) Knife\n(B) White flag\n(C) Sword\n(D) Cheese\n(E) Apple\n(F) Stick", "candidates": ["Knife", "White flag", "Sword", "Cheese", "Apple", "Stick"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_2", "indices": ["0", "522", "638", "1189", "1479", "1885", "1972", "2494", "2784", "3161", "3567", "3654", "3973", "4292", "4814", "5278", "5307", "5945", "6003", "6380", "6786", "7192", "7569", "7830", "8149", "8381", "8845", "9164", "9454", "9715", "10237", "10353"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "34a5b92c66daf21e535ca1d747f80a257f1a28f74d82e3c94746a35d878e8e6f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What did the mouse find in the box after running into the mouse hole?\n(A) Knife\n(B) White flag\n(C) Sword\n(D) Cheese\n(E) Apple\n(F) Stick", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What did the mouse find in the box after running into the mouse hole?\n(A) Knife\n(B) White flag\n(C) Sword\n(D) Cheese\n(E) Apple\n(F) Stick\nOnly give the best option.\nBest option: ("}
{"doc_id": 211, "doc": {"video_name": "test_entv_13.mp4", "duration": 678.6181831810065, "question": "In the video, what did the man do after grabbing the woman's hand?\n(A) Hurt her\n(B) Put on a bracelet\n(C) Cut her nails\n(D) Put on a ring\n(E) Applied a band-aid\n(F) Applied nail polish", "candidates": ["Hurt her", "Put on a bracelet", "Cut her nails", "Put on a ring", "Applied a band-aid", "Applied nail polish"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_6", "indices": ["450", "875", "1550", "1950", "2100", "2625", "3625", "4025", "4450", "5175", "5375", "5800", "6800", "7300", "7400", "7925", "8850", "9000", "9675", "10375", "10600", "11125", "11900", "12300", "12750", "13450", "13825", "14375", "14975", "15475", "16300", "16425"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c19334c2698d53e2c97ccfb6f8bf75cd618bd98bda4ab24bd6b678305a72c4b2", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In the video, what did the man do after grabbing the woman's hand?\n(A) Hurt her\n(B) Put on a bracelet\n(C) Cut her nails\n(D) Put on a ring\n(E) Applied a band-aid\n(F) Applied nail polish", "task_type": "plotQA", "pred_answer": "D", "answer": "E"}, "input": "In the video, what did the man do after grabbing the woman's hand?\n(A) Hurt her\n(B) Put on a bracelet\n(C) Cut her nails\n(D) Put on a ring\n(E) Applied a band-aid\n(F) Applied nail polish\nOnly give the best option.\nBest option: ("}
{"doc_id": 215, "doc": {"video_name": "test_entv_5.mp4", "duration": 700.678139986795, "question": "In the video, what is floating on the water?\n(A) Water Bottles\n(B) Seaweed\n(C) Ropes\n(D) Sailboats\n(E) Feathers\n(F) Plastic Bags", "candidates": ["Water Bottles", "Seaweed", "Ropes", "Sailboats", "Feathers", "Plastic Bags"], "answer": "D", "task_type": "plotQA", "question_id": "PQA_10", "indices": ["276", "506", "1173", "1679", "2415", "2990", "3289", "4140", "4577", "4738", "5543", "5934", "6371", "6900", "7613", "8349", "8395", "9085", "9936", "10281", "10925", "11293", "11523", "12075", "12995", "13501", "14122", "14398", "14858", "15364", "16054", "16422"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95289eab06ef6e42c19cc509260cb8ba76ae593f605a355dba521768581deaf4", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In the video, what is floating on the water?\n(A) Water Bottles\n(B) Seaweed\n(C) Ropes\n(D) Sailboats\n(E) Feathers\n(F) Plastic Bags", "task_type": "plotQA", "pred_answer": "B", "answer": "D"}, "input": "In the video, what is floating on the water?\n(A) Water Bottles\n(B) Seaweed\n(C) Ropes\n(D) Sailboats\n(E) Feathers\n(F) Plastic Bags\nOnly give the best option.\nBest option: ("}
{"doc_id": 219, "doc": {"video_name": "test_entv_2.mp4", "duration": 403.40632292212683, "question": "What color is the young man's hair at the beginning of the video?\n(A) Yellow\n(B) Brown\n(C) White\n(D) Black\n(E) Pink\n(F) Blue", "candidates": ["Yellow", "Brown", "White", "Black", "Pink", "Blue"], "answer": "C", "task_type": "plotQA", "question_id": "PQA_14", "indices": ["23", "575", "598", "1104", "1288", "1587", "1955", "2369", "2438", "2921", "3289", "3427", "3841", "4163", "4209", "4807", "4945", "5221", "5451", "5957", "6072", "6440", "6762", "7015", "7291", "7682", "7912", "8395", "8487", "8993", "9246", "9522"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d76a155ff5857bea20e2278697327e99174c2e97cf79d15771d543359dc49e6", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What color is the young man's hair at the beginning of the video?\n(A) Yellow\n(B) Brown\n(C) White\n(D) Black\n(E) Pink\n(F) Blue", "task_type": "plotQA", "pred_answer": "A", "answer": "C"}, "input": "What color is the young man's hair at the beginning of the video?\n(A) Yellow\n(B) Brown\n(C) White\n(D) Black\n(E) Pink\n(F) Blue\nOnly give the best option.\nBest option: ("}
{"doc_id": 223, "doc": {"video_name": "test_entv_18.mp4", "duration": 651.8112954069612, "question": "In the video, what is the man in the black T-shirt crying while looking at?\n(A) Scarf\n(B) Photo\n(C) Glove\n(D) Computer\n(E) Stone\n(F) Ring", "candidates": ["Scarf", "Photo", "Glove", "Computer", "Stone", "Ring"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_18", "indices": ["0", "1044", "1566", "2349", "2726", "3277", "3828", "4785", "4872", "5684", "6206", "7279", "7627", "8352", "8816", "9628", "10063", "10585", "11107", "11600", "12470", "13021", "13775", "14355", "14993", "15631", "16327", "16907", "17574", "17690", "18560", "19024"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "06daec722ab4c6905105f3e0160a250006af196d3ef64a1dc8417da1ace523b4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In the video, what is the man in the black T-shirt crying while looking at?\n(A) Scarf\n(B) Photo\n(C) Glove\n(D) Computer\n(E) Stone\n(F) Ring", "task_type": "plotQA", "pred_answer": "D", "answer": "B"}, "input": "In the video, what is the man in the black T-shirt crying while looking at?\n(A) Scarf\n(B) Photo\n(C) Glove\n(D) Computer\n(E) Stone\n(F) Ring\nOnly give the best option.\nBest option: ("}
{"doc_id": 227, "doc": {"video_name": "test_cartoon_22.mp4", "duration": 604.2316257134521, "question": "What did the big snake turn into after being defeated in the video?\n(A) Dust\n(B) Stone\n(C) Mud\n(D) Clean water\n(E) Plant\n(F) Sewage", "candidates": ["Dust", "Stone", "Mud", "Clean water", "Plant", "Sewage"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_22", "indices": ["200", "500", "1000", "1800", "2225", "2550", "2875", "3725", "3875", "4500", "5050", "5225", "5700", "6500", "6700", "7525", "7825", "8075", "8875", "9275", "9525", "10225", "10425", "10875", "11750", "12050", "12250", "12825", "13500", "14100", "14150", "14875"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "115f7840c9e621179bbc6cf1c06ec8410a3313ab2d5a2b989843a4100a112b09", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What did the big snake turn into after being defeated in the video?\n(A) Dust\n(B) Stone\n(C) Mud\n(D) Clean water\n(E) Plant\n(F) Sewage", "task_type": "plotQA", "pred_answer": "D", "answer": "F"}, "input": "What did the big snake turn into after being defeated in the video?\n(A) Dust\n(B) Stone\n(C) Mud\n(D) Clean water\n(E) Plant\n(F) Sewage\nOnly give the best option.\nBest option: ("}
{"doc_id": 231, "doc": {"video_name": "test_cartoon_1.mp4", "duration": 483.07277143454803, "question": "What did the yellow sponge use to fly into the sky?\n(A) Wings\n(B) Spoon\n(C) Boat\n(D) Pot Lid\n(E) Bamboo Dragonfly\n(F) Shovel", "candidates": ["Wings", "Spoon", "Boat", "Pot Lid", "Bamboo Dragonfly", "Shovel"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_26", "indices": ["0", "525", "875", "1175", "1700", "2150", "2275", "2750", "3050", "3700", "3750", "4425", "4825", "4900", "5350", "5625", "6075", "6525", "6775", "7350", "7750", "8200", "8375", "9000", "9300", "9475", "10050", "10525", "10700", "11175", "11325", "12025"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9f3d11b9139b566c5fdc4e91fd0328ee7ea07f82660864247a04d127a1665467", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What did the yellow sponge use to fly into the sky?\n(A) Wings\n(B) Spoon\n(C) Boat\n(D) Pot Lid\n(E) Bamboo Dragonfly\n(F) Shovel", "task_type": "plotQA", "pred_answer": "D", "answer": "F"}, "input": "What did the yellow sponge use to fly into the sky?\n(A) Wings\n(B) Spoon\n(C) Boat\n(D) Pot Lid\n(E) Bamboo Dragonfly\n(F) Shovel\nOnly give the best option.\nBest option: ("}
{"doc_id": 235, "doc": {"video_name": "test_cartoon_22.mp4", "duration": 604.2316257134521, "question": "What rescued the fish being chased by lava in the video?\n(A) Turtle\n(B) Octopus\n(C) Shrimp\n(D) Seahorse\n(E) Snake\n(F) Bird", "candidates": ["Turtle", "Octopus", "Shrimp", "Seahorse", "Snake", "Bird"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_30", "indices": ["300", "625", "1250", "1450", "2175", "2775", "3275", "3325", "3825", "4650", "5025", "5275", "6050", "6200", "6775", "7425", "7725", "8450", "8600", "9400", "9650", "10050", "10725", "11175", "11325", "11775", "12600", "13050", "13350", "14050", "14350", "15075"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd3c112e7b28a2d25493964436fcb27b6c330695b88e93f8c8053f9fefce07c4", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What rescued the fish being chased by lava in the video?\n(A) Turtle\n(B) Octopus\n(C) Shrimp\n(D) Seahorse\n(E) Snake\n(F) Bird", "task_type": "plotQA", "pred_answer": "D", "answer": "F"}, "input": "What rescued the fish being chased by lava in the video?\n(A) Turtle\n(B) Octopus\n(C) Shrimp\n(D) Seahorse\n(E) Snake\n(F) Bird\nOnly give the best option.\nBest option: ("}
{"doc_id": 239, "doc": {"video_name": "test_movie101_94.mp4", "duration": 439.0, "question": "What is inside the black bag that the two people are fighting over in the video?\n(A) Rocks\n(B) Painting\n(C) Phone\n(D) Diamonds\n(E) Gold\n(F) Cash", "candidates": ["Rocks", "Painting", "Phone", "Diamonds", "Gold", "Cash"], "answer": "F", "task_type": "plotQA", "question_id": "PQA_34", "indices": ["150", "525", "950", "1300", "1525", "1950", "2025", "2700", "2975", "3275", "3400", "3850", "4250", "4550", "4850", "5150", "5550", "6075", "6175", "6775", "7000", "7350", "7600", "7900", "8375", "8750", "8925", "9275", "9875", "9950", "10600", "10950"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "28bf4cdbe44feb9eaf7af807cc4996413c72f712d96e56ade04f225db7e0aa67", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What is inside the black bag that the two people are fighting over in the video?\n(A) Rocks\n(B) Painting\n(C) Phone\n(D) Diamonds\n(E) Gold\n(F) Cash", "task_type": "plotQA", "pred_answer": "F", "answer": "F"}, "input": "What is inside the black bag that the two people are fighting over in the video?\n(A) Rocks\n(B) Painting\n(C) Phone\n(D) Diamonds\n(E) Gold\n(F) Cash\nOnly give the best option.\nBest option: ("}
{"doc_id": 243, "doc": {"video_name": "test_cartoon_10.mp4", "duration": 586.918084061671, "question": "How did the yellow sponge calm down after receiving the letter in the video?\n(A) Jumped rope\n(B) Went for a run\n(C) Sang with the snail\n(D) Cooked\n(E) Got splashed with a bucket of water by the snail\n(F) Banged head against the wall", "candidates": ["Jumped rope", "Went for a run", "Sang with the snail", "Cooked", "Got splashed with a bucket of water by the snail", "Banged head against the wall"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_38", "indices": ["275", "825", "950", "1550", "1875", "2275", "3025", "3425", "4050", "4350", "4800", "5125", "5800", "6225", "6800", "6850", "7550", "7875", "8550", "9100", "9325", "9875", "10250", "10900", "11000", "11450", "12300", "12400", "12825", "13300", "13825", "14350"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03ef12794f11281f3f6d5e7136f5e461990f3ba56b95be5535a07b5a6bd1fda1", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How did the yellow sponge calm down after receiving the letter in the video?\n(A) Jumped rope\n(B) Went for a run\n(C) Sang with the snail\n(D) Cooked\n(E) Got splashed with a bucket of water by the snail\n(F) Banged head against the wall", "task_type": "plotQA", "pred_answer": "E", "answer": "E"}, "input": "How did the yellow sponge calm down after receiving the letter in the video?\n(A) Jumped rope\n(B) Went for a run\n(C) Sang with the snail\n(D) Cooked\n(E) Got splashed with a bucket of water by the snail\n(F) Banged head against the wall\nOnly give the best option.\nBest option: ("}
{"doc_id": 247, "doc": {"video_name": "test_movie101_98.mp4", "duration": 394.0, "question": "What is the woman at the banquet wearing?\n(A) Hanfu\n(B) Hanbok\n(C) Tang suit\n(D) Kimono\n(E) Dress\n(F) Shirt", "candidates": ["Hanfu", "Hanbok", "Tang suit", "Kimono", "Dress", "Shirt"], "answer": "B", "task_type": "plotQA", "question_id": "PQA_42", "indices": ["25", "350", "775", "1000", "1475", "1625", "1850", "2325", "2525", "2975", "3125", "3500", "3900", "4250", "4475", "4675", "4925", "5250", "5775", "5825", "6425", "6700", "6900", "7050", "7425", "7800", "8250", "8550", "8750", "9200", "9225", "9750"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "744b7a39c3a36278c54aa19a75d99d51abc5323db7c2b01b9636900e713bef01", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the woman at the banquet wearing?\n(A) Hanfu\n(B) Hanbok\n(C) Tang suit\n(D) Kimono\n(E) Dress\n(F) Shirt", "task_type": "plotQA", "pred_answer": "B", "answer": "B"}, "input": "What is the woman at the banquet wearing?\n(A) Hanfu\n(B) Hanbok\n(C) Tang suit\n(D) Kimono\n(E) Dress\n(F) Shirt\nOnly give the best option.\nBest option: ("}
{"doc_id": 251, "doc": {"video_name": "test_entv_1.mp4", "duration": 594.6644105054177, "question": "What are the girls doing at the beginning of the video?\n(A) Archery\n(B) Makeup\n(C) Playing\n(D) Playing ping pong\n(E) Embroidery\n(F) Acupuncture", "candidates": ["Archery", "Makeup", "Playing", "Playing ping pong", "Embroidery", "Acupuncture"], "answer": "E", "task_type": "plotQA", "question_id": "PQA_46", "indices": ["69", "690", "1288", "1380", "1978", "2438", "2806", "3519", "3910", "4255", "4439", "5129", "5727", "5957", "6210", "6923", "7406", "7544", "8395", "8418", "9177", "9660", "10166", "10580", "10672", "11201", "11546", "12259", "12489", "13294", "13754", "13800"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eef81acb08d3ca45c957550febf597e8df27e11910d7133e97201a77509a45b5", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What are the girls doing at the beginning of the video?\n(A) Archery\n(B) Makeup\n(C) Playing\n(D) Playing ping pong\n(E) Embroidery\n(F) Acupuncture", "task_type": "plotQA", "pred_answer": "E", "answer": "E"}, "input": "What are the girls doing at the beginning of the video?\n(A) Archery\n(B) Makeup\n(C) Playing\n(D) Playing ping pong\n(E) Embroidery\n(F) Acupuncture\nOnly give the best option.\nBest option: ("}
{"doc_id": 255, "doc": {"video_name": "test_game_2.mp4", "duration": 329.0, "question": "Under what kind of scenario is this building being constructed?\n(A) City\n(B) Beach\n(C) Grassland\n(D) Forest\n(E) Snowy terrain\n(F) Desert", "candidates": ["City", "Beach", "Grassland", "Forest", "Snowy terrain", "Desert"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_0", "indices": ["120", "480", "750", "1170", "1230", "1650", "2070", "2190", "2730", "2850", "3330", "3600", "3690", "4230", "4290", "4890", "4950", "5430", "5760", "5910", "6330", "6630", "6960", "7170", "7650", "7710", "8220", "8520", "8670", "9030", "9330", "9570"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "945040c9befeb220accbbde32a27bc8bab11b642c565ce961a763361e7885ae6", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Under what kind of scenario is this building being constructed?\n(A) City\n(B) Beach\n(C) Grassland\n(D) Forest\n(E) Snowy terrain\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "Under what kind of scenario is this building being constructed?\n(A) City\n(B) Beach\n(C) Grassland\n(D) Forest\n(E) Snowy terrain\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 259, "doc": {"video_name": "test_cartoon_21.mp4", "duration": 662.3029651724227, "question": "Who is the main character of the entire video?\n(A) Rabbit\n(B) Dog\n(C) Turtle\n(D) Cow\n(E) Cat\n(F) Snake", "candidates": ["Rabbit", "Dog", "Turtle", "Cow", "Cat", "Snake"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_4", "indices": ["0", "825", "1425", "1775", "2350", "2925", "3275", "3725", "4550", "4850", "5250", "6125", "6400", "6750", "7225", "8250", "8425", "8975", "9750", "9950", "10450", "11275", "11475", "12300", "12875", "13275", "13725", "14250", "14900", "15275", "15700", "16300"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f46f2874e90dc44db383e09ccc417cbd1f49b00d2a868a273d51672a2c75e6bb", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Who is the main character of the entire video?\n(A) Rabbit\n(B) Dog\n(C) Turtle\n(D) Cow\n(E) Cat\n(F) Snake", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "Who is the main character of the entire video?\n(A) Rabbit\n(B) Dog\n(C) Turtle\n(D) Cow\n(E) Cat\n(F) Snake\nOnly give the best option.\nBest option: ("}
{"doc_id": 263, "doc": {"video_name": "test_entv_22.mp4", "duration": 407.116439508766, "question": "In what setting does the video take place?\n(A) Ocean\n(B) Desert\n(C) Campus\n(D) Forest\n(E) Countryside\n(F) Urban", "candidates": ["Ocean", "Desert", "Campus", "Forest", "Countryside", "Urban"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_8", "indices": ["23", "322", "736", "943", "1426", "1794", "1863", "2116", "2599", "3013", "3174", "3450", "3749", "4163", "4531", "4715", "4876", "5451", "5681", "5773", "6371", "6394", "6969", "7015", "7590", "7636", "7958", "8395", "8648", "8901", "9200", "9729"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Urban"]], "filtered_resps": ["(F) Urban"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "69a6e9bdba33e796431b6b3d0efbb002786b0d443f10e8caa0d29117123029e5", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In what setting does the video take place?\n(A) Ocean\n(B) Desert\n(C) Campus\n(D) Forest\n(E) Countryside\n(F) Urban", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "In what setting does the video take place?\n(A) Ocean\n(B) Desert\n(C) Campus\n(D) Forest\n(E) Countryside\n(F) Urban\nOnly give the best option.\nBest option: ("}
{"doc_id": 267, "doc": {"video_name": "test_game_1.mp4", "duration": 316.0, "question": "What kind of building is most likely being constructed in the video?\n(A) Skyscraper\n(B) Lighthouse\n(C) Bungalow\n(D) Villa\n(E) Temple\n(F) Palace", "candidates": ["Skyscraper", "Lighthouse", "Bungalow", "Villa", "Temple", "Palace"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_12", "indices": ["120", "450", "780", "900", "1440", "1500", "1980", "2190", "2490", "2640", "2970", "3390", "3660", "3990", "4260", "4620", "4980", "5040", "5550", "5610", "6150", "6420", "6750", "7050", "7140", "7470", "7710", "8220", "8520", "8760", "9150", "9360"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8df9d6c2855c97a5d9130e480c87a4d5e588b3fecf04225975bc2b3402a4572", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What kind of building is most likely being constructed in the video?\n(A) Skyscraper\n(B) Lighthouse\n(C) Bungalow\n(D) Villa\n(E) Temple\n(F) Palace", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What kind of building is most likely being constructed in the video?\n(A) Skyscraper\n(B) Lighthouse\n(C) Bungalow\n(D) Villa\n(E) Temple\n(F) Palace\nOnly give the best option.\nBest option: ("}
{"doc_id": 271, "doc": {"video_name": "test_food_5.mp4", "duration": 721.77, "question": "What is the main environment in which the video takes place?\n(A) Classroom\n(B) Shopping Mall\n(C) Park\n(D) Field\n(E) Factory\n(F) Street", "candidates": ["Classroom", "Shopping Mall", "Park", "Field", "Factory", "Street"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_16", "indices": ["390", "1230", "1440", "2370", "3270", "3600", "4560", "5070", "5850", "6240", "7320", "7620", "8370", "9360", "9630", "10320", "10920", "11580", "12390", "12900", "13710", "14220", "14970", "15750", "16380", "17100", "17670", "18750", "19410", "19920", "20700", "21330"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4c5eddf6201e1b83876fe0b97d7cf4f42d44da4ca412aed0e293a65cef65b93", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the main environment in which the video takes place?\n(A) Classroom\n(B) Shopping Mall\n(C) Park\n(D) Field\n(E) Factory\n(F) Street", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What is the main environment in which the video takes place?\n(A) Classroom\n(B) Shopping Mall\n(C) Park\n(D) Field\n(E) Factory\n(F) Street\nOnly give the best option.\nBest option: ("}
{"doc_id": 275, "doc": {"video_name": "test_UWA-6.mp4", "duration": 450.0, "question": "Which animal is primarily discussed in the second half of the video?\n(A) Elephant\n(B) Rhino\n(C) Crocodile\n(D) Deer\n(E) Dragonfly\n(F) Bird", "candidates": ["Elephant", "Rhino", "Crocodile", "Deer", "Dragonfly", "Bird"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_20", "indices": ["300", "425", "1025", "1375", "1425", "1775", "2425", "2650", "3000", "3200", "3500", "4150", "4225", "4775", "5225", "5425", "5950", "5975", "6375", "6975", "7075", "7400", "7850", "8100", "8750", "9025", "9150", "9750", "10150", "10475", "10625", "10875"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "93c09d3cf81387dd296ab6c18839c56e116aa8dfb19fdac9714b62f07bd86f04", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Which animal is primarily discussed in the second half of the video?\n(A) Elephant\n(B) Rhino\n(C) Crocodile\n(D) Deer\n(E) Dragonfly\n(F) Bird", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "Which animal is primarily discussed in the second half of the video?\n(A) Elephant\n(B) Rhino\n(C) Crocodile\n(D) Deer\n(E) Dragonfly\n(F) Bird\nOnly give the best option.\nBest option: ("}
{"doc_id": 279, "doc": {"video_name": "test_cartoon_2.mp4", "duration": 733.4075768505323, "question": "Which animal does not appear in the entire video?\n(A) Octopus\n(B) Fish\n(C) Starfish\n(D) Snail\n(E) Dog\n(F) Squirrel", "candidates": ["Octopus", "Fish", "Starfish", "Snail", "Dog", "Squirrel"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_24", "indices": ["525", "550", "1375", "2125", "2400", "3225", "3600", "4100", "4925", "5550", "5950", "6525", "7275", "7650", "8175", "9100", "9450", "9900", "10825", "10875", "11450", "12125", "12650", "13225", "14125", "14575", "15400", "15950", "16500", "17125", "17275", "18050"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "29678f9ffed3af2ea433c9c021e87b8fc2b6c70416c0ac6bbf3a9d31a3343fe8", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which animal does not appear in the entire video?\n(A) Octopus\n(B) Fish\n(C) Starfish\n(D) Snail\n(E) Dog\n(F) Squirrel", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "Which animal does not appear in the entire video?\n(A) Octopus\n(B) Fish\n(C) Starfish\n(D) Snail\n(E) Dog\n(F) Squirrel\nOnly give the best option.\nBest option: ("}
{"doc_id": 283, "doc": {"video_name": "test_cartoon_4.mp4", "duration": 632.9816415843876, "question": "In what environment does the scene in the video take place?\n(A) Forest\n(B) City\n(C) Snow Mountain\n(D) Grassland\n(E) Ocean\n(F) Desert", "candidates": ["Forest", "City", "Snow Mountain", "Grassland", "Ocean", "Desert"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_28", "indices": ["0", "525", "1250", "1925", "2350", "2500", "3300", "3450", "4225", "4850", "4925", "5425", "6100", "6475", "7200", "7600", "8200", "8475", "9300", "9400", "9950", "10400", "11275", "11675", "11925", "12650", "12850", "13600", "14275", "14725", "15200", "15600"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e1fe1225d54f14ee22b2616d97504acfecec57a2612c82b72bf7fccbcf9eb6df", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Forest\n(B) City\n(C) Snow Mountain\n(D) Grassland\n(E) Ocean\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "In what environment does the scene in the video take place?\n(A) Forest\n(B) City\n(C) Snow Mountain\n(D) Grassland\n(E) Ocean\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 287, "doc": {"video_name": "test_movie101_97.mp4", "duration": 556.0, "question": "What event does the entire video depict?\n(A) Romantic event\n(B) Pursuit event\n(C) Natural disaster event\n(D) Political event\n(E) Medical event\n(F) Sports event", "candidates": ["Romantic event", "Pursuit event", "Natural disaster event", "Political event", "Medical event", "Sports event"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_32", "indices": ["0", "550", "1100", "1350", "2125", "2300", "2775", "3075", "3675", "4225", "4575", "4925", "5400", "6050", "6225", "6875", "7250", "7650", "8000", "8600", "8800", "9250", "9625", "10175", "10450", "11225", "11450", "11925", "12200", "12600", "13050", "13700"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Pursuit event"]], "filtered_resps": ["(B) Pursuit event"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0f3b0cfd9ce39cfbe3e02a0e7252685575c8cac35f3801b9d6b633c273ffe89", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What event does the entire video depict?\n(A) Romantic event\n(B) Pursuit event\n(C) Natural disaster event\n(D) Political event\n(E) Medical event\n(F) Sports event", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What event does the entire video depict?\n(A) Romantic event\n(B) Pursuit event\n(C) Natural disaster event\n(D) Political event\n(E) Medical event\n(F) Sports event\nOnly give the best option.\nBest option: ("}
{"doc_id": 291, "doc": {"video_name": "test_movie101_94.mp4", "duration": 439.0, "question": "What type of film is this?\n(A) Romance\n(B) Crime thriller\n(C) Horror\n(D) Sci-fi\n(E) Documentary\n(F) War", "candidates": ["Romance", "Crime thriller", "Horror", "Sci-fi", "Documentary", "War"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_36", "indices": ["0", "625", "725", "1125", "1450", "2000", "2175", "2550", "3025", "3175", "3625", "3825", "4250", "4650", "5025", "5125", "5625", "6125", "6175", "6600", "7075", "7350", "7625", "8050", "8475", "8825", "9200", "9500", "9850", "10150", "10500", "10950"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Crime thriller"]], "filtered_resps": ["(B) Crime thriller"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02823a4a6bd1af72281bea00ee4df5b9a0d056106c063326ed3afc3080251808", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Romance\n(B) Crime thriller\n(C) Horror\n(D) Sci-fi\n(E) Documentary\n(F) War", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What type of film is this?\n(A) Romance\n(B) Crime thriller\n(C) Horror\n(D) Sci-fi\n(E) Documentary\n(F) War\nOnly give the best option.\nBest option: ("}
{"doc_id": 295, "doc": {"video_name": "test_entv_5.mp4", "duration": 700.678139986795, "question": "What genre is this film?\n(A) Documentary\n(B) Horror\n(C) Adventure\n(D) War\n(E) Romance\n(F) Action", "candidates": ["Documentary", "Horror", "Adventure", "War", "Romance", "Action"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_40", "indices": ["483", "805", "1127", "1748", "2093", "3105", "3473", "3887", "4600", "5037", "5428", "6026", "6348", "7084", "7567", "7889", "8602", "9085", "9913", "10373", "10580", "11477", "12006", "12075", "13064", "13501", "14053", "14237", "14812", "15663", "15985", "16445"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) War"]], "filtered_resps": ["(D) War"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c20f53c207b3e4e584bb74990b8138c8c0d07c881af69770626d135bce96d01", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Documentary\n(B) Horror\n(C) Adventure\n(D) War\n(E) Romance\n(F) Action", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "C"}, "input": "What genre is this film?\n(A) Documentary\n(B) Horror\n(C) Adventure\n(D) War\n(E) Romance\n(F) Action\nOnly give the best option.\nBest option: ("}
{"doc_id": 299, "doc": {"video_name": "test_game_4.mp4", "duration": 244.0, "question": "What is the main image being constructed in the video?\n(A) Middle-aged man\n(B) Little girl\n(C) Elderly man\n(D) Little boy\n(E) Middle-aged woman\n(F) Elderly woman", "candidates": ["Middle-aged man", "Little girl", "Elderly man", "Little boy", "Middle-aged woman", "Elderly woman"], "answer": "B", "task_type": "topic_reasoning", "question_id": "TR_44", "indices": ["120", "360", "450", "840", "960", "1320", "1560", "1680", "2010", "2100", "2340", "2610", "2730", "3060", "3180", "3420", "3810", "4020", "4260", "4350", "4620", "4830", "5010", "5400", "5640", "5760", "5940", "6240", "6540", "6600", "6990", "7080"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "45e4cbea0d5b846ea1e3541002d2e68ff6bba345341d49c5952f4a070266d87a", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the main image being constructed in the video?\n(A) Middle-aged man\n(B) Little girl\n(C) Elderly man\n(D) Little boy\n(E) Middle-aged woman\n(F) Elderly woman", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "B"}, "input": "What is the main image being constructed in the video?\n(A) Middle-aged man\n(B) Little girl\n(C) Elderly man\n(D) Little boy\n(E) Middle-aged woman\n(F) Elderly woman\nOnly give the best option.\nBest option: ("}
{"doc_id": 303, "doc": {"video_name": "test_cartoon_1.mp4", "duration": 483.07277143454803, "question": "In what environment does the video take place?\n(A) Campus\n(B) Desert\n(C) Underwater\n(D) Snowfield\n(E) Forest\n(F) Library", "candidates": ["Campus", "Desert", "Underwater", "Snowfield", "Forest", "Library"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_48", "indices": ["100", "700", "850", "1275", "1700", "2125", "2575", "2800", "3325", "3500", "4050", "4450", "4750", "5000", "5375", "5750", "6250", "6700", "7125", "7250", "7550", "8075", "8325", "8825", "9100", "9450", "10150", "10275", "10700", "11100", "11300", "11875"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4fcf31dab18d4242cdfd960b65a655e3e25731da3cd2f4529e927e51a25a7785", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In what environment does the video take place?\n(A) Campus\n(B) Desert\n(C) Underwater\n(D) Snowfield\n(E) Forest\n(F) Library", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "In what environment does the video take place?\n(A) Campus\n(B) Desert\n(C) Underwater\n(D) Snowfield\n(E) Forest\n(F) Library\nOnly give the best option.\nBest option: ("}
{"doc_id": 307, "doc": {"video_name": "test_movie101_93.mp4", "duration": 641.0, "question": "What genre is this film?\n(A) Mystery\n(B) Thriller\n(C) Romance\n(D) Comedy\n(E) Action\n(F) Crime", "candidates": ["Mystery", "Thriller", "Romance", "Comedy", "Action", "Crime"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_52", "indices": ["450", "725", "1475", "1925", "2250", "2850", "3000", "3625", "4000", "4775", "5475", "5900", "6275", "6775", "7050", "7825", "8400", "8925", "9025", "9600", "10200", "10550", "11000", "11675", "12475", "12525", "13075", "13950", "14475", "14650", "15350", "15675"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Crime"]], "filtered_resps": ["(F) Crime"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "794485c8f569b8fd7e75d46280e4bed27e7b1432a92492b8371f452017bd12a7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What genre is this film?\n(A) Mystery\n(B) Thriller\n(C) Romance\n(D) Comedy\n(E) Action\n(F) Crime", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "C"}, "input": "What genre is this film?\n(A) Mystery\n(B) Thriller\n(C) Romance\n(D) Comedy\n(E) Action\n(F) Crime\nOnly give the best option.\nBest option: ("}
{"doc_id": 311, "doc": {"video_name": "test_cartoon_13.mp4", "duration": 379.33, "question": "What type of film is this?\n(A) Romance film\n(B) Horror film\n(C) Animated film\n(D) Action film\n(E) Documentary film\n(F) War film", "candidates": ["Romance film", "Horror film", "Animated film", "Action film", "Documentary film", "War film"], "answer": "C", "task_type": "topic_reasoning", "question_id": "TR_56", "indices": ["87", "638", "986", "1044", "1740", "1827", "2378", "2494", "2929", "3364", "3567", "3886", "4524", "4611", "5162", "5626", "5771", "6032", "6641", "6757", "7134", "7685", "8120", "8149", "8700", "8932", "9483", "9570", "10063", "10498", "10643", "11252"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Animated film"]], "filtered_resps": ["(C) Animated film"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b256be3d0965046fcd87480e8ee218ead233fe3ba8e0b539ce03f437db642c5", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Romance film\n(B) Horror film\n(C) Animated film\n(D) Action film\n(E) Documentary film\n(F) War film", "task_type": "topic_reasoning", "pred_answer": "C", "answer": "C"}, "input": "What type of film is this?\n(A) Romance film\n(B) Horror film\n(C) Animated film\n(D) Action film\n(E) Documentary film\n(F) War film\nOnly give the best option.\nBest option: ("}
{"doc_id": 315, "doc": {"video_name": "test_AWD-3.mp4", "duration": 450.13, "question": "How many types of animals appear in the video?\n(A) 3\n(B) 4\n(C) 1\n(D) 2\n(E) 5\n(F) 6", "candidates": ["3", "4", "1", "2", "5", "6"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_60", "indices": ["299", "483", "759", "1058", "1449", "1656", "2162", "2507", "2599", "3013", "3220", "3611", "3864", "4462", "4761", "4922", "5198", "5635", "5819", "6141", "6670", "6831", "7130", "7498", "7889", "8142", "8602", "8786", "9085", "9384", "9752", "10327"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B)"]], "filtered_resps": ["(B)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a1a691adc23d57fb7a55c235e4dc81bdf1825c8b246e93d98497e847fc89e7e3", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "How many types of animals appear in the video?\n(A) 3\n(B) 4\n(C) 1\n(D) 2\n(E) 5\n(F) 6", "task_type": "topic_reasoning", "pred_answer": "B", "answer": "E"}, "input": "How many types of animals appear in the video?\n(A) 3\n(B) 4\n(C) 1\n(D) 2\n(E) 5\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 319, "doc": {"video_name": "test_movie101_99.mp4", "duration": 464.0, "question": "What genre of movie is the clip in the video?\n(A) War\n(B) Horror\n(C) Animation\n(D) Romance\n(E) Documentary\n(F) Comedy", "candidates": ["War", "Horror", "Animation", "Romance", "Documentary", "Comedy"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_64", "indices": ["75", "675", "750", "1075", "1550", "1975", "2200", "2600", "3125", "3500", "3625", "4125", "4500", "4775", "5400", "5425", "6075", "6450", "6825", "7125", "7400", "7675", "8125", "8600", "9025", "9375", "9650", "9775", "10250", "10675", "11175", "11325"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Comedy"]], "filtered_resps": ["(F) Comedy"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bc280d812abc2f4acadb6202b304993126298f87cc504270bc35bfb1fd91a79f", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What genre of movie is the clip in the video?\n(A) War\n(B) Horror\n(C) Animation\n(D) Romance\n(E) Documentary\n(F) Comedy", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "D"}, "input": "What genre of movie is the clip in the video?\n(A) War\n(B) Horror\n(C) Animation\n(D) Romance\n(E) Documentary\n(F) Comedy\nOnly give the best option.\nBest option: ("}
{"doc_id": 323, "doc": {"video_name": "test_movie101_90.mp4", "duration": 378.0, "question": "In what setting does the entire video take place?\n(A) Beach\n(B) Library\n(C) Desert\n(D) City\n(E) Forest\n(F) Campus", "candidates": ["Beach", "Library", "Desert", "City", "Forest", "Campus"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_68", "indices": ["250", "275", "675", "900", "1300", "1500", "1875", "2200", "2425", "2875", "3100", "3450", "3700", "3950", "4150", "4700", "4975", "5200", "5575", "5625", "5950", "6225", "6750", "7050", "7125", "7625", "7700", "7950", "8425", "8750", "8900", "9150"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff7291c1dae3b5b994493a394f8843c558e0a65e6bb3b20bdf041a8857571f6a", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what setting does the entire video take place?\n(A) Beach\n(B) Library\n(C) Desert\n(D) City\n(E) Forest\n(F) Campus", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what setting does the entire video take place?\n(A) Beach\n(B) Library\n(C) Desert\n(D) City\n(E) Forest\n(F) Campus\nOnly give the best option.\nBest option: ("}
{"doc_id": 327, "doc": {"video_name": "test_BWB-5.mp4", "duration": 450.12, "question": "What type of film is this?\n(A) Animation\n(B) Horror\n(C) Comedy\n(D) Action\n(E) Romance\n(F) Documentary", "candidates": ["Animation", "Horror", "Comedy", "Action", "Romance", "Documentary"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_72", "indices": ["275", "575", "950", "1050", "1725", "1875", "2175", "2600", "2975", "3350", "3675", "3975", "4525", "4725", "5200", "5275", "5625", "5975", "6425", "6900", "7200", "7575", "7725", "8300", "8575", "8875", "9200", "9800", "9925", "10225", "10850", "11200"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Documentary"]], "filtered_resps": ["(F) Documentary"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "798b76e131e516892c0ef203db76a5c738f7c87da4711477972e9c450ba43919", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) Animation\n(B) Horror\n(C) Comedy\n(D) Action\n(E) Romance\n(F) Documentary", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What type of film is this?\n(A) Animation\n(B) Horror\n(C) Comedy\n(D) Action\n(E) Romance\n(F) Documentary\nOnly give the best option.\nBest option: ("}
{"doc_id": 331, "doc": {"video_name": "test_AWB-6.mp4", "duration": 450.0, "question": "In what environment does the scene in the video take place?\n(A) Ocean\n(B) City\n(C) Rainforest\n(D) Wasteland\n(E) Snow Mountain\n(F) Desert", "candidates": ["Ocean", "City", "Rainforest", "Wasteland", "Snow Mountain", "Desert"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_76", "indices": ["300", "600", "975", "1175", "1475", "1850", "2100", "2525", "3125", "3150", "3575", "4175", "4200", "4600", "5100", "5400", "5650", "6175", "6525", "6725", "7175", "7550", "7750", "8100", "8450", "8975", "9400", "9650", "10100", "10350", "10800", "10875"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) Wasteland"]], "filtered_resps": ["(D) Wasteland"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5f360b842e746f9f18c654d3dc5a2b40c60d8408a816fb4c5de4bf07834e583", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In what environment does the scene in the video take place?\n(A) Ocean\n(B) City\n(C) Rainforest\n(D) Wasteland\n(E) Snow Mountain\n(F) Desert", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "In what environment does the scene in the video take place?\n(A) Ocean\n(B) City\n(C) Rainforest\n(D) Wasteland\n(E) Snow Mountain\n(F) Desert\nOnly give the best option.\nBest option: ("}
{"doc_id": 335, "doc": {"video_name": "test_entv_18.mp4", "duration": 651.8112954069612, "question": "What type of film is this?\n(A) War\n(B) Documentary\n(C) Action\n(D) Comedy\n(E) Science Fiction\n(F) Horror", "candidates": ["War", "Documentary", "Action", "Comedy", "Science Fiction", "Horror"], "answer": "D", "task_type": "topic_reasoning", "question_id": "TR_80", "indices": ["145", "1131", "1508", "1914", "2552", "3596", "4205", "4437", "5104", "5974", "6409", "7163", "7308", "8294", "8758", "9715", "9889", "10527", "11426", "11571", "12470", "13079", "13572", "14326", "14616", "15370", "16356", "16559", "17255", "17980", "18821", "18995"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b26420364a37fc15ac5c77da0d04847a612f63e914bac0d2402f3ca2b483f18", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What type of film is this?\n(A) War\n(B) Documentary\n(C) Action\n(D) Comedy\n(E) Science Fiction\n(F) Horror", "task_type": "topic_reasoning", "pred_answer": "D", "answer": "D"}, "input": "What type of film is this?\n(A) War\n(B) Documentary\n(C) Action\n(D) Comedy\n(E) Science Fiction\n(F) Horror\nOnly give the best option.\nBest option: ("}
{"doc_id": 339, "doc": {"video_name": "test_movie101_91.mp4", "duration": 244.0, "question": "What weather condition is depicted in the video scene?\n(A) Cloudy day\n(B) Foggy day\n(C) Sunny day\n(D) Dust storm day\n(E) Rainy day\n(F) Snowy day", "candidates": ["Cloudy day", "Foggy day", "Sunny day", "Dust storm day", "Rainy day", "Snowy day"], "answer": "E", "task_type": "topic_reasoning", "question_id": "TR_84", "indices": ["150", "225", "525", "550", "900", "925", "1125", "1375", "1675", "1700", "1975", "2150", "2400", "2450", "2725", "3000", "3075", "3375", "3425", "3725", "3925", "4075", "4275", "4550", "4600", "4775", "5075", "5250", "5475", "5500", "5825", "5950"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbb9f924309554a34c74f8dcaa5f643e873f68b8885d66252afc6bcd5549ea14", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What weather condition is depicted in the video scene?\n(A) Cloudy day\n(B) Foggy day\n(C) Sunny day\n(D) Dust storm day\n(E) Rainy day\n(F) Snowy day", "task_type": "topic_reasoning", "pred_answer": "E", "answer": "E"}, "input": "What weather condition is depicted in the video scene?\n(A) Cloudy day\n(B) Foggy day\n(C) Sunny day\n(D) Dust storm day\n(E) Rainy day\n(F) Snowy day\nOnly give the best option.\nBest option: ("}
{"doc_id": 343, "doc": {"video_name": "test_cartoon_3.mp4", "duration": 572.577543878504, "question": "What genre of movie is depicted in the video clip?\n(A) Documentary\n(B) Comedy\n(C) Romance\n(D) Horror\n(E) War\n(F) Animation", "candidates": ["Documentary", "Comedy", "Romance", "Horror", "War", "Animation"], "answer": "F", "task_type": "topic_reasoning", "question_id": "TR_88", "indices": ["350", "650", "1100", "1450", "1775", "2300", "2900", "3500", "3575", "4000", "4525", "5300", "5400", "6050", "6625", "6825", "7550", "7575", "8175", "8675", "9075", "9400", "10200", "10400", "10750", "11300", "11650", "12450", "12525", "13250", "13550", "13925"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "287db6d04a38e27a31f0857fa0bd2d39a1c5b142163702ff998ca37d89ba7561", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What genre of movie is depicted in the video clip?\n(A) Documentary\n(B) Comedy\n(C) Romance\n(D) Horror\n(E) War\n(F) Animation", "task_type": "topic_reasoning", "pred_answer": "F", "answer": "F"}, "input": "What genre of movie is depicted in the video clip?\n(A) Documentary\n(B) Comedy\n(C) Romance\n(D) Horror\n(E) War\n(F) Animation\nOnly give the best option.\nBest option: ("}
{"doc_id": 347, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "What did I put in the dustbin?\n(A) Books\n(B) Leftover food\n(C) Trash cleaned from the floor.\n(D) Recyclable materials\n(E) Laundry\n(F) Toys", "candidates": ["Books", "Leftover food", "Trash cleaned from the floor.", "Recyclable materials", "Laundry", "Toys"], "answer": "C", "task_type": "ego", "question_id": "ER_1", "indices": ["180", "570", "1050", "1410", "1980", "2580", "2790", "3510", "3630", "4290", "4890", "5370", "5760", "5910", "6300", "6960", "7530", "7650", "8490", "8670", "9420", "9450", "9960", "10680", "11220", "11250", "11880", "12540", "12660", "13440", "13890", "14250"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Trash cleaned from the floor."]], "filtered_resps": ["(C) Trash cleaned from the floor."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1030584bb4b4319cce7f6944ab1d194047a72034f36e6a2bfe292b74796c976b", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What did I put in the dustbin?\n(A) Books\n(B) Leftover food\n(C) Trash cleaned from the floor.\n(D) Recyclable materials\n(E) Laundry\n(F) Toys", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What did I put in the dustbin?\n(A) Books\n(B) Leftover food\n(C) Trash cleaned from the floor.\n(D) Recyclable materials\n(E) Laundry\n(F) Toys\nOnly give the best option.\nBest option: ("}
{"doc_id": 351, "doc": {"video_name": "test_ego_7.mp4", "duration": 480.0000000000001, "question": "What word was printed in blue on the white box?\n(A) Cheez-It\n(B) Club Crackers\n(C) Triscuit.\n(D) Ritz\n(E) Wheat Thins\n(F) Saltines", "candidates": ["Cheez-It", "Club Crackers", "Triscuit.", "Ritz", "Wheat Thins", "Saltines"], "answer": "C", "task_type": "ego", "question_id": "ER_5", "indices": ["120", "630", "1230", "1440", "2070", "2430", "3030", "3210", "3810", "4170", "4680", "5340", "5430", "6090", "6690", "7170", "7260", "7710", "8400", "8790", "9240", "9450", "9930", "10530", "11040", "11430", "11850", "12510", "12690", "13380", "13530", "14220"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) Triscuit."]], "filtered_resps": ["(C) Triscuit."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0cb9666f10fc5e15825c96df62a3e64232c2232da0b253d18183d7e8c5354847", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "What word was printed in blue on the white box?\n(A) Cheez-It\n(B) Club Crackers\n(C) Triscuit.\n(D) Ritz\n(E) Wheat Thins\n(F) Saltines", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "What word was printed in blue on the white box?\n(A) Cheez-It\n(B) Club Crackers\n(C) Triscuit.\n(D) Ritz\n(E) Wheat Thins\n(F) Saltines\nOnly give the best option.\nBest option: ("}
{"doc_id": 355, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "Where was my phone before I picked it up\n(A) on the arm of the couch\n(B) in your car\n(C) on the coffee table\n(D) on the dining table\n(E) in your bedroom\n(F) on the kitchen counter", "candidates": ["on the arm of the couch", "in your car", "on the coffee table", "on the dining table", "in your bedroom", "on the kitchen counter"], "answer": "A", "task_type": "ego", "question_id": "ER_9", "indices": ["180", "810", "1020", "1680", "1920", "2670", "3000", "3480", "3900", "4080", "4590", "5040", "5640", "5850", "6360", "6990", "7530", "7920", "8160", "8760", "9360", "9750", "10170", "10470", "11010", "11430", "11790", "12150", "12690", "13170", "13740", "14160"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ff53d029b8e146d288b01d3f312191bc1e33eab1bdec1272aa670d95c80eef5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Where was my phone before I picked it up\n(A) on the arm of the couch\n(B) in your car\n(C) on the coffee table\n(D) on the dining table\n(E) in your bedroom\n(F) on the kitchen counter", "task_type": "ego", "pred_answer": "C", "answer": "A"}, "input": "Where was my phone before I picked it up\n(A) on the arm of the couch\n(B) in your car\n(C) on the coffee table\n(D) on the dining table\n(E) in your bedroom\n(F) on the kitchen counter\nOnly give the best option.\nBest option: ("}
{"doc_id": 359, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "Did I leave the curtains in the dining drawn open\n(A) partially\n(B) slightly open\n(C) yes\n(D) closed\n(E) no\n(F) half-drawn", "candidates": ["partially", "slightly open", "yes", "closed", "no", "half-drawn"], "answer": "C", "task_type": "ego", "question_id": "ER_13", "indices": ["180", "870", "900", "1710", "1980", "2340", "2880", "3390", "3960", "4110", "4620", "5130", "5760", "6120", "6660", "7110", "7410", "7650", "8430", "8640", "9090", "9660", "9990", "10620", "11100", "11460", "11730", "12180", "12930", "13290", "13890", "14220"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) yes"]], "filtered_resps": ["(C) yes"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b77562afb5b23adea12aed216c0de4bff2c6d59bfad0294a2ab479db5afa6ff7", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Did I leave the curtains in the dining drawn open\n(A) partially\n(B) slightly open\n(C) yes\n(D) closed\n(E) no\n(F) half-drawn", "task_type": "ego", "pred_answer": "C", "answer": "C"}, "input": "Did I leave the curtains in the dining drawn open\n(A) partially\n(B) slightly open\n(C) yes\n(D) closed\n(E) no\n(F) half-drawn\nOnly give the best option.\nBest option: ("}
{"doc_id": 363, "doc": {"video_name": "test_ego_6.mp4", "duration": 480.03333333333336, "question": "Did I leave the television on\n(A) no\n(B) maybe\n(C) I don't know\n(D) yes\n(E) probably not\n(F) unlikely", "candidates": ["no", "maybe", "I don't know", "yes", "probably not", "unlikely"], "answer": "D", "task_type": "ego", "question_id": "ER_17", "indices": ["390", "780", "1110", "1530", "2190", "2460", "2880", "3540", "3600", "4110", "4800", "5100", "5490", "6090", "6330", "6900", "7320", "7740", "8490", "8970", "9030", "9600", "10050", "10740", "11190", "11250", "12120", "12240", "12600", "13170", "13770", "14100"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(D) yes"]], "filtered_resps": ["(D) yes"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64218bf5453980da7830a1d9a06d80236b93e3df2c204fe8320cd0ea2372f583", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Did I leave the television on\n(A) no\n(B) maybe\n(C) I don't know\n(D) yes\n(E) probably not\n(F) unlikely", "task_type": "ego", "pred_answer": "D", "answer": "D"}, "input": "Did I leave the television on\n(A) no\n(B) maybe\n(C) I don't know\n(D) yes\n(E) probably not\n(F) unlikely\nOnly give the best option.\nBest option: ("}
{"doc_id": 367, "doc": {"video_name": "test_ego_3.mp4", "duration": 480.0, "question": "Where was the fork?\n(A) in the pantry\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) on the dining table\n(E) in the sink\n(F) in utensil organizer", "candidates": ["in the pantry", "on the kitchen counter", "in the dishwasher", "on the dining table", "in the sink", "in utensil organizer"], "answer": "F", "task_type": "ego", "question_id": "ER_21", "indices": ["120", "570", "960", "1440", "1860", "2460", "3120", "3300", "3660", "4380", "4740", "5160", "5730", "6240", "6330", "7080", "7620", "7950", "8520", "8940", "9000", "9630", "9930", "10620", "10800", "11250", "11910", "12240", "12990", "13080", "13860", "14130"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d9372e214e16538639d024f987bf67e9c0e83135274f2df2fd2e9890d4ccb59a", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where was the fork?\n(A) in the pantry\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) on the dining table\n(E) in the sink\n(F) in utensil organizer", "task_type": "ego", "pred_answer": "B", "answer": "F"}, "input": "Where was the fork?\n(A) in the pantry\n(B) on the kitchen counter\n(C) in the dishwasher\n(D) on the dining table\n(E) in the sink\n(F) in utensil organizer\nOnly give the best option.\nBest option: ("}
{"doc_id": 371, "doc": {"video_name": "test_ego_10.mp4", "duration": 480.0333333333333, "question": "What colour was the cloth I placed on the table?\n(A) light gray\n(B) white\n(C) ivory\n(D) cream\n(E) off-white\n(F) beige", "candidates": ["light gray", "white", "ivory", "cream", "off-white", "beige"], "answer": "B", "task_type": "ego", "question_id": "ER_25", "indices": ["420", "480", "930", "1620", "1920", "2340", "2820", "3480", "3660", "4290", "4740", "5190", "5640", "6150", "6330", "7050", "7350", "7680", "8280", "8670", "9360", "9600", "10080", "10410", "11160", "11610", "11850", "12390", "12600", "13470", "13500", "14100"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24f4ef5d9ae244950e9637ce152598e41b2b8a5ab1073a71e791498b6d327362", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What colour was the cloth I placed on the table?\n(A) light gray\n(B) white\n(C) ivory\n(D) cream\n(E) off-white\n(F) beige", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "What colour was the cloth I placed on the table?\n(A) light gray\n(B) white\n(C) ivory\n(D) cream\n(E) off-white\n(F) beige\nOnly give the best option.\nBest option: ("}
{"doc_id": 375, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "What colour was the internet router\n(A) white\n(B) red\n(C) grey\n(D) green\n(E) blue\n(F) black", "candidates": ["white", "red", "grey", "green", "blue", "black"], "answer": "A", "task_type": "ego", "question_id": "ER_29", "indices": ["30", "750", "1080", "1710", "1860", "2460", "2760", "3540", "3870", "4410", "4500", "5130", "5790", "6120", "6660", "7110", "7560", "7680", "8400", "8580", "9060", "9660", "10140", "10590", "11100", "11580", "12120", "12420", "12990", "13080", "13500", "14010"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9da22278937938566e31ac51afeae3bd80207b24fd976d83e9731e7b067ab03", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "What colour was the internet router\n(A) white\n(B) red\n(C) grey\n(D) green\n(E) blue\n(F) black", "task_type": "ego", "pred_answer": "F", "answer": "A"}, "input": "What colour was the internet router\n(A) white\n(B) red\n(C) grey\n(D) green\n(E) blue\n(F) black\nOnly give the best option.\nBest option: ("}
{"doc_id": 379, "doc": {"video_name": "test_ego_9.mp4", "duration": 480.0666666666667, "question": "Who did I interact with when I last operated my tablet on the kitchen divider\n(A) the woman in white\n(B) the woman in green\n(C) the teenager in black\n(D) the child in yellow\n(E) the man in blue\n(F) the elderly lady in red", "candidates": ["the woman in white", "the woman in green", "the teenager in black", "the child in yellow", "the man in blue", "the elderly lady in red"], "answer": "B", "task_type": "ego", "question_id": "ER_33", "indices": ["30", "780", "1320", "1380", "1920", "2520", "3000", "3450", "3990", "4440", "4800", "5160", "5520", "5880", "6660", "6990", "7590", "8070", "8100", "8820", "9120", "9570", "10290", "10350", "11160", "11580", "11820", "12210", "12690", "13260", "13830", "14340"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) the woman in green"]], "filtered_resps": ["(B) the woman in green"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "085746bae987dd8eb3f86d3ffca1dc2bb407bba42a9a4a86b9db3e7340ff821b", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Who did I interact with when I last operated my tablet on the kitchen divider\n(A) the woman in white\n(B) the woman in green\n(C) the teenager in black\n(D) the child in yellow\n(E) the man in blue\n(F) the elderly lady in red", "task_type": "ego", "pred_answer": "B", "answer": "B"}, "input": "Who did I interact with when I last operated my tablet on the kitchen divider\n(A) the woman in white\n(B) the woman in green\n(C) the teenager in black\n(D) the child in yellow\n(E) the man in blue\n(F) the elderly lady in red\nOnly give the best option.\nBest option: ("}
{"doc_id": 383, "doc": {"video_name": "test_ego_10.mp4", "duration": 480.0333333333333, "question": "Did I leave the door open?\n(A) No, I didn't\n(B) I'm not sure\n(C) I think so\n(D) Yes, you did\n(E) Possibly\n(F) Maybe", "candidates": ["No, I didn't", "I'm not sure", "I think so", "Yes, you did", "Possibly", "Maybe"], "answer": "A", "task_type": "ego", "question_id": "ER_37", "indices": ["390", "510", "1140", "1740", "1890", "2580", "2730", "3480", "3930", "4050", "4920", "5310", "5400", "5910", "6600", "6810", "7440", "8070", "8460", "8820", "9390", "9600", "10170", "10530", "10890", "11610", "12060", "12180", "12720", "13200", "13620", "14070"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "742b8a777d6b22ee41b182f258439397aaae7f791aaa6043ad2e85813f17267b", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "Did I leave the door open?\n(A) No, I didn't\n(B) I'm not sure\n(C) I think so\n(D) Yes, you did\n(E) Possibly\n(F) Maybe", "task_type": "ego", "pred_answer": "D", "answer": "A"}, "input": "Did I leave the door open?\n(A) No, I didn't\n(B) I'm not sure\n(C) I think so\n(D) Yes, you did\n(E) Possibly\n(F) Maybe\nOnly give the best option.\nBest option: ("}
{"doc_id": 387, "doc": {"video_name": "test_ego_4.mp4", "duration": 480.0333333333333, "question": "How many glass cups were in front of the white plate rack on the countertop\n(A) 4\n(B) 1\n(C) 6\n(D) 2\n(E) 3\n(F) 5", "candidates": ["4", "1", "6", "2", "3", "5"], "answer": "D", "task_type": "ego", "question_id": "ER_41", "indices": ["90", "450", "1050", "1380", "1980", "2370", "3120", "3180", "3600", "4260", "4860", "5370", "5400", "5970", "6360", "6990", "7530", "7650", "8430", "8670", "9300", "9810", "9960", "10440", "10830", "11400", "11970", "12360", "12870", "13470", "13500", "14190"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "707dd3da6d287c5baa62bbd0073e8a0460f00bc6870ed1196cdbcec8f8fc662d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "How many glass cups were in front of the white plate rack on the countertop\n(A) 4\n(B) 1\n(C) 6\n(D) 2\n(E) 3\n(F) 5", "task_type": "ego", "pred_answer": "B", "answer": "D"}, "input": "How many glass cups were in front of the white plate rack on the countertop\n(A) 4\n(B) 1\n(C) 6\n(D) 2\n(E) 3\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 391, "doc": {"video_name": "test_ego_3.mp4", "duration": 480.0, "question": "Where was the cheese pack?\n(A) in the oven\n(B) in the cupboard\n(C) in the microwave\n(D) on the counter\n(E) in the fridge\n(F) in the pantry", "candidates": ["in the oven", "in the cupboard", "in the microwave", "on the counter", "in the fridge", "in the pantry"], "answer": "E", "task_type": "ego", "question_id": "ER_45", "indices": ["360", "690", "1290", "1440", "2130", "2520", "3120", "3300", "3600", "4350", "4830", "5190", "5640", "6270", "6690", "6750", "7440", "7950", "8100", "8970", "9210", "9540", "10020", "10590", "10860", "11460", "11730", "12180", "12990", "13170", "13740", "13980"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65c8877e673270d2e7c1be3de966a2c3dfdd11c443e4e5de224b7ff7b7d3cd65", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Where was the cheese pack?\n(A) in the oven\n(B) in the cupboard\n(C) in the microwave\n(D) on the counter\n(E) in the fridge\n(F) in the pantry", "task_type": "ego", "pred_answer": "D", "answer": "E"}, "input": "Where was the cheese pack?\n(A) in the oven\n(B) in the cupboard\n(C) in the microwave\n(D) on the counter\n(E) in the fridge\n(F) in the pantry\nOnly give the best option.\nBest option: ("}
{"doc_id": 395, "doc": {"video_name": "test_ego_10.mp4", "duration": 480.0333333333333, "question": "Where was the dog before I left the house?\n(A) In the living room\n(B) In the backyard\n(C) In the bedroom\n(D) In the bathroom\n(E) In the garage\n(F) In the kitchen", "candidates": ["In the living room", "In the backyard", "In the bedroom", "In the bathroom", "In the garage", "In the kitchen"], "answer": "F", "task_type": "ego", "question_id": "ER_49", "indices": ["270", "750", "1140", "1770", "1860", "2310", "3060", "3420", "3930", "4290", "4530", "5040", "5400", "5910", "6600", "6810", "7560", "8070", "8160", "8790", "9390", "9720", "10320", "10500", "10890", "11610", "11850", "12360", "12720", "13410", "13530", "14100"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "efc1e2b422a36390c459d2d9e56ddecf92e7f2f2d37b226f9e45d2296b2bd2f1", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Where was the dog before I left the house?\n(A) In the living room\n(B) In the backyard\n(C) In the bedroom\n(D) In the bathroom\n(E) In the garage\n(F) In the kitchen", "task_type": "ego", "pred_answer": "F", "answer": "F"}, "input": "Where was the dog before I left the house?\n(A) In the living room\n(B) In the backyard\n(C) In the bedroom\n(D) In the bathroom\n(E) In the garage\n(F) In the kitchen\nOnly give the best option.\nBest option: ("}
{"doc_id": 399, "doc": {"video_name": "test_tutorial_20.mp4", "duration": 811.57, "question": "Which action is performed after punching a hole in the bottle cap\n(A) Sealing\n(B) Mixing the contents\n(C) Adding a straw\n(D) Pouring out the liquid\n(E) Inserting a pen through the hole\n(F) Injecting a beverage", "candidates": ["Sealing", "Mixing the contents", "Adding a straw", "Pouring out the liquid", "Inserting a pen through the hole", "Injecting a beverage"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_0", "indices": ["510", "810", "1770", "2790", "3090", "3900", "4530", "5520", "6420", "7320", "8010", "8910", "9600", "10020", "10920", "11700", "12510", "12990", "14190", "14460", "15180", "16020", "17130", "18000", "18960", "18990", "20130", "20670", "21900", "22230", "23400", "23790"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Inserting a pen through the hole"]], "filtered_resps": ["(E) Inserting a pen through the hole"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38025c7d759579ad7c07c217822bd689c8e974d148030e753c217ecc810f653f", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "Which action is performed after punching a hole in the bottle cap\n(A) Sealing\n(B) Mixing the contents\n(C) Adding a straw\n(D) Pouring out the liquid\n(E) Inserting a pen through the hole\n(F) Injecting a beverage", "task_type": "tutorialQA", "pred_answer": "E", "answer": "E"}, "input": "Which action is performed after punching a hole in the bottle cap\n(A) Sealing\n(B) Mixing the contents\n(C) Adding a straw\n(D) Pouring out the liquid\n(E) Inserting a pen through the hole\n(F) Injecting a beverage\nOnly give the best option.\nBest option: ("}
{"doc_id": 403, "doc": {"video_name": "test_tutorial_2.mp4", "duration": 459.52, "question": "Which action is performed after the man has prepared the raw fish?\n(A) Adjusting the heat of the stove\n(B) Preparing the sauce\n(C) Cleaning the cutting board\n(D) Opening the refrigerator to get other ingredients\n(E) Washing hands\n(F) Sharpening the knife", "candidates": ["Adjusting the heat of the stove", "Preparing the sauce", "Cleaning the cutting board", "Opening the refrigerator to get other ingredients", "Washing hands", "Sharpening the knife"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_4", "indices": ["300", "575", "925", "1350", "1550", "1850", "2450", "2650", "3150", "3325", "3700", "4000", "4275", "4875", "5325", "5625", "6050", "6250", "6500", "6950", "7200", "7575", "8000", "8500", "8875", "9150", "9500", "10000", "10150", "10375", "10825", "11100"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) Washing hands"]], "filtered_resps": ["(E) Washing hands"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "539a6f4346ac9801585e60c77f521e414a16b88fbca19240a15ae7d817a88a09", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which action is performed after the man has prepared the raw fish?\n(A) Adjusting the heat of the stove\n(B) Preparing the sauce\n(C) Cleaning the cutting board\n(D) Opening the refrigerator to get other ingredients\n(E) Washing hands\n(F) Sharpening the knife", "task_type": "tutorialQA", "pred_answer": "E", "answer": "F"}, "input": "Which action is performed after the man has prepared the raw fish?\n(A) Adjusting the heat of the stove\n(B) Preparing the sauce\n(C) Cleaning the cutting board\n(D) Opening the refrigerator to get other ingredients\n(E) Washing hands\n(F) Sharpening the knife\nOnly give the best option.\nBest option: ("}
{"doc_id": 407, "doc": {"video_name": "test_tutorial_23.mp4", "duration": 1471.75, "question": "What is the sequence of images processed in the video?\n(A) A photo of a person in a white skirt, a photo of a person in a long dress with a floral pattern, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(B) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of a person in a long dress with a floral pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(C) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person in a long dress with a floral pattern, a photo of a person holding a cat\n(D) A photo of a person in a long dress with a floral pattern, a photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(E) A photo of a person in a white skirt, a landscape photo, a photo of a person in a long dress with a floral pattern, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(F) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of a person in a long dress with a floral pattern, a photo of four people sitting on a bench, a photo of a person holding a cat", "candidates": ["A photo of a person in a white skirt, a photo of a person in a long dress with a floral pattern, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat", "A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of a person in a long dress with a floral pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat", "A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person in a long dress with a floral pattern, a photo of a person holding a cat", "A photo of a person in a long dress with a floral pattern, a photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat", "A photo of a person in a white skirt, a landscape photo, a photo of a person in a long dress with a floral pattern, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat", "A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of a person in a long dress with a floral pattern, a photo of four people sitting on a bench, a photo of a person holding a cat"], "answer": "D", "task_type": "tutorialQA", "question_id": "TQA_8", "indices": ["540", "1530", "3480", "4980", "6330", "7620", "8370", "9720", "12120", "12540", "14790", "15510", "17880", "18510", "19470", "20910", "23250", "24660", "25320", "27000", "28530", "30300", "31650", "31950", "34170", "34950", "36330", "38070", "38940", "40170", "41460", "43980"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a855aaeeacdabf21742c0c221b32f880b93dd921a1c509e97baaf56260cd977", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "What is the sequence of images processed in the video?\n(A) A photo of a person in a white skirt, a photo of a person in a long dress with a floral pattern, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(B) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of a person in a long dress with a floral pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(C) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person in a long dress with a floral pattern, a photo of a person holding a cat\n(D) A photo of a person in a long dress with a floral pattern, a photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(E) A photo of a person in a white skirt, a landscape photo, a photo of a person in a long dress with a floral pattern, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(F) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of a person in a long dress with a floral pattern, a photo of four people sitting on a bench, a photo of a person holding a cat", "task_type": "tutorialQA", "pred_answer": "E", "answer": "D"}, "input": "What is the sequence of images processed in the video?\n(A) A photo of a person in a white skirt, a photo of a person in a long dress with a floral pattern, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(B) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of a person in a long dress with a floral pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(C) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person in a long dress with a floral pattern, a photo of a person holding a cat\n(D) A photo of a person in a long dress with a floral pattern, a photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(E) A photo of a person in a white skirt, a landscape photo, a photo of a person in a long dress with a floral pattern, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of four people sitting on a bench, a photo of a person holding a cat\n(F) A photo of a person in a white skirt, a landscape photo, a cartoon photo with a panda pattern, a photo of two people in a restaurant, a photo of a person in a long dress with a floral pattern, a photo of four people sitting on a bench, a photo of a person holding a cat\nOnly give the best option.\nBest option: ("}
{"doc_id": 411, "doc": {"video_name": "test_tutorial_28.mp4", "duration": 3553.39, "question": "What is the order of tools used in the video?\n(A) Marker, knife, tape, screwdriver\n(B) Knife, marker, tape, screwdriver\n(C) Tape, marker, screwdriver, knife\n(D) Tape, knife, marker, screwdriver\n(E) Knife, tape, marker, screwdriver\n(F) Tape, marker, knife, screwdriver", "candidates": ["Marker, knife, tape, screwdriver", "Knife, marker, tape, screwdriver", "Tape, marker, screwdriver, knife", "Tape, knife, marker, screwdriver", "Knife, tape, marker, screwdriver", "Tape, marker, knife, screwdriver"], "answer": "E", "task_type": "tutorialQA", "question_id": "TQA_12", "indices": ["3103", "5829", "9367", "11745", "14413", "18908", "20068", "23693", "28014", "30624", "36134", "36946", "39817", "45849", "47415", "49967", "56173", "56376", "61045", "65337", "66468", "69629", "74704", "79199", "81432", "82969", "87232", "91553", "94134", "99267", "100369", "103704"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Knife, marker, tape, screwdriver"]], "filtered_resps": ["(B) Knife, marker, tape, screwdriver"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8e82ab53e013842e7641d0cbb8031b81da4267decda388855d5df8b6fbf39545", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "What is the order of tools used in the video?\n(A) Marker, knife, tape, screwdriver\n(B) Knife, marker, tape, screwdriver\n(C) Tape, marker, screwdriver, knife\n(D) Tape, knife, marker, screwdriver\n(E) Knife, tape, marker, screwdriver\n(F) Tape, marker, knife, screwdriver", "task_type": "tutorialQA", "pred_answer": "B", "answer": "E"}, "input": "What is the order of tools used in the video?\n(A) Marker, knife, tape, screwdriver\n(B) Knife, marker, tape, screwdriver\n(C) Tape, marker, screwdriver, knife\n(D) Tape, knife, marker, screwdriver\n(E) Knife, tape, marker, screwdriver\n(F) Tape, marker, knife, screwdriver\nOnly give the best option.\nBest option: ("}
{"doc_id": 415, "doc": {"video_name": "test_tutorial_26.mp4", "duration": 1281.49, "question": "What is the next step after the first plot is completed in the video?\n(A) Documenting\n(B) Optimizing the chart\n(C) Sharing and presenting\n(D) Interpreting results\n(E) Validation and testing\n(F) Model fitting", "candidates": ["Documenting", "Optimizing the chart", "Sharing and presenting", "Interpreting results", "Validation and testing", "Model fitting"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_16", "indices": ["480", "2250", "3210", "3660", "5880", "6060", "7470", "8910", "10050", "11070", "12510", "13650", "15510", "15840", "17190", "18510", "20220", "20790", "21930", "23850", "24030", "25590", "27150", "28110", "29790", "30450", "31650", "32700", "34110", "34830", "36210", "37590"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) Optimizing the chart"]], "filtered_resps": ["(B) Optimizing the chart"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad944e11f917d0fc625c3a2df5effba7cbf510d955c291fb0766514c54569980", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the next step after the first plot is completed in the video?\n(A) Documenting\n(B) Optimizing the chart\n(C) Sharing and presenting\n(D) Interpreting results\n(E) Validation and testing\n(F) Model fitting", "task_type": "tutorialQA", "pred_answer": "B", "answer": "B"}, "input": "What is the next step after the first plot is completed in the video?\n(A) Documenting\n(B) Optimizing the chart\n(C) Sharing and presenting\n(D) Interpreting results\n(E) Validation and testing\n(F) Model fitting\nOnly give the best option.\nBest option: ("}
{"doc_id": 419, "doc": {"video_name": "test_tutorial_21.mp4", "duration": 1765.25, "question": "What is the sequence of types of images edited in the video?\n(A) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a red long dress, a photo of a person in a purple long dress, a dimly lit profile photo\n(B) A photo of a person in a red long dress, a landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(C) Landscape photo, a photo of a person in a red long dress, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(D) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person in a red long dress, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(E) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a red long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(F) Landscape photo, a photo of a person in denim jeans, a photo of a person in a red long dress, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "candidates": ["Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a red long dress, a photo of a person in a purple long dress, a dimly lit profile photo", "A photo of a person in a red long dress, a landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "Landscape photo, a photo of a person in a red long dress, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person in a red long dress, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a red long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "Landscape photo, a photo of a person in denim jeans, a photo of a person in a red long dress, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_20", "indices": ["810", "2430", "3330", "6060", "7140", "8610", "10260", "12750", "14610", "15540", "17460", "19500", "20100", "22950", "24540", "26370", "26490", "28590", "31140", "32130", "34080", "35280", "36600", "38010", "40500", "42810", "43140", "44850", "47160", "47970", "51000", "52740"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["C"]], "filtered_resps": ["C"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd2afa5b56e4a5119b5f8b721d9660dc1205811c8cd99bf955e616aa01053ed4", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "What is the sequence of types of images edited in the video?\n(A) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a red long dress, a photo of a person in a purple long dress, a dimly lit profile photo\n(B) A photo of a person in a red long dress, a landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(C) Landscape photo, a photo of a person in a red long dress, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(D) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person in a red long dress, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(E) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a red long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(F) Landscape photo, a photo of a person in denim jeans, a photo of a person in a red long dress, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo", "task_type": "tutorialQA", "pred_answer": "C", "answer": "B"}, "input": "What is the sequence of types of images edited in the video?\n(A) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a red long dress, a photo of a person in a purple long dress, a dimly lit profile photo\n(B) A photo of a person in a red long dress, a landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(C) Landscape photo, a photo of a person in a red long dress, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(D) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person in a red long dress, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(E) Landscape photo, a photo of a person in denim jeans, a photo of a person in a white long dress, a photo of a person in a red long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\n(F) Landscape photo, a photo of a person in denim jeans, a photo of a person in a red long dress, a photo of a person in a white long dress, a photo of a person in a white crop top, a photo of a person with bright red lips, a photo of a person in a purple long dress, a dimly lit profile photo\nOnly give the best option.\nBest option: ("}
{"doc_id": 423, "doc": {"video_name": "test_tutorial_3.mp4", "duration": 1827.44, "question": "Which action does the man perform after putting the ingredients into a glass jar?\n(A) Takes the ingredients out of the glass jar\n(B) Takes a photo of the ingredients\n(C) Puts the glass jar into the microwave to heat up\n(D) Adds some seasoning to the glass jar\n(E) Puts the glass jar into the fridge for cooling\n(F) Blends the food with a blender", "candidates": ["Takes the ingredients out of the glass jar", "Takes a photo of the ingredients", "Puts the glass jar into the microwave to heat up", "Adds some seasoning to the glass jar", "Puts the glass jar into the fridge for cooling", "Blends the food with a blender"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_24", "indices": ["1250", "2200", "2850", "5575", "6475", "7525", "9625", "11100", "12450", "13450", "14425", "16225", "18175", "19325", "21100", "21800", "22925", "24675", "26950", "28150", "29675", "30950", "32500", "33875", "34550", "36125", "37975", "39800", "40650", "41400", "43200", "44350"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03027fc95947f7fba32babdbfe1bbd7f21744c638b9dccc83044f8f60dd70e54", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "Which action does the man perform after putting the ingredients into a glass jar?\n(A) Takes the ingredients out of the glass jar\n(B) Takes a photo of the ingredients\n(C) Puts the glass jar into the microwave to heat up\n(D) Adds some seasoning to the glass jar\n(E) Puts the glass jar into the fridge for cooling\n(F) Blends the food with a blender", "task_type": "tutorialQA", "pred_answer": "D", "answer": "F"}, "input": "Which action does the man perform after putting the ingredients into a glass jar?\n(A) Takes the ingredients out of the glass jar\n(B) Takes a photo of the ingredients\n(C) Puts the glass jar into the microwave to heat up\n(D) Adds some seasoning to the glass jar\n(E) Puts the glass jar into the fridge for cooling\n(F) Blends the food with a blender\nOnly give the best option.\nBest option: ("}
{"doc_id": 427, "doc": {"video_name": "test_tutorial_13.mp4", "duration": 676.68, "question": "The sequence of handling the color of the strips in the process in the video\n(A) Pink, White, Yellow, White, Dark Green, Purple, Light Green\n(B) White, Yellow, White, Dark Green, Purple, Pink, Light Green\n(C) White, Yellow, Pink, White, Dark Green, Purple, Light Green\n(D) White, Pink, Yellow, White, Dark Green, Purple, Light Green\n(E) White, Yellow, White, Pink, Dark Green, Purple, Light Green\n(F) White, Yellow, White, Dark Green, Pink, Purple, Light Green", "candidates": ["Pink, White, Yellow, White, Dark Green, Purple, Light Green", "White, Yellow, White, Dark Green, Purple, Pink, Light Green", "White, Yellow, Pink, White, Dark Green, Purple, Light Green", "White, Pink, Yellow, White, Dark Green, Purple, Light Green", "White, Yellow, White, Pink, Dark Green, Purple, Light Green", "White, Yellow, White, Dark Green, Pink, Purple, Light Green"], "answer": "A", "task_type": "tutorialQA", "question_id": "TQA_28", "indices": ["408", "504", "1056", "1968", "2376", "2544", "3432", "3888", "4416", "4608", "5256", "5952", "6336", "6576", "7104", "7848", "8376", "8736", "9408", "9912", "10344", "10824", "11184", "11784", "12288", "12768", "13368", "13704", "14256", "15096", "15408", "16104"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bd83c1a6f43611fd8ce00a1c0431e2774ea2fb3feda9df89312244aecaf9fe88", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "The sequence of handling the color of the strips in the process in the video\n(A) Pink, White, Yellow, White, Dark Green, Purple, Light Green\n(B) White, Yellow, White, Dark Green, Purple, Pink, Light Green\n(C) White, Yellow, Pink, White, Dark Green, Purple, Light Green\n(D) White, Pink, Yellow, White, Dark Green, Purple, Light Green\n(E) White, Yellow, White, Pink, Dark Green, Purple, Light Green\n(F) White, Yellow, White, Dark Green, Pink, Purple, Light Green", "task_type": "tutorialQA", "pred_answer": "E", "answer": "A"}, "input": "The sequence of handling the color of the strips in the process in the video\n(A) Pink, White, Yellow, White, Dark Green, Purple, Light Green\n(B) White, Yellow, White, Dark Green, Purple, Pink, Light Green\n(C) White, Yellow, Pink, White, Dark Green, Purple, Light Green\n(D) White, Pink, Yellow, White, Dark Green, Purple, Light Green\n(E) White, Yellow, White, Pink, Dark Green, Purple, Light Green\n(F) White, Yellow, White, Dark Green, Pink, Purple, Light Green\nOnly give the best option.\nBest option: ("}
{"doc_id": 431, "doc": {"video_name": "test_tutorial_20.mp4", "duration": 811.57, "question": "Sequence of tools used in the video\n(A) Hole puncher, knife, hole puncher, sandpaper, glue, needle, screwdriver\n(B) Sandpaper, hole puncher, knife, hole puncher, glue, needle, screwdriver\n(C) Hole puncher, knife, hole puncher, glue, sandpaper, needle, screwdriver\n(D) Hole puncher, knife, sandpaper, hole puncher, glue, needle, screwdriver\n(E) Hole puncher, knife, hole puncher, glue, needle, sandpaper, screwdriver\n(F) Hole puncher, sandpaper, knife, hole puncher, glue, needle, screwdriver", "candidates": ["Hole puncher, knife, hole puncher, sandpaper, glue, needle, screwdriver", "Sandpaper, hole puncher, knife, hole puncher, glue, needle, screwdriver", "Hole puncher, knife, hole puncher, glue, sandpaper, needle, screwdriver", "Hole puncher, knife, sandpaper, hole puncher, glue, needle, screwdriver", "Hole puncher, knife, hole puncher, glue, needle, sandpaper, screwdriver", "Hole puncher, sandpaper, knife, hole puncher, glue, needle, screwdriver"], "answer": "B", "task_type": "tutorialQA", "question_id": "TQA_32", "indices": ["540", "930", "1830", "2280", "3570", "3900", "4740", "5370", "6690", "7320", "7920", "8310", "9660", "9870", "10980", "11490", "12510", "13590", "13740", "14790", "15600", "16380", "16860", "17910", "18270", "19560", "20310", "21210", "21720", "22320", "22830", "24120"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "88d3d2017d1487f939afe58ced6a0455ce18fcc364ff0afc019e8ad5796c8781", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Sequence of tools used in the video\n(A) Hole puncher, knife, hole puncher, sandpaper, glue, needle, screwdriver\n(B) Sandpaper, hole puncher, knife, hole puncher, glue, needle, screwdriver\n(C) Hole puncher, knife, hole puncher, glue, sandpaper, needle, screwdriver\n(D) Hole puncher, knife, sandpaper, hole puncher, glue, needle, screwdriver\n(E) Hole puncher, knife, hole puncher, glue, needle, sandpaper, screwdriver\n(F) Hole puncher, sandpaper, knife, hole puncher, glue, needle, screwdriver", "task_type": "tutorialQA", "pred_answer": "D", "answer": "B"}, "input": "Sequence of tools used in the video\n(A) Hole puncher, knife, hole puncher, sandpaper, glue, needle, screwdriver\n(B) Sandpaper, hole puncher, knife, hole puncher, glue, needle, screwdriver\n(C) Hole puncher, knife, hole puncher, glue, sandpaper, needle, screwdriver\n(D) Hole puncher, knife, sandpaper, hole puncher, glue, needle, screwdriver\n(E) Hole puncher, knife, hole puncher, glue, needle, sandpaper, screwdriver\n(F) Hole puncher, sandpaper, knife, hole puncher, glue, needle, screwdriver\nOnly give the best option.\nBest option: ("}
{"doc_id": 435, "doc": {"video_name": "test_tutorial_24.mp4", "duration": 242.58, "question": "What operation was performed after importing data in the video?\n(A) Data transformation\n(B) Data filtering\n(C) Data aggregation\n(D) Data association\n(E) Data modeling\n(F) Data visualization", "candidates": ["Data transformation", "Data filtering", "Data aggregation", "Data association", "Data modeling", "Data visualization"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_36", "indices": ["30", "390", "450", "660", "1080", "1110", "1440", "1680", "1980", "2010", "2430", "2490", "2850", "3120", "3180", "3600", "3750", "3900", "4080", "4440", "4650", "4890", "5160", "5190", "5520", "5730", "6060", "6270", "6330", "6600", "6840", "7170"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(F) Data visualization"]], "filtered_resps": ["(F) Data visualization"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "10b1bae428f43b6f0738de6c41b96de8089e689fa717801326c73ca9ddc64104", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "What operation was performed after importing data in the video?\n(A) Data transformation\n(B) Data filtering\n(C) Data aggregation\n(D) Data association\n(E) Data modeling\n(F) Data visualization", "task_type": "tutorialQA", "pred_answer": "F", "answer": "F"}, "input": "What operation was performed after importing data in the video?\n(A) Data transformation\n(B) Data filtering\n(C) Data aggregation\n(D) Data association\n(E) Data modeling\n(F) Data visualization\nOnly give the best option.\nBest option: ("}
{"doc_id": 439, "doc": {"video_name": "test_tutorial_11.mp4", "duration": 191.87, "question": "The sequence of origami actions in the video\n(A) Cutting the strip of paper, dipping in glue, bending the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(B) Bending the strip of paper, dipping in glue, cutting the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(C) Bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper, cutting the strip of paper\n(D) Cutting the strip of paper, bending the strip of paper, dipping in glue, adjusting the position of the strip of paper, fixing the strip of paper\n(E) Bending the strip of paper, cutting the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper\n(F) Cutting the strip of paper, bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper", "candidates": ["Cutting the strip of paper, dipping in glue, bending the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper", "Bending the strip of paper, dipping in glue, cutting the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper", "Bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper, cutting the strip of paper", "Cutting the strip of paper, bending the strip of paper, dipping in glue, adjusting the position of the strip of paper, fixing the strip of paper", "Bending the strip of paper, cutting the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper", "Cutting the strip of paper, bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper"], "answer": "F", "task_type": "tutorialQA", "question_id": "TQA_40", "indices": ["90", "180", "420", "510", "840", "870", "1050", "1350", "1560", "1680", "1890", "2040", "2160", "2430", "2640", "2820", "2940", "3060", "3330", "3480", "3570", "3840", "3990", "4200", "4290", "4620", "4770", "4830", "5070", "5220", "5370", "5580"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8bff3d7094610e3864b57e280f393036873542bde90a3e50908964468b1bf383", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "The sequence of origami actions in the video\n(A) Cutting the strip of paper, dipping in glue, bending the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(B) Bending the strip of paper, dipping in glue, cutting the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(C) Bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper, cutting the strip of paper\n(D) Cutting the strip of paper, bending the strip of paper, dipping in glue, adjusting the position of the strip of paper, fixing the strip of paper\n(E) Bending the strip of paper, cutting the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper\n(F) Cutting the strip of paper, bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper", "task_type": "tutorialQA", "pred_answer": "F", "answer": "F"}, "input": "The sequence of origami actions in the video\n(A) Cutting the strip of paper, dipping in glue, bending the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(B) Bending the strip of paper, dipping in glue, cutting the strip of paper, fixing the strip of paper, adjusting the position of the strip of paper\n(C) Bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper, cutting the strip of paper\n(D) Cutting the strip of paper, bending the strip of paper, dipping in glue, adjusting the position of the strip of paper, fixing the strip of paper\n(E) Bending the strip of paper, cutting the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper\n(F) Cutting the strip of paper, bending the strip of paper, dipping in glue, fixing the strip of paper, adjusting the position of the strip of paper\nOnly give the best option.\nBest option: ("}
{"doc_id": 443, "doc": {"video_name": "test_count_37.mp4", "duration": 284.04999999999995, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 6\n(B) 4\n(C) 5\n(D) 0\n(E) 1\n(F) 3", "candidates": ["6", "4", "5", "0", "1", "3"], "answer": "B", "task_type": "count", "question_id": "AC_1", "indices": ["203", "261", "754", "812", "1131", "1334", "1682", "2030", "2262", "2407", "2871", "2958", "3335", "3538", "3828", "4118", "4234", "4611", "4814", "5104", "5278", "5771", "5916", "6061", "6380", "6699", "7047", "7192", "7453", "7772", "7946", "8410"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 4"]], "filtered_resps": ["(B) 4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "942be82c6a2086456089fd56b77af5dee20f92fedea0917893d38408f0a0cd9f", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 6\n(B) 4\n(C) 5\n(D) 0\n(E) 1\n(F) 3", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'playing trombone' action\n(A) 6\n(B) 4\n(C) 5\n(D) 0\n(E) 1\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 447, "doc": {"video_name": "test_count_29.mp4", "duration": 566.03, "question": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 1\n(C) 2\n(D) 5\n(E) 0\n(F) 3", "candidates": ["4", "1", "2", "5", "0", "3"], "answer": "B", "task_type": "count", "question_id": "AC_5", "indices": ["0", "1015", "1044", "1595", "2175", "3074", "3190", "3683", "4263", "5249", "5452", "5829", "6844", "7250", "7685", "7946", "8758", "9048", "9802", "10411", "10585", "11397", "12035", "12644", "12760", "13659", "13920", "14355", "14935", "15573", "15892", "16733"]}, "target": "B", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(B) 1"]], "filtered_resps": ["(B) 1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8952af96cc26eb947ea2f46c1b832dd5be786ac8a696e42ba8ab91454480a54c", "target_hash": "df7e70e5021544f4834bbee64a9e3789febc4be81470df629cad6ddb03320a5c", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 1\n(C) 2\n(D) 5\n(E) 0\n(F) 3", "task_type": "count", "pred_answer": "B", "answer": "B"}, "input": "In this video, how many instances are there of the 'playing harp' action scene in total?\n(A) 4\n(B) 1\n(C) 2\n(D) 5\n(E) 0\n(F) 3\nOnly give the best option.\nBest option: ("}
{"doc_id": 451, "doc": {"video_name": "test_count_15.mp4", "duration": 673.07, "question": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 6\n(E) 0\n(F) 4", "candidates": ["3", "5", "2", "6", "0", "4"], "answer": "D", "task_type": "count", "question_id": "AC_9", "indices": ["4560", "4590", "4620", "7200", "7230", "7260", "7290", "7320", "7350", "7380", "7410", "7440", "7470", "13800", "13830", "13860", "13890", "13920", "13950", "13980", "14010", "14040", "14070", "15330", "15420", "18990", "19020", "19050", "19080", "19110", "19140", "19170"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab3aeb6c9701ea1ef3586d25360e538fc69b2411d3980f62e1d6bd74cd8d0c59", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 6\n(E) 0\n(F) 4", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'milking cow' action scene in total?\n(A) 3\n(B) 5\n(C) 2\n(D) 6\n(E) 0\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 455, "doc": {"video_name": "test_count_3.mp4", "duration": 468.48, "question": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 2\n(E) 4\n(F) 6", "candidates": ["0", "5", "1", "2", "4", "6"], "answer": "D", "task_type": "count", "question_id": "AC_13", "indices": ["406", "609", "928", "1566", "2001", "2581", "3016", "3393", "3625", "3944", "4495", "4930", "5539", "5713", "6148", "6699", "7221", "7453", "7975", "8381", "8903", "9251", "9976", "10179", "10788", "11368", "11629", "11977", "12615", "12963", "13137", "13949"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3630fe0480f466cba983acbb55ee470229a1e9e3508d6c344dec72db41156aef", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 2\n(E) 4\n(F) 6", "task_type": "count", "pred_answer": "A", "answer": "D"}, "input": "In this video, how many instances are there of the 'tossing coin' action scene in total?\n(A) 0\n(B) 5\n(C) 1\n(D) 2\n(E) 4\n(F) 6\nOnly give the best option.\nBest option: ("}
{"doc_id": 459, "doc": {"video_name": "test_count_20.mp4", "duration": 612.99, "question": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 2", "candidates": ["1", "6", "3", "5", "0", "2"], "answer": "F", "task_type": "count", "question_id": "AC_17", "indices": ["319", "928", "1479", "1856", "2813", "3364", "3712", "4205", "4669", "5626", "5713", "6322", "6960", "7917", "8439", "9048", "9686", "9715", "10585", "10875", "11542", "12093", "12847", "13601", "14239", "14674", "15370", "15863", "16153", "16965", "17516", "18009"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 0"]], "filtered_resps": ["(E) 0"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb89c1c2eeea7224e96b29f65a00efccd8e9e43d7f5375de0c2f141affdc7966", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 2", "task_type": "count", "pred_answer": "E", "answer": "F"}, "input": "In this video, how many instances are there of the 'pole vault' action scene in total?\n(A) 1\n(B) 6\n(C) 3\n(D) 5\n(E) 0\n(F) 2\nOnly give the best option.\nBest option: ("}
{"doc_id": 463, "doc": {"video_name": "test_count_1.mp4", "duration": 489.06999999999994, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 0\n(B) 6\n(C) 5\n(D) 1\n(E) 3\n(F) 4", "candidates": ["0", "6", "5", "1", "3", "4"], "answer": "C", "task_type": "count", "question_id": "AC_21", "indices": ["720", "750", "780", "810", "840", "870", "900", "930", "960", "990", "4050", "7080", "7140", "7200", "7230", "7320", "7350", "8700", "8730", "8760", "8790", "8820", "11610", "11640", "11670", "11700", "11730", "11760", "11790", "11820", "11850", "11880"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 3"]], "filtered_resps": ["(E) 3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8e76b4a34bece8fe12c90dbd542c11d80dddb0cc34ee0b93d21eb23096d20ba", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 0\n(B) 6\n(C) 5\n(D) 1\n(E) 3\n(F) 4", "task_type": "count", "pred_answer": "E", "answer": "C"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'abseiling' action\n(A) 0\n(B) 6\n(C) 5\n(D) 1\n(E) 3\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 467, "doc": {"video_name": "test_count_25.mp4", "duration": 619.42, "question": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 1\n(E) 2\n(F) 0", "candidates": ["3", "5", "4", "1", "2", "0"], "answer": "D", "task_type": "count", "question_id": "AC_25", "indices": ["319", "928", "1160", "2204", "2726", "3422", "3770", "4553", "5046", "5307", "6061", "6438", "7076", "7685", "8555", "8816", "9657", "10266", "10440", "11020", "12122", "12731", "12876", "13630", "14210", "14500", "15602", "15921", "16733", "16994", "17516", "18502"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["E"]], "filtered_resps": ["E"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a0db79d5415816bdfc7543a58f9fab829f91be56ca85e828b829a35149c9f120", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 1\n(E) 2\n(F) 0", "task_type": "count", "pred_answer": "E", "answer": "D"}, "input": "In this video, how many times does the scene of the 'jetskiing' action appear in total?\n(A) 3\n(B) 5\n(C) 4\n(D) 1\n(E) 2\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 471, "doc": {"video_name": "test_count_55.mp4", "duration": 5641.3200000000015, "question": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n(E) 2\n(F) 1", "candidates": ["5", "3", "0", "4", "2", "1"], "answer": "A", "task_type": "count", "question_id": "AC_29", "indices": ["13410", "13470", "13500", "13530", "13560", "13590", "89700", "98310", "98340", "98370", "98400", "98430", "98460", "98490", "98520", "98550", "98580", "158670", "158700", "158760", "158790", "158820", "158850", "158880", "158910", "158940", "159570", "159600", "159660", "159690", "159720", "159750"]}, "target": "A", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f4b5d4d5e24d64dcc833f675e92b796d2838bbe709608f8b6b40a769f6c7f6f5", "target_hash": "559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n(E) 2\n(F) 1", "task_type": "count", "pred_answer": "D", "answer": "A"}, "input": "In this video, how many instances are there of the 'paragliding' action scene in total?\n(A) 5\n(B) 3\n(C) 0\n(D) 4\n(E) 2\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 475, "doc": {"video_name": "test_count_30.mp4", "duration": 596.0699999999999, "question": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 0\n(C) 6\n(D) 2\n(E) 4\n(F) 1", "candidates": ["5", "0", "6", "2", "4", "1"], "answer": "E", "task_type": "count", "question_id": "AC_33", "indices": ["1980", "2010", "2040", "2070", "2100", "2130", "2160", "2190", "2220", "2250", "4410", "4440", "4470", "4500", "4530", "4560", "4590", "4620", "4650", "4680", "10560", "10590", "10620", "10650", "10680", "10710", "10740", "10770", "10800", "11220", "11250", "11280"]}, "target": "E", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(E) 4"]], "filtered_resps": ["(E) 4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4bb462423e836a09ba3f935ebb9e7f5621ec035a37ccc7cb8c78da6d6e8520ab", "target_hash": "a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 0\n(C) 6\n(D) 2\n(E) 4\n(F) 1", "task_type": "count", "pred_answer": "E", "answer": "E"}, "input": "In this video, how many instances are there of the 'carving pumpkin' action scene in total?\n(A) 5\n(B) 0\n(C) 6\n(D) 2\n(E) 4\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 479, "doc": {"video_name": "test_count_13.mp4", "duration": 530.0699999999999, "question": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 5", "candidates": ["2", "3", "1", "0", "4", "5"], "answer": "F", "task_type": "count", "question_id": "AC_37", "indices": ["330", "870", "1440", "1710", "2190", "2610", "3240", "3930", "4380", "4590", "5370", "5880", "6090", "6420", "7140", "7590", "8310", "8790", "8970", "9900", "10380", "10680", "11370", "11400", "12180", "12540", "13290", "13500", "14340", "14430", "15360", "15660"]}, "target": "F", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(A) 2"]], "filtered_resps": ["(A) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6552a67090b930460f6546bb3441fe098a675d4864c24425f41b5d67ebb567e", "target_hash": "f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d3788d2723d7bfacbe28a9", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 5", "task_type": "count", "pred_answer": "A", "answer": "F"}, "input": "In this video, how many instances are there of the 'riding mule' action scene in total?\n(A) 2\n(B) 3\n(C) 1\n(D) 0\n(E) 4\n(F) 5\nOnly give the best option.\nBest option: ("}
{"doc_id": 483, "doc": {"video_name": "test_count_58.mp4", "duration": 576.04, "question": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n(E) 0\n(F) 4", "candidates": ["6", "3", "2", "5", "0", "4"], "answer": "C", "task_type": "count", "question_id": "AC_41", "indices": ["0", "986", "1073", "1914", "2639", "2842", "3596", "4263", "4582", "5191", "5858", "6322", "6844", "7482", "7511", "8584", "8990", "9483", "9802", "10585", "11107", "11397", "11977", "12499", "13021", "13746", "14007", "14848", "15225", "15805", "16182", "17023"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["(C) 2"]], "filtered_resps": ["(C) 2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c606ee4ce881389419395e8e163b8d34fa55e23abb011a9fb166cc5dd236922", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n(E) 0\n(F) 4", "task_type": "count", "pred_answer": "C", "answer": "C"}, "input": "In this video, how many instances are there of the 'baking cookies' action scene in total?\n(A) 6\n(B) 3\n(C) 2\n(D) 5\n(E) 0\n(F) 4\nOnly give the best option.\nBest option: ("}
{"doc_id": 487, "doc": {"video_name": "test_count_21.mp4", "duration": 419.38, "question": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n(E) 5\n(F) 1", "candidates": ["0", "2", "4", "3", "5", "1"], "answer": "C", "task_type": "count", "question_id": "AC_45", "indices": ["0", "580", "1073", "1160", "1682", "2088", "2610", "3045", "3480", "3509", "3973", "4524", "4727", "5162", "5771", "6003", "6351", "6931", "7076", "7801", "7888", "8352", "8787", "9135", "9686", "9918", "10295", "10846", "11252", "11687", "11948", "12354"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["B"]], "filtered_resps": ["B"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e77af9055bd7c879684ccdaadf141989c60dcb1e183ae0c87e10bda26e8c927d", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n(E) 5\n(F) 1", "task_type": "count", "pred_answer": "B", "answer": "C"}, "input": "In this video, how many times does the scene of the 'cleaning toilet' action appear in total?\n(A) 0\n(B) 2\n(C) 4\n(D) 3\n(E) 5\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 491, "doc": {"video_name": "test_count_34.mp4", "duration": 642.3299999999999, "question": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 2\n(E) 0\n(F) 1", "candidates": ["4", "6", "3", "2", "0", "1"], "answer": "D", "task_type": "count", "question_id": "AC_49", "indices": ["551", "696", "1421", "2146", "2378", "3161", "3886", "4437", "5133", "5916", "5974", "6583", "7569", "8120", "8961", "8990", "9773", "10759", "11368", "11397", "12441", "12673", "13775", "13949", "14964", "14993", "15747", "16791", "16965", "17806", "18009", "18937"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["D"]], "filtered_resps": ["D"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f6997e096ca2494918bbaebd8330bfd9c529586d6cd5f7196187c4905c68081d", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 2\n(E) 0\n(F) 1", "task_type": "count", "pred_answer": "D", "answer": "D"}, "input": "In this video, how many times does the scene of the 'stomping grapes' action appear in total?\n(A) 4\n(B) 6\n(C) 3\n(D) 2\n(E) 0\n(F) 1\nOnly give the best option.\nBest option: ("}
{"doc_id": 495, "doc": {"video_name": "test_count_12.mp4", "duration": 649.8, "question": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "candidates": ["1", "6", "4", "2", "5", "0"], "answer": "D", "task_type": "count", "question_id": "AC_53", "indices": ["58", "812", "1479", "1914", "2813", "3422", "3683", "4785", "4959", "5916", "6119", "6844", "7540", "8410", "8758", "9106", "10266", "10353", "11310", "11658", "12673", "13079", "13833", "14471", "14993", "15631", "16327", "16907", "17603", "17777", "18531", "19256"]}, "target": "D", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["F"]], "filtered_resps": ["F"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5de6f83e642015979b3ddc36fcb4a1374c3989766aaa530b3afa09dc8df35320", "target_hash": "3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0ebfda1a01ec05745d43", "mlvu_percetion_score": {"question_id": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0", "task_type": "count", "pred_answer": "F", "answer": "D"}, "input": "Throughout this video, what is the total count of occurrences for the scene featuring the 'riding mule' action\n(A) 1\n(B) 6\n(C) 4\n(D) 2\n(E) 5\n(F) 0\nOnly give the best option.\nBest option: ("}
{"doc_id": 499, "doc": {"video_name": "test_count_4.mp4", "duration": 538.5, "question": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 1\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 2", "candidates": ["1", "3", "6", "4", "0", "2"], "answer": "C", "task_type": "count", "question_id": "AC_57", "indices": ["87", "841", "1218", "1479", "2175", "2871", "3016", "3828", "4466", "4756", "5307", "5539", "6177", "6583", "7047", "8033", "8323", "8845", "9396", "9570", "10179", "10701", "11339", "12035", "12325", "13050", "13514", "14007", "14413", "15080", "15312", "15776"]}, "target": "C", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "until"}, "resps": [["A", "A"]], "filtered_resps": ["A"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9182a91a88624c2827c323b8095f21f144eb84e920ac05a881650810bd942284", "target_hash": "6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d", "mlvu_percetion_score": {"question_id": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 1\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 2", "task_type": "count", "pred_answer": "A", "answer": "C"}, "input": "In this video, how many times does the scene of the 'tossing coin' action appear in total?\n(A) 1\n(B) 3\n(C) 6\n(D) 4\n(E) 0\n(F) 2\nOnly give the best option.\nBest option: ("}
